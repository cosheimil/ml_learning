{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c).unsqueeze(1) # <1>\n",
    "t_u = torch.tensor(t_u).unsqueeze(1) # <1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([8, 6, 5, 9, 4, 1, 2, 3, 0]), tensor([10,  7]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = t_u.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "\n",
    "shuffled_indices = torch.randperm(n_samples)\n",
    "\n",
    "train_indices = shuffled_indices[:-n_val]\n",
    "val_indices = shuffled_indices[-n_val:]\n",
    "\n",
    "train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_u_train = t_u[train_indices]\n",
    "t_c_train = t_c[train_indices]\n",
    "\n",
    "t_u_val = t_u[val_indices]\n",
    "t_c_val = t_c[val_indices]\n",
    "\n",
    "t_un_train = 0.1 * t_u_train\n",
    "t_un_val = 0.1 * t_u_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.0894],\n",
       "        [1.0605]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "linear_model(t_un_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.4354]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.1114], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5468], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5468],\n",
       "        [0.5468],\n",
       "        [0.5468],\n",
       "        [0.5468],\n",
       "        [0.5468],\n",
       "        [0.5468],\n",
       "        [0.5468],\n",
       "        [0.5468],\n",
       "        [0.5468],\n",
       "        [0.5468]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, 1)\n",
    "linear_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/micromamba/envs/ml/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(1, 1)\n",
    "optimizer = optim.SGD(\n",
    "    linear_model.parameters(),\n",
    "    lr=1e-2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train, t_c_val):\n",
    "    for epoch in range(n_epochs):\n",
    "        t_p_train = model(t_u_train)\n",
    "        loss_train = loss_fn(t_p_train, t_c_train)\n",
    "        \n",
    "        t_p_val = model(t_u_val)\n",
    "        loss_val = loss_fn(t_p_val, t_c_val)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}: Train loss: {loss_train} Valid loss: {loss_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1: Train loss: 184.00115966796875 Valid loss: 238.3381805419922\n",
      "Epoch: 2: Train loss: 48.75218963623047 Valid loss: 98.01850128173828\n",
      "Epoch: 3: Train loss: 29.499080657958984 Valid loss: 72.10547637939453\n",
      "Epoch: 4: Train loss: 26.718034744262695 Valid loss: 66.08155822753906\n",
      "Epoch: 5: Train loss: 26.276199340820312 Valid loss: 64.25494384765625\n",
      "Epoch: 6: Train loss: 26.166664123535156 Valid loss: 63.541934967041016\n",
      "Epoch: 7: Train loss: 26.10441017150879 Valid loss: 63.182342529296875\n",
      "Epoch: 8: Train loss: 26.04899024963379 Valid loss: 62.94672393798828\n",
      "Epoch: 9: Train loss: 25.994646072387695 Valid loss: 62.75675964355469\n",
      "Epoch: 10: Train loss: 25.940563201904297 Valid loss: 62.58403778076172\n",
      "Epoch: 11: Train loss: 25.886632919311523 Valid loss: 62.418060302734375\n",
      "Epoch: 12: Train loss: 25.832820892333984 Valid loss: 62.254859924316406\n",
      "Epoch: 13: Train loss: 25.779142379760742 Valid loss: 62.092979431152344\n",
      "Epoch: 14: Train loss: 25.725582122802734 Valid loss: 61.93183898925781\n",
      "Epoch: 15: Train loss: 25.67215347290039 Valid loss: 61.77122497558594\n",
      "Epoch: 16: Train loss: 25.618844985961914 Valid loss: 61.6110725402832\n",
      "Epoch: 17: Train loss: 25.56566619873047 Valid loss: 61.45134735107422\n",
      "Epoch: 18: Train loss: 25.51259994506836 Valid loss: 61.292030334472656\n",
      "Epoch: 19: Train loss: 25.45966911315918 Valid loss: 61.13311767578125\n",
      "Epoch: 20: Train loss: 25.4068603515625 Valid loss: 60.97459411621094\n",
      "Epoch: 21: Train loss: 25.35417366027832 Valid loss: 60.81648254394531\n",
      "Epoch: 22: Train loss: 25.30160903930664 Valid loss: 60.65876770019531\n",
      "Epoch: 23: Train loss: 25.24916648864746 Valid loss: 60.50145721435547\n",
      "Epoch: 24: Train loss: 25.196847915649414 Valid loss: 60.34453582763672\n",
      "Epoch: 25: Train loss: 25.144657135009766 Valid loss: 60.188018798828125\n",
      "Epoch: 26: Train loss: 25.09257698059082 Valid loss: 60.03190612792969\n",
      "Epoch: 27: Train loss: 25.040620803833008 Valid loss: 59.87617492675781\n",
      "Epoch: 28: Train loss: 24.98879051208496 Valid loss: 59.72083282470703\n",
      "Epoch: 29: Train loss: 24.93708038330078 Valid loss: 59.56590270996094\n",
      "Epoch: 30: Train loss: 24.885488510131836 Valid loss: 59.411354064941406\n",
      "Epoch: 31: Train loss: 24.834022521972656 Valid loss: 59.25719451904297\n",
      "Epoch: 32: Train loss: 24.78266716003418 Valid loss: 59.10342025756836\n",
      "Epoch: 33: Train loss: 24.73143768310547 Valid loss: 58.950050354003906\n",
      "Epoch: 34: Train loss: 24.68032455444336 Valid loss: 58.797061920166016\n",
      "Epoch: 35: Train loss: 24.62934112548828 Valid loss: 58.64445495605469\n",
      "Epoch: 36: Train loss: 24.578468322753906 Valid loss: 58.49223327636719\n",
      "Epoch: 37: Train loss: 24.527711868286133 Valid loss: 58.34040832519531\n",
      "Epoch: 38: Train loss: 24.477075576782227 Valid loss: 58.18896484375\n",
      "Epoch: 39: Train loss: 24.426557540893555 Valid loss: 58.037906646728516\n",
      "Epoch: 40: Train loss: 24.376163482666016 Valid loss: 57.88722229003906\n",
      "Epoch: 41: Train loss: 24.325883865356445 Valid loss: 57.736915588378906\n",
      "Epoch: 42: Train loss: 24.27571678161621 Valid loss: 57.587005615234375\n",
      "Epoch: 43: Train loss: 24.225669860839844 Valid loss: 57.43746566772461\n",
      "Epoch: 44: Train loss: 24.175743103027344 Valid loss: 57.28830337524414\n",
      "Epoch: 45: Train loss: 24.12592887878418 Valid loss: 57.13953399658203\n",
      "Epoch: 46: Train loss: 24.07623291015625 Valid loss: 56.99113464355469\n",
      "Epoch: 47: Train loss: 24.026649475097656 Valid loss: 56.84310531616211\n",
      "Epoch: 48: Train loss: 23.97718620300293 Valid loss: 56.69544982910156\n",
      "Epoch: 49: Train loss: 23.92783546447754 Valid loss: 56.54817199707031\n",
      "Epoch: 50: Train loss: 23.878604888916016 Valid loss: 56.401268005371094\n",
      "Epoch: 51: Train loss: 23.82948112487793 Valid loss: 56.25474166870117\n",
      "Epoch: 52: Train loss: 23.780479431152344 Valid loss: 56.10857391357422\n",
      "Epoch: 53: Train loss: 23.731590270996094 Valid loss: 55.962791442871094\n",
      "Epoch: 54: Train loss: 23.68280792236328 Valid loss: 55.817378997802734\n",
      "Epoch: 55: Train loss: 23.6341495513916 Valid loss: 55.672325134277344\n",
      "Epoch: 56: Train loss: 23.585601806640625 Valid loss: 55.52763748168945\n",
      "Epoch: 57: Train loss: 23.53716278076172 Valid loss: 55.38332748413086\n",
      "Epoch: 58: Train loss: 23.488840103149414 Valid loss: 55.2393798828125\n",
      "Epoch: 59: Train loss: 23.44063377380371 Valid loss: 55.095787048339844\n",
      "Epoch: 60: Train loss: 23.39253807067871 Valid loss: 54.952579498291016\n",
      "Epoch: 61: Train loss: 23.344552993774414 Valid loss: 54.809722900390625\n",
      "Epoch: 62: Train loss: 23.29667854309082 Valid loss: 54.66722106933594\n",
      "Epoch: 63: Train loss: 23.248920440673828 Valid loss: 54.52509307861328\n",
      "Epoch: 64: Train loss: 23.201271057128906 Valid loss: 54.383338928222656\n",
      "Epoch: 65: Train loss: 23.153730392456055 Valid loss: 54.241920471191406\n",
      "Epoch: 66: Train loss: 23.10630226135254 Valid loss: 54.100868225097656\n",
      "Epoch: 67: Train loss: 23.058990478515625 Valid loss: 53.960174560546875\n",
      "Epoch: 68: Train loss: 23.011777877807617 Valid loss: 53.819854736328125\n",
      "Epoch: 69: Train loss: 22.96468734741211 Valid loss: 53.679866790771484\n",
      "Epoch: 70: Train loss: 22.917699813842773 Valid loss: 53.540252685546875\n",
      "Epoch: 71: Train loss: 22.870826721191406 Valid loss: 53.400978088378906\n",
      "Epoch: 72: Train loss: 22.824058532714844 Valid loss: 53.26206970214844\n",
      "Epoch: 73: Train loss: 22.77739715576172 Valid loss: 53.12351608276367\n",
      "Epoch: 74: Train loss: 22.73085594177246 Valid loss: 52.98530578613281\n",
      "Epoch: 75: Train loss: 22.684410095214844 Valid loss: 52.847450256347656\n",
      "Epoch: 76: Train loss: 22.638076782226562 Valid loss: 52.709938049316406\n",
      "Epoch: 77: Train loss: 22.591856002807617 Valid loss: 52.57278823852539\n",
      "Epoch: 78: Train loss: 22.545738220214844 Valid loss: 52.43598175048828\n",
      "Epoch: 79: Train loss: 22.499732971191406 Valid loss: 52.29951858520508\n",
      "Epoch: 80: Train loss: 22.453834533691406 Valid loss: 52.16340637207031\n",
      "Epoch: 81: Train loss: 22.40804100036621 Valid loss: 52.02764129638672\n",
      "Epoch: 82: Train loss: 22.362356185913086 Valid loss: 51.892234802246094\n",
      "Epoch: 83: Train loss: 22.316774368286133 Valid loss: 51.75715255737305\n",
      "Epoch: 84: Train loss: 22.271297454833984 Valid loss: 51.62242126464844\n",
      "Epoch: 85: Train loss: 22.225933074951172 Valid loss: 51.488037109375\n",
      "Epoch: 86: Train loss: 22.18067169189453 Valid loss: 51.35399627685547\n",
      "Epoch: 87: Train loss: 22.135520935058594 Valid loss: 51.22028350830078\n",
      "Epoch: 88: Train loss: 22.09046745300293 Valid loss: 51.08692169189453\n",
      "Epoch: 89: Train loss: 22.04552459716797 Valid loss: 50.953887939453125\n",
      "Epoch: 90: Train loss: 22.00068473815918 Valid loss: 50.82122039794922\n",
      "Epoch: 91: Train loss: 21.95594596862793 Valid loss: 50.688865661621094\n",
      "Epoch: 92: Train loss: 21.91131591796875 Valid loss: 50.556854248046875\n",
      "Epoch: 93: Train loss: 21.866790771484375 Valid loss: 50.425174713134766\n",
      "Epoch: 94: Train loss: 21.822370529174805 Valid loss: 50.293846130371094\n",
      "Epoch: 95: Train loss: 21.778053283691406 Valid loss: 50.162837982177734\n",
      "Epoch: 96: Train loss: 21.73383331298828 Valid loss: 50.03217315673828\n",
      "Epoch: 97: Train loss: 21.689720153808594 Valid loss: 49.90184020996094\n",
      "Epoch: 98: Train loss: 21.645709991455078 Valid loss: 49.77183532714844\n",
      "Epoch: 99: Train loss: 21.601802825927734 Valid loss: 49.64216232299805\n",
      "Epoch: 100: Train loss: 21.55799674987793 Valid loss: 49.5128173828125\n",
      "Epoch: 101: Train loss: 21.514297485351562 Valid loss: 49.38380432128906\n",
      "Epoch: 102: Train loss: 21.470699310302734 Valid loss: 49.25513458251953\n",
      "Epoch: 103: Train loss: 21.427202224731445 Valid loss: 49.12677001953125\n",
      "Epoch: 104: Train loss: 21.38380241394043 Valid loss: 48.99873733520508\n",
      "Epoch: 105: Train loss: 21.34050750732422 Valid loss: 48.87104034423828\n",
      "Epoch: 106: Train loss: 21.29731559753418 Valid loss: 48.74366760253906\n",
      "Epoch: 107: Train loss: 21.254220962524414 Valid loss: 48.61662292480469\n",
      "Epoch: 108: Train loss: 21.211225509643555 Valid loss: 48.489898681640625\n",
      "Epoch: 109: Train loss: 21.1683349609375 Valid loss: 48.36349105834961\n",
      "Epoch: 110: Train loss: 21.125539779663086 Valid loss: 48.237422943115234\n",
      "Epoch: 111: Train loss: 21.08285140991211 Valid loss: 48.11165237426758\n",
      "Epoch: 112: Train loss: 21.040260314941406 Valid loss: 47.986228942871094\n",
      "Epoch: 113: Train loss: 20.997766494750977 Valid loss: 47.861122131347656\n",
      "Epoch: 114: Train loss: 20.955373764038086 Valid loss: 47.73632049560547\n",
      "Epoch: 115: Train loss: 20.91307830810547 Valid loss: 47.611846923828125\n",
      "Epoch: 116: Train loss: 20.870882034301758 Valid loss: 47.48767852783203\n",
      "Epoch: 117: Train loss: 20.828779220581055 Valid loss: 47.36384582519531\n",
      "Epoch: 118: Train loss: 20.78678321838379 Valid loss: 47.24031066894531\n",
      "Epoch: 119: Train loss: 20.744882583618164 Valid loss: 47.11710739135742\n",
      "Epoch: 120: Train loss: 20.703081130981445 Valid loss: 46.994224548339844\n",
      "Epoch: 121: Train loss: 20.661376953125 Valid loss: 46.871639251708984\n",
      "Epoch: 122: Train loss: 20.619766235351562 Valid loss: 46.749366760253906\n",
      "Epoch: 123: Train loss: 20.57825469970703 Valid loss: 46.62741470336914\n",
      "Epoch: 124: Train loss: 20.536842346191406 Valid loss: 46.505775451660156\n",
      "Epoch: 125: Train loss: 20.495521545410156 Valid loss: 46.38444519042969\n",
      "Epoch: 126: Train loss: 20.45429801940918 Valid loss: 46.263423919677734\n",
      "Epoch: 127: Train loss: 20.413177490234375 Valid loss: 46.14271926879883\n",
      "Epoch: 128: Train loss: 20.372148513793945 Valid loss: 46.022315979003906\n",
      "Epoch: 129: Train loss: 20.33121109008789 Valid loss: 45.9022216796875\n",
      "Epoch: 130: Train loss: 20.290372848510742 Valid loss: 45.78243637084961\n",
      "Epoch: 131: Train loss: 20.2496337890625 Valid loss: 45.66294860839844\n",
      "Epoch: 132: Train loss: 20.208986282348633 Valid loss: 45.54377365112305\n",
      "Epoch: 133: Train loss: 20.168434143066406 Valid loss: 45.4249153137207\n",
      "Epoch: 134: Train loss: 20.127975463867188 Valid loss: 45.30635070800781\n",
      "Epoch: 135: Train loss: 20.087608337402344 Valid loss: 45.188087463378906\n",
      "Epoch: 136: Train loss: 20.047346115112305 Valid loss: 45.070125579833984\n",
      "Epoch: 137: Train loss: 20.007165908813477 Valid loss: 44.95248031616211\n",
      "Epoch: 138: Train loss: 19.96708869934082 Valid loss: 44.835121154785156\n",
      "Epoch: 139: Train loss: 19.92709732055664 Valid loss: 44.71807098388672\n",
      "Epoch: 140: Train loss: 19.88720703125 Valid loss: 44.601314544677734\n",
      "Epoch: 141: Train loss: 19.847402572631836 Valid loss: 44.48486328125\n",
      "Epoch: 142: Train loss: 19.807695388793945 Valid loss: 44.36870574951172\n",
      "Epoch: 143: Train loss: 19.76807975769043 Valid loss: 44.25284957885742\n",
      "Epoch: 144: Train loss: 19.728553771972656 Valid loss: 44.137290954589844\n",
      "Epoch: 145: Train loss: 19.689125061035156 Valid loss: 44.02202606201172\n",
      "Epoch: 146: Train loss: 19.649789810180664 Valid loss: 43.90705490112305\n",
      "Epoch: 147: Train loss: 19.61054229736328 Valid loss: 43.79237747192383\n",
      "Epoch: 148: Train loss: 19.57138442993164 Valid loss: 43.678009033203125\n",
      "Epoch: 149: Train loss: 19.53232192993164 Valid loss: 43.56391906738281\n",
      "Epoch: 150: Train loss: 19.493345260620117 Valid loss: 43.45012283325195\n",
      "Epoch: 151: Train loss: 19.4544677734375 Valid loss: 43.33662414550781\n",
      "Epoch: 152: Train loss: 19.41567611694336 Valid loss: 43.223419189453125\n",
      "Epoch: 153: Train loss: 19.37697410583496 Valid loss: 43.1104850769043\n",
      "Epoch: 154: Train loss: 19.338367462158203 Valid loss: 42.99785614013672\n",
      "Epoch: 155: Train loss: 19.299850463867188 Valid loss: 42.88551712036133\n",
      "Epoch: 156: Train loss: 19.261417388916016 Valid loss: 42.77347183227539\n",
      "Epoch: 157: Train loss: 19.223073959350586 Valid loss: 42.66170883178711\n",
      "Epoch: 158: Train loss: 19.184823989868164 Valid loss: 42.55023193359375\n",
      "Epoch: 159: Train loss: 19.14666748046875 Valid loss: 42.43904113769531\n",
      "Epoch: 160: Train loss: 19.10858917236328 Valid loss: 42.32813262939453\n",
      "Epoch: 161: Train loss: 19.070608139038086 Valid loss: 42.21751403808594\n",
      "Epoch: 162: Train loss: 19.03271484375 Valid loss: 42.107177734375\n",
      "Epoch: 163: Train loss: 18.99490737915039 Valid loss: 41.99711990356445\n",
      "Epoch: 164: Train loss: 18.95718765258789 Valid loss: 41.88735580444336\n",
      "Epoch: 165: Train loss: 18.919559478759766 Valid loss: 41.777862548828125\n",
      "Epoch: 166: Train loss: 18.882017135620117 Valid loss: 41.66865539550781\n",
      "Epoch: 167: Train loss: 18.84455680847168 Valid loss: 41.55973815917969\n",
      "Epoch: 168: Train loss: 18.807191848754883 Valid loss: 41.451087951660156\n",
      "Epoch: 169: Train loss: 18.769912719726562 Valid loss: 41.34272003173828\n",
      "Epoch: 170: Train loss: 18.732725143432617 Valid loss: 41.234642028808594\n",
      "Epoch: 171: Train loss: 18.69561767578125 Valid loss: 41.12683868408203\n",
      "Epoch: 172: Train loss: 18.658592224121094 Valid loss: 41.0192985534668\n",
      "Epoch: 173: Train loss: 18.62166404724121 Valid loss: 40.91204833984375\n",
      "Epoch: 174: Train loss: 18.584815979003906 Valid loss: 40.805076599121094\n",
      "Epoch: 175: Train loss: 18.54805564880371 Valid loss: 40.6983642578125\n",
      "Epoch: 176: Train loss: 18.511381149291992 Valid loss: 40.591941833496094\n",
      "Epoch: 177: Train loss: 18.47479248046875 Valid loss: 40.48578643798828\n",
      "Epoch: 178: Train loss: 18.43828582763672 Valid loss: 40.37990188598633\n",
      "Epoch: 179: Train loss: 18.40186882019043 Valid loss: 40.27429962158203\n",
      "Epoch: 180: Train loss: 18.365537643432617 Valid loss: 40.16895294189453\n",
      "Epoch: 181: Train loss: 18.329286575317383 Valid loss: 40.06389617919922\n",
      "Epoch: 182: Train loss: 18.293123245239258 Valid loss: 39.959102630615234\n",
      "Epoch: 183: Train loss: 18.25704002380371 Valid loss: 39.85456848144531\n",
      "Epoch: 184: Train loss: 18.221046447753906 Valid loss: 39.75032424926758\n",
      "Epoch: 185: Train loss: 18.185136795043945 Valid loss: 39.646339416503906\n",
      "Epoch: 186: Train loss: 18.149309158325195 Valid loss: 39.54261016845703\n",
      "Epoch: 187: Train loss: 18.113563537597656 Valid loss: 39.43916702270508\n",
      "Epoch: 188: Train loss: 18.07790184020996 Valid loss: 39.33598709106445\n",
      "Epoch: 189: Train loss: 18.04232406616211 Valid loss: 39.23305892944336\n",
      "Epoch: 190: Train loss: 18.006832122802734 Valid loss: 39.13042068481445\n",
      "Epoch: 191: Train loss: 17.971418380737305 Valid loss: 39.02802276611328\n",
      "Epoch: 192: Train loss: 17.936092376708984 Valid loss: 38.925899505615234\n",
      "Epoch: 193: Train loss: 17.900846481323242 Valid loss: 38.824039459228516\n",
      "Epoch: 194: Train loss: 17.86568260192871 Valid loss: 38.72243881225586\n",
      "Epoch: 195: Train loss: 17.830598831176758 Valid loss: 38.62111282348633\n",
      "Epoch: 196: Train loss: 17.79559898376465 Valid loss: 38.52003479003906\n",
      "Epoch: 197: Train loss: 17.76068115234375 Valid loss: 38.41922378540039\n",
      "Epoch: 198: Train loss: 17.725839614868164 Valid loss: 38.31866455078125\n",
      "Epoch: 199: Train loss: 17.69108772277832 Valid loss: 38.2183837890625\n",
      "Epoch: 200: Train loss: 17.656414031982422 Valid loss: 38.11835479736328\n",
      "Epoch: 201: Train loss: 17.6218204498291 Valid loss: 38.018577575683594\n",
      "Epoch: 202: Train loss: 17.587310791015625 Valid loss: 37.91905975341797\n",
      "Epoch: 203: Train loss: 17.55287742614746 Valid loss: 37.81979751586914\n",
      "Epoch: 204: Train loss: 17.51852035522461 Valid loss: 37.72079086303711\n",
      "Epoch: 205: Train loss: 17.484256744384766 Valid loss: 37.622047424316406\n",
      "Epoch: 206: Train loss: 17.45006561279297 Valid loss: 37.5235481262207\n",
      "Epoch: 207: Train loss: 17.415952682495117 Valid loss: 37.425315856933594\n",
      "Epoch: 208: Train loss: 17.381919860839844 Valid loss: 37.32733154296875\n",
      "Epoch: 209: Train loss: 17.347965240478516 Valid loss: 37.2296142578125\n",
      "Epoch: 210: Train loss: 17.31409454345703 Valid loss: 37.13213348388672\n",
      "Epoch: 211: Train loss: 17.280303955078125 Valid loss: 37.034912109375\n",
      "Epoch: 212: Train loss: 17.24658203125 Valid loss: 36.93791961669922\n",
      "Epoch: 213: Train loss: 17.21294593811035 Valid loss: 36.84120559692383\n",
      "Epoch: 214: Train loss: 17.179393768310547 Valid loss: 36.74474334716797\n",
      "Epoch: 215: Train loss: 17.145912170410156 Valid loss: 36.64850997924805\n",
      "Epoch: 216: Train loss: 17.112510681152344 Valid loss: 36.55253601074219\n",
      "Epoch: 217: Train loss: 17.079185485839844 Valid loss: 36.456817626953125\n",
      "Epoch: 218: Train loss: 17.045942306518555 Valid loss: 36.3613395690918\n",
      "Epoch: 219: Train loss: 17.012773513793945 Valid loss: 36.26610565185547\n",
      "Epoch: 220: Train loss: 16.979684829711914 Valid loss: 36.171119689941406\n",
      "Epoch: 221: Train loss: 16.946670532226562 Valid loss: 36.076385498046875\n",
      "Epoch: 222: Train loss: 16.913734436035156 Valid loss: 35.981895446777344\n",
      "Epoch: 223: Train loss: 16.880870819091797 Valid loss: 35.88763427734375\n",
      "Epoch: 224: Train loss: 16.84809112548828 Valid loss: 35.79364013671875\n",
      "Epoch: 225: Train loss: 16.815385818481445 Valid loss: 35.69987869262695\n",
      "Epoch: 226: Train loss: 16.78275489807129 Valid loss: 35.60636901855469\n",
      "Epoch: 227: Train loss: 16.750200271606445 Valid loss: 35.513092041015625\n",
      "Epoch: 228: Train loss: 16.717721939086914 Valid loss: 35.4200553894043\n",
      "Epoch: 229: Train loss: 16.685321807861328 Valid loss: 35.3272705078125\n",
      "Epoch: 230: Train loss: 16.652992248535156 Valid loss: 35.234710693359375\n",
      "Epoch: 231: Train loss: 16.620742797851562 Valid loss: 35.14241027832031\n",
      "Epoch: 232: Train loss: 16.588563919067383 Valid loss: 35.05033493041992\n",
      "Epoch: 233: Train loss: 16.55646514892578 Valid loss: 34.95850372314453\n",
      "Epoch: 234: Train loss: 16.524442672729492 Valid loss: 34.86690139770508\n",
      "Epoch: 235: Train loss: 16.492494583129883 Valid loss: 34.77555847167969\n",
      "Epoch: 236: Train loss: 16.460615158081055 Valid loss: 34.68444061279297\n",
      "Epoch: 237: Train loss: 16.42881202697754 Valid loss: 34.59355545043945\n",
      "Epoch: 238: Train loss: 16.39708709716797 Valid loss: 34.50291061401367\n",
      "Epoch: 239: Train loss: 16.365432739257812 Valid loss: 34.412498474121094\n",
      "Epoch: 240: Train loss: 16.33384895324707 Valid loss: 34.32232666015625\n",
      "Epoch: 241: Train loss: 16.30234718322754 Valid loss: 34.23237609863281\n",
      "Epoch: 242: Train loss: 16.270912170410156 Valid loss: 34.142669677734375\n",
      "Epoch: 243: Train loss: 16.23955535888672 Valid loss: 34.053199768066406\n",
      "Epoch: 244: Train loss: 16.20827293395996 Valid loss: 33.96394729614258\n",
      "Epoch: 245: Train loss: 16.17705535888672 Valid loss: 33.874942779541016\n",
      "Epoch: 246: Train loss: 16.14592170715332 Valid loss: 33.78617477416992\n",
      "Epoch: 247: Train loss: 16.114850997924805 Valid loss: 33.69761657714844\n",
      "Epoch: 248: Train loss: 16.083852767944336 Valid loss: 33.609291076660156\n",
      "Epoch: 249: Train loss: 16.052934646606445 Valid loss: 33.521209716796875\n",
      "Epoch: 250: Train loss: 16.02208709716797 Valid loss: 33.43335723876953\n",
      "Epoch: 251: Train loss: 15.991307258605957 Valid loss: 33.34572219848633\n",
      "Epoch: 252: Train loss: 15.960603713989258 Valid loss: 33.25832748413086\n",
      "Epoch: 253: Train loss: 15.92996883392334 Valid loss: 33.171142578125\n",
      "Epoch: 254: Train loss: 15.89940071105957 Valid loss: 33.084190368652344\n",
      "Epoch: 255: Train loss: 15.868913650512695 Valid loss: 32.99747085571289\n",
      "Epoch: 256: Train loss: 15.83849048614502 Valid loss: 32.91098403930664\n",
      "Epoch: 257: Train loss: 15.808144569396973 Valid loss: 32.82471466064453\n",
      "Epoch: 258: Train loss: 15.77786636352539 Valid loss: 32.73866271972656\n",
      "Epoch: 259: Train loss: 15.747654914855957 Valid loss: 32.652835845947266\n",
      "Epoch: 260: Train loss: 15.717517852783203 Valid loss: 32.56723403930664\n",
      "Epoch: 261: Train loss: 15.68745231628418 Valid loss: 32.48186492919922\n",
      "Epoch: 262: Train loss: 15.657455444335938 Valid loss: 32.39670944213867\n",
      "Epoch: 263: Train loss: 15.627528190612793 Valid loss: 32.3117790222168\n",
      "Epoch: 264: Train loss: 15.597676277160645 Valid loss: 32.227073669433594\n",
      "Epoch: 265: Train loss: 15.567888259887695 Valid loss: 32.14258575439453\n",
      "Epoch: 266: Train loss: 15.53817367553711 Valid loss: 32.05832290649414\n",
      "Epoch: 267: Train loss: 15.50852108001709 Valid loss: 31.97426414489746\n",
      "Epoch: 268: Train loss: 15.478939056396484 Valid loss: 31.89043426513672\n",
      "Epoch: 269: Train loss: 15.449432373046875 Valid loss: 31.80683135986328\n",
      "Epoch: 270: Train loss: 15.41999340057373 Valid loss: 31.723434448242188\n",
      "Epoch: 271: Train loss: 15.390618324279785 Valid loss: 31.640254974365234\n",
      "Epoch: 272: Train loss: 15.36131763458252 Valid loss: 31.557289123535156\n",
      "Epoch: 273: Train loss: 15.332080841064453 Valid loss: 31.47455596923828\n",
      "Epoch: 274: Train loss: 15.302913665771484 Valid loss: 31.39202880859375\n",
      "Epoch: 275: Train loss: 15.273815155029297 Valid loss: 31.30972671508789\n",
      "Epoch: 276: Train loss: 15.244786262512207 Valid loss: 31.227628707885742\n",
      "Epoch: 277: Train loss: 15.2158203125 Valid loss: 31.145751953125\n",
      "Epoch: 278: Train loss: 15.186924934387207 Valid loss: 31.064075469970703\n",
      "Epoch: 279: Train loss: 15.158098220825195 Valid loss: 30.982620239257812\n",
      "Epoch: 280: Train loss: 15.129337310791016 Valid loss: 30.90138053894043\n",
      "Epoch: 281: Train loss: 15.100643157958984 Valid loss: 30.820350646972656\n",
      "Epoch: 282: Train loss: 15.072016716003418 Valid loss: 30.739532470703125\n",
      "Epoch: 283: Train loss: 15.04345703125 Valid loss: 30.658931732177734\n",
      "Epoch: 284: Train loss: 15.014965057373047 Valid loss: 30.578542709350586\n",
      "Epoch: 285: Train loss: 14.986539840698242 Valid loss: 30.498355865478516\n",
      "Epoch: 286: Train loss: 14.95817756652832 Valid loss: 30.418384552001953\n",
      "Epoch: 287: Train loss: 14.929882049560547 Valid loss: 30.338611602783203\n",
      "Epoch: 288: Train loss: 14.901655197143555 Valid loss: 30.259063720703125\n",
      "Epoch: 289: Train loss: 14.873494148254395 Valid loss: 30.179702758789062\n",
      "Epoch: 290: Train loss: 14.845396041870117 Valid loss: 30.10056495666504\n",
      "Epoch: 291: Train loss: 14.817363739013672 Valid loss: 30.021644592285156\n",
      "Epoch: 292: Train loss: 14.789402961730957 Valid loss: 29.94291114807129\n",
      "Epoch: 293: Train loss: 14.761500358581543 Valid loss: 29.864391326904297\n",
      "Epoch: 294: Train loss: 14.73366641998291 Valid loss: 29.786067962646484\n",
      "Epoch: 295: Train loss: 14.705891609191895 Valid loss: 29.70796775817871\n",
      "Epoch: 296: Train loss: 14.67818832397461 Valid loss: 29.63005828857422\n",
      "Epoch: 297: Train loss: 14.65054702758789 Valid loss: 29.552349090576172\n",
      "Epoch: 298: Train loss: 14.622974395751953 Valid loss: 29.47486114501953\n",
      "Epoch: 299: Train loss: 14.595465660095215 Valid loss: 29.39755630493164\n",
      "Epoch: 300: Train loss: 14.568015098571777 Valid loss: 29.32046890258789\n",
      "Epoch: 301: Train loss: 14.540634155273438 Valid loss: 29.24358558654785\n",
      "Epoch: 302: Train loss: 14.513312339782715 Valid loss: 29.166885375976562\n",
      "Epoch: 303: Train loss: 14.486058235168457 Valid loss: 29.090404510498047\n",
      "Epoch: 304: Train loss: 14.458866119384766 Valid loss: 29.01410675048828\n",
      "Epoch: 305: Train loss: 14.431736946105957 Valid loss: 28.938030242919922\n",
      "Epoch: 306: Train loss: 14.404671669006348 Valid loss: 28.862138748168945\n",
      "Epoch: 307: Train loss: 14.377668380737305 Valid loss: 28.786449432373047\n",
      "Epoch: 308: Train loss: 14.35073184967041 Valid loss: 28.710968017578125\n",
      "Epoch: 309: Train loss: 14.323854446411133 Valid loss: 28.635677337646484\n",
      "Epoch: 310: Train loss: 14.297039031982422 Valid loss: 28.560585021972656\n",
      "Epoch: 311: Train loss: 14.270292282104492 Valid loss: 28.485679626464844\n",
      "Epoch: 312: Train loss: 14.24360466003418 Valid loss: 28.410980224609375\n",
      "Epoch: 313: Train loss: 14.21697998046875 Valid loss: 28.33647918701172\n",
      "Epoch: 314: Train loss: 14.190411567687988 Valid loss: 28.262168884277344\n",
      "Epoch: 315: Train loss: 14.163908958435059 Valid loss: 28.188060760498047\n",
      "Epoch: 316: Train loss: 14.137472152709961 Valid loss: 28.1141357421875\n",
      "Epoch: 317: Train loss: 14.11109447479248 Valid loss: 28.040420532226562\n",
      "Epoch: 318: Train loss: 14.084778785705566 Valid loss: 27.96688461303711\n",
      "Epoch: 319: Train loss: 14.058523178100586 Valid loss: 27.89354705810547\n",
      "Epoch: 320: Train loss: 14.032326698303223 Valid loss: 27.82040023803711\n",
      "Epoch: 321: Train loss: 14.006197929382324 Valid loss: 27.74746322631836\n",
      "Epoch: 322: Train loss: 13.980123519897461 Valid loss: 27.67469024658203\n",
      "Epoch: 323: Train loss: 13.95411491394043 Valid loss: 27.602127075195312\n",
      "Epoch: 324: Train loss: 13.92816162109375 Valid loss: 27.529747009277344\n",
      "Epoch: 325: Train loss: 13.902273178100586 Valid loss: 27.457563400268555\n",
      "Epoch: 326: Train loss: 13.876445770263672 Valid loss: 27.385570526123047\n",
      "Epoch: 327: Train loss: 13.85067367553711 Valid loss: 27.313758850097656\n",
      "Epoch: 328: Train loss: 13.824968338012695 Valid loss: 27.242145538330078\n",
      "Epoch: 329: Train loss: 13.799320220947266 Valid loss: 27.17070960998535\n",
      "Epoch: 330: Train loss: 13.77372932434082 Valid loss: 27.099472045898438\n",
      "Epoch: 331: Train loss: 13.74820327758789 Valid loss: 27.02842140197754\n",
      "Epoch: 332: Train loss: 13.722734451293945 Valid loss: 26.957550048828125\n",
      "Epoch: 333: Train loss: 13.697321891784668 Valid loss: 26.88687515258789\n",
      "Epoch: 334: Train loss: 13.67197322845459 Valid loss: 26.816383361816406\n",
      "Epoch: 335: Train loss: 13.64668083190918 Valid loss: 26.746070861816406\n",
      "Epoch: 336: Train loss: 13.621448516845703 Valid loss: 26.67594337463379\n",
      "Epoch: 337: Train loss: 13.59627628326416 Valid loss: 26.606006622314453\n",
      "Epoch: 338: Train loss: 13.571160316467285 Valid loss: 26.536264419555664\n",
      "Epoch: 339: Train loss: 13.54610538482666 Valid loss: 26.466693878173828\n",
      "Epoch: 340: Train loss: 13.521109580993652 Valid loss: 26.397300720214844\n",
      "Epoch: 341: Train loss: 13.49616813659668 Valid loss: 26.328102111816406\n",
      "Epoch: 342: Train loss: 13.471282958984375 Valid loss: 26.259084701538086\n",
      "Epoch: 343: Train loss: 13.446463584899902 Valid loss: 26.190242767333984\n",
      "Epoch: 344: Train loss: 13.42170238494873 Valid loss: 26.12158203125\n",
      "Epoch: 345: Train loss: 13.396991729736328 Valid loss: 26.053112030029297\n",
      "Epoch: 346: Train loss: 13.372340202331543 Valid loss: 25.984817504882812\n",
      "Epoch: 347: Train loss: 13.34775161743164 Valid loss: 25.916709899902344\n",
      "Epoch: 348: Train loss: 13.32321548461914 Valid loss: 25.84878158569336\n",
      "Epoch: 349: Train loss: 13.29874038696289 Valid loss: 25.781017303466797\n",
      "Epoch: 350: Train loss: 13.274317741394043 Valid loss: 25.71344757080078\n",
      "Epoch: 351: Train loss: 13.249958038330078 Valid loss: 25.64604949951172\n",
      "Epoch: 352: Train loss: 13.225650787353516 Valid loss: 25.578834533691406\n",
      "Epoch: 353: Train loss: 13.201401710510254 Valid loss: 25.51179313659668\n",
      "Epoch: 354: Train loss: 13.17720890045166 Valid loss: 25.444936752319336\n",
      "Epoch: 355: Train loss: 13.15307331085205 Valid loss: 25.37824249267578\n",
      "Epoch: 356: Train loss: 13.12899398803711 Valid loss: 25.31174087524414\n",
      "Epoch: 357: Train loss: 13.10496997833252 Valid loss: 25.245407104492188\n",
      "Epoch: 358: Train loss: 13.081002235412598 Valid loss: 25.17924690246582\n",
      "Epoch: 359: Train loss: 13.057089805603027 Valid loss: 25.113264083862305\n",
      "Epoch: 360: Train loss: 13.033233642578125 Valid loss: 25.04745101928711\n",
      "Epoch: 361: Train loss: 13.009434700012207 Valid loss: 24.981826782226562\n",
      "Epoch: 362: Train loss: 12.98569107055664 Valid loss: 24.916364669799805\n",
      "Epoch: 363: Train loss: 12.96199893951416 Valid loss: 24.851078033447266\n",
      "Epoch: 364: Train loss: 12.938369750976562 Valid loss: 24.785964965820312\n",
      "Epoch: 365: Train loss: 12.914787292480469 Valid loss: 24.721019744873047\n",
      "Epoch: 366: Train loss: 12.89126205444336 Valid loss: 24.65625\n",
      "Epoch: 367: Train loss: 12.867795944213867 Valid loss: 24.591651916503906\n",
      "Epoch: 368: Train loss: 12.844381332397461 Valid loss: 24.527233123779297\n",
      "Epoch: 369: Train loss: 12.821025848388672 Valid loss: 24.462974548339844\n",
      "Epoch: 370: Train loss: 12.797718048095703 Valid loss: 24.398887634277344\n",
      "Epoch: 371: Train loss: 12.774468421936035 Valid loss: 24.334978103637695\n",
      "Epoch: 372: Train loss: 12.751270294189453 Valid loss: 24.271228790283203\n",
      "Epoch: 373: Train loss: 12.728129386901855 Valid loss: 24.207656860351562\n",
      "Epoch: 374: Train loss: 12.705041885375977 Valid loss: 24.14425277709961\n",
      "Epoch: 375: Train loss: 12.6820068359375 Valid loss: 24.08100700378418\n",
      "Epoch: 376: Train loss: 12.659025192260742 Valid loss: 24.017932891845703\n",
      "Epoch: 377: Train loss: 12.636100769042969 Valid loss: 23.955028533935547\n",
      "Epoch: 378: Train loss: 12.613228797912598 Valid loss: 23.89229965209961\n",
      "Epoch: 379: Train loss: 12.590410232543945 Valid loss: 23.82972526550293\n",
      "Epoch: 380: Train loss: 12.567642211914062 Valid loss: 23.767318725585938\n",
      "Epoch: 381: Train loss: 12.544930458068848 Valid loss: 23.705093383789062\n",
      "Epoch: 382: Train loss: 12.522270202636719 Valid loss: 23.643016815185547\n",
      "Epoch: 383: Train loss: 12.49966049194336 Valid loss: 23.58111000061035\n",
      "Epoch: 384: Train loss: 12.477108001708984 Valid loss: 23.519359588623047\n",
      "Epoch: 385: Train loss: 12.454606056213379 Valid loss: 23.457778930664062\n",
      "Epoch: 386: Train loss: 12.432156562805176 Valid loss: 23.396366119384766\n",
      "Epoch: 387: Train loss: 12.40975570678711 Valid loss: 23.33510971069336\n",
      "Epoch: 388: Train loss: 12.38741397857666 Valid loss: 23.274032592773438\n",
      "Epoch: 389: Train loss: 12.36512279510498 Valid loss: 23.213104248046875\n",
      "Epoch: 390: Train loss: 12.34288215637207 Valid loss: 23.15234375\n",
      "Epoch: 391: Train loss: 12.320690155029297 Valid loss: 23.09173583984375\n",
      "Epoch: 392: Train loss: 12.298553466796875 Valid loss: 23.031299591064453\n",
      "Epoch: 393: Train loss: 12.276471138000488 Valid loss: 22.971023559570312\n",
      "Epoch: 394: Train loss: 12.254436492919922 Valid loss: 22.910903930664062\n",
      "Epoch: 395: Train loss: 12.232454299926758 Valid loss: 22.85093879699707\n",
      "Epoch: 396: Train loss: 12.210527420043945 Valid loss: 22.7911434173584\n",
      "Epoch: 397: Train loss: 12.188644409179688 Valid loss: 22.731502532958984\n",
      "Epoch: 398: Train loss: 12.166817665100098 Valid loss: 22.672019958496094\n",
      "Epoch: 399: Train loss: 12.145038604736328 Valid loss: 22.612703323364258\n",
      "Epoch: 400: Train loss: 12.123312950134277 Valid loss: 22.55353355407715\n",
      "Epoch: 401: Train loss: 12.10163688659668 Valid loss: 22.49452781677246\n",
      "Epoch: 402: Train loss: 12.080011367797852 Valid loss: 22.435680389404297\n",
      "Epoch: 403: Train loss: 12.058440208435059 Valid loss: 22.376983642578125\n",
      "Epoch: 404: Train loss: 12.03691291809082 Valid loss: 22.31844711303711\n",
      "Epoch: 405: Train loss: 12.015438079833984 Valid loss: 22.260072708129883\n",
      "Epoch: 406: Train loss: 11.994013786315918 Valid loss: 22.201858520507812\n",
      "Epoch: 407: Train loss: 11.97264289855957 Valid loss: 22.143789291381836\n",
      "Epoch: 408: Train loss: 11.95131778717041 Valid loss: 22.085872650146484\n",
      "Epoch: 409: Train loss: 11.930042266845703 Valid loss: 22.028118133544922\n",
      "Epoch: 410: Train loss: 11.908818244934082 Valid loss: 21.970504760742188\n",
      "Epoch: 411: Train loss: 11.88764476776123 Valid loss: 21.91305160522461\n",
      "Epoch: 412: Train loss: 11.86651611328125 Valid loss: 21.85576629638672\n",
      "Epoch: 413: Train loss: 11.845439910888672 Valid loss: 21.798622131347656\n",
      "Epoch: 414: Train loss: 11.824414253234863 Valid loss: 21.741636276245117\n",
      "Epoch: 415: Train loss: 11.803435325622559 Valid loss: 21.684799194335938\n",
      "Epoch: 416: Train loss: 11.782506942749023 Valid loss: 21.628110885620117\n",
      "Epoch: 417: Train loss: 11.761626243591309 Valid loss: 21.571582794189453\n",
      "Epoch: 418: Train loss: 11.74079418182373 Valid loss: 21.515195846557617\n",
      "Epoch: 419: Train loss: 11.720012664794922 Valid loss: 21.458972930908203\n",
      "Epoch: 420: Train loss: 11.699277877807617 Valid loss: 21.402883529663086\n",
      "Epoch: 421: Train loss: 11.678590774536133 Valid loss: 21.34695053100586\n",
      "Epoch: 422: Train loss: 11.657955169677734 Valid loss: 21.291168212890625\n",
      "Epoch: 423: Train loss: 11.63736629486084 Valid loss: 21.23553466796875\n",
      "Epoch: 424: Train loss: 11.616825103759766 Valid loss: 21.180051803588867\n",
      "Epoch: 425: Train loss: 11.596330642700195 Valid loss: 21.12472152709961\n",
      "Epoch: 426: Train loss: 11.575885772705078 Valid loss: 21.069536209106445\n",
      "Epoch: 427: Train loss: 11.555485725402832 Valid loss: 21.01449966430664\n",
      "Epoch: 428: Train loss: 11.535139083862305 Valid loss: 20.959606170654297\n",
      "Epoch: 429: Train loss: 11.5148344039917 Valid loss: 20.904861450195312\n",
      "Epoch: 430: Train loss: 11.494579315185547 Valid loss: 20.850269317626953\n",
      "Epoch: 431: Train loss: 11.474370002746582 Valid loss: 20.79581642150879\n",
      "Epoch: 432: Train loss: 11.454207420349121 Valid loss: 20.74151611328125\n",
      "Epoch: 433: Train loss: 11.43409538269043 Valid loss: 20.68735122680664\n",
      "Epoch: 434: Train loss: 11.414029121398926 Valid loss: 20.633338928222656\n",
      "Epoch: 435: Train loss: 11.394009590148926 Valid loss: 20.579471588134766\n",
      "Epoch: 436: Train loss: 11.37403392791748 Valid loss: 20.52574920654297\n",
      "Epoch: 437: Train loss: 11.354108810424805 Valid loss: 20.472166061401367\n",
      "Epoch: 438: Train loss: 11.334228515625 Valid loss: 20.418733596801758\n",
      "Epoch: 439: Train loss: 11.314396858215332 Valid loss: 20.36544418334961\n",
      "Epoch: 440: Train loss: 11.294605255126953 Valid loss: 20.312297821044922\n",
      "Epoch: 441: Train loss: 11.274861335754395 Valid loss: 20.259296417236328\n",
      "Epoch: 442: Train loss: 11.255169868469238 Valid loss: 20.20642852783203\n",
      "Epoch: 443: Train loss: 11.235519409179688 Valid loss: 20.153701782226562\n",
      "Epoch: 444: Train loss: 11.21591567993164 Valid loss: 20.101131439208984\n",
      "Epoch: 445: Train loss: 11.19636058807373 Valid loss: 20.048694610595703\n",
      "Epoch: 446: Train loss: 11.176847457885742 Valid loss: 19.996402740478516\n",
      "Epoch: 447: Train loss: 11.15738296508789 Valid loss: 19.94423484802246\n",
      "Epoch: 448: Train loss: 11.137961387634277 Valid loss: 19.892223358154297\n",
      "Epoch: 449: Train loss: 11.118586540222168 Valid loss: 19.840351104736328\n",
      "Epoch: 450: Train loss: 11.099254608154297 Valid loss: 19.78861427307129\n",
      "Epoch: 451: Train loss: 11.07996940612793 Valid loss: 19.737022399902344\n",
      "Epoch: 452: Train loss: 11.060728073120117 Valid loss: 19.685558319091797\n",
      "Epoch: 453: Train loss: 11.041534423828125 Valid loss: 19.63424301147461\n",
      "Epoch: 454: Train loss: 11.022382736206055 Valid loss: 19.583065032958984\n",
      "Epoch: 455: Train loss: 11.003276824951172 Valid loss: 19.53202247619629\n",
      "Epoch: 456: Train loss: 10.98421573638916 Valid loss: 19.48111343383789\n",
      "Epoch: 457: Train loss: 10.965198516845703 Valid loss: 19.430343627929688\n",
      "Epoch: 458: Train loss: 10.946226119995117 Valid loss: 19.379718780517578\n",
      "Epoch: 459: Train loss: 10.927301406860352 Valid loss: 19.3292236328125\n",
      "Epoch: 460: Train loss: 10.908415794372559 Valid loss: 19.27886962890625\n",
      "Epoch: 461: Train loss: 10.889575004577637 Valid loss: 19.22864532470703\n",
      "Epoch: 462: Train loss: 10.870780944824219 Valid loss: 19.178558349609375\n",
      "Epoch: 463: Train loss: 10.852028846740723 Valid loss: 19.128618240356445\n",
      "Epoch: 464: Train loss: 10.833322525024414 Valid loss: 19.078792572021484\n",
      "Epoch: 465: Train loss: 10.814657211303711 Valid loss: 19.02911376953125\n",
      "Epoch: 466: Train loss: 10.796037673950195 Valid loss: 18.979564666748047\n",
      "Epoch: 467: Train loss: 10.777457237243652 Valid loss: 18.93015480041504\n",
      "Epoch: 468: Train loss: 10.75892448425293 Valid loss: 18.880878448486328\n",
      "Epoch: 469: Train loss: 10.740437507629395 Valid loss: 18.83171844482422\n",
      "Epoch: 470: Train loss: 10.721988677978516 Valid loss: 18.78271484375\n",
      "Epoch: 471: Train loss: 10.703583717346191 Valid loss: 18.733829498291016\n",
      "Epoch: 472: Train loss: 10.685221672058105 Valid loss: 18.68508529663086\n",
      "Epoch: 473: Train loss: 10.666901588439941 Valid loss: 18.63646697998047\n",
      "Epoch: 474: Train loss: 10.648628234863281 Valid loss: 18.587982177734375\n",
      "Epoch: 475: Train loss: 10.630393981933594 Valid loss: 18.539627075195312\n",
      "Epoch: 476: Train loss: 10.612202644348145 Valid loss: 18.491397857666016\n",
      "Epoch: 477: Train loss: 10.59405517578125 Valid loss: 18.44330596923828\n",
      "Epoch: 478: Train loss: 10.575947761535645 Valid loss: 18.395357131958008\n",
      "Epoch: 479: Train loss: 10.55788516998291 Valid loss: 18.347511291503906\n",
      "Epoch: 480: Train loss: 10.539863586425781 Valid loss: 18.2998104095459\n",
      "Epoch: 481: Train loss: 10.521886825561523 Valid loss: 18.252243041992188\n",
      "Epoch: 482: Train loss: 10.503950119018555 Valid loss: 18.20479393005371\n",
      "Epoch: 483: Train loss: 10.486053466796875 Valid loss: 18.157482147216797\n",
      "Epoch: 484: Train loss: 10.468199729919434 Valid loss: 18.11029815673828\n",
      "Epoch: 485: Train loss: 10.450389862060547 Valid loss: 18.063243865966797\n",
      "Epoch: 486: Train loss: 10.432615280151367 Valid loss: 18.016305923461914\n",
      "Epoch: 487: Train loss: 10.41489028930664 Valid loss: 17.969497680664062\n",
      "Epoch: 488: Train loss: 10.397201538085938 Valid loss: 17.922828674316406\n",
      "Epoch: 489: Train loss: 10.379555702209473 Valid loss: 17.876270294189453\n",
      "Epoch: 490: Train loss: 10.36195182800293 Valid loss: 17.829849243164062\n",
      "Epoch: 491: Train loss: 10.344386100769043 Valid loss: 17.78354263305664\n",
      "Epoch: 492: Train loss: 10.326863288879395 Valid loss: 17.737380981445312\n",
      "Epoch: 493: Train loss: 10.309382438659668 Valid loss: 17.691329956054688\n",
      "Epoch: 494: Train loss: 10.29194164276123 Valid loss: 17.645408630371094\n",
      "Epoch: 495: Train loss: 10.27453899383545 Valid loss: 17.599620819091797\n",
      "Epoch: 496: Train loss: 10.257182121276855 Valid loss: 17.553936004638672\n",
      "Epoch: 497: Train loss: 10.239861488342285 Valid loss: 17.508392333984375\n",
      "Epoch: 498: Train loss: 10.222583770751953 Valid loss: 17.462970733642578\n",
      "Epoch: 499: Train loss: 10.205342292785645 Valid loss: 17.41767120361328\n",
      "Epoch: 500: Train loss: 10.188145637512207 Valid loss: 17.372493743896484\n",
      "Epoch: 501: Train loss: 10.170991897583008 Valid loss: 17.327434539794922\n",
      "Epoch: 502: Train loss: 10.153867721557617 Valid loss: 17.28249740600586\n",
      "Epoch: 503: Train loss: 10.13679027557373 Valid loss: 17.237703323364258\n",
      "Epoch: 504: Train loss: 10.119752883911133 Valid loss: 17.193010330200195\n",
      "Epoch: 505: Train loss: 10.10275650024414 Valid loss: 17.1484432220459\n",
      "Epoch: 506: Train loss: 10.085795402526855 Valid loss: 17.10399055480957\n",
      "Epoch: 507: Train loss: 10.068876266479492 Valid loss: 17.059673309326172\n",
      "Epoch: 508: Train loss: 10.051995277404785 Valid loss: 17.015472412109375\n",
      "Epoch: 509: Train loss: 10.035154342651367 Valid loss: 16.971393585205078\n",
      "Epoch: 510: Train loss: 10.018356323242188 Valid loss: 16.927433013916016\n",
      "Epoch: 511: Train loss: 10.001593589782715 Valid loss: 16.88358497619629\n",
      "Epoch: 512: Train loss: 9.984871864318848 Valid loss: 16.839859008789062\n",
      "Epoch: 513: Train loss: 9.968185424804688 Valid loss: 16.796266555786133\n",
      "Epoch: 514: Train loss: 9.951541900634766 Valid loss: 16.752769470214844\n",
      "Epoch: 515: Train loss: 9.934940338134766 Valid loss: 16.709407806396484\n",
      "Epoch: 516: Train loss: 9.91837215423584 Valid loss: 16.66617202758789\n",
      "Epoch: 517: Train loss: 9.90184211730957 Valid loss: 16.62303924560547\n",
      "Epoch: 518: Train loss: 9.885351181030273 Valid loss: 16.580032348632812\n",
      "Epoch: 519: Train loss: 9.868901252746582 Valid loss: 16.537137985229492\n",
      "Epoch: 520: Train loss: 9.852489471435547 Valid loss: 16.494361877441406\n",
      "Epoch: 521: Train loss: 9.836115837097168 Valid loss: 16.451702117919922\n",
      "Epoch: 522: Train loss: 9.819780349731445 Valid loss: 16.409162521362305\n",
      "Epoch: 523: Train loss: 9.803478240966797 Valid loss: 16.366744995117188\n",
      "Epoch: 524: Train loss: 9.78722095489502 Valid loss: 16.324426651000977\n",
      "Epoch: 525: Train loss: 9.77099895477295 Valid loss: 16.282241821289062\n",
      "Epoch: 526: Train loss: 9.754812240600586 Valid loss: 16.240154266357422\n",
      "Epoch: 527: Train loss: 9.738666534423828 Valid loss: 16.198192596435547\n",
      "Epoch: 528: Train loss: 9.72255802154541 Valid loss: 16.156347274780273\n",
      "Epoch: 529: Train loss: 9.706487655639648 Valid loss: 16.114612579345703\n",
      "Epoch: 530: Train loss: 9.690454483032227 Valid loss: 16.072988510131836\n",
      "Epoch: 531: Train loss: 9.674457550048828 Valid loss: 16.03148651123047\n",
      "Epoch: 532: Train loss: 9.65849781036377 Valid loss: 15.990097045898438\n",
      "Epoch: 533: Train loss: 9.642576217651367 Valid loss: 15.94882869720459\n",
      "Epoch: 534: Train loss: 9.626693725585938 Valid loss: 15.907663345336914\n",
      "Epoch: 535: Train loss: 9.610845565795898 Valid loss: 15.866613388061523\n",
      "Epoch: 536: Train loss: 9.5950345993042 Valid loss: 15.825664520263672\n",
      "Epoch: 537: Train loss: 9.579258918762207 Valid loss: 15.784845352172852\n",
      "Epoch: 538: Train loss: 9.563526153564453 Valid loss: 15.74412727355957\n",
      "Epoch: 539: Train loss: 9.54782485961914 Valid loss: 15.70352554321289\n",
      "Epoch: 540: Train loss: 9.532161712646484 Valid loss: 15.663036346435547\n",
      "Epoch: 541: Train loss: 9.516536712646484 Valid loss: 15.622648239135742\n",
      "Epoch: 542: Train loss: 9.500946044921875 Valid loss: 15.582387924194336\n",
      "Epoch: 543: Train loss: 9.485392570495605 Valid loss: 15.542228698730469\n",
      "Epoch: 544: Train loss: 9.469874382019043 Valid loss: 15.502179145812988\n",
      "Epoch: 545: Train loss: 9.45439338684082 Valid loss: 15.46224594116211\n",
      "Epoch: 546: Train loss: 9.438949584960938 Valid loss: 15.422412872314453\n",
      "Epoch: 547: Train loss: 9.423534393310547 Valid loss: 15.38270378112793\n",
      "Epoch: 548: Train loss: 9.408162117004395 Valid loss: 15.343088150024414\n",
      "Epoch: 549: Train loss: 9.392827033996582 Valid loss: 15.303590774536133\n",
      "Epoch: 550: Train loss: 9.37752628326416 Valid loss: 15.264189720153809\n",
      "Epoch: 551: Train loss: 9.36225700378418 Valid loss: 15.224910736083984\n",
      "Epoch: 552: Train loss: 9.347027778625488 Valid loss: 15.185736656188965\n",
      "Epoch: 553: Train loss: 9.331832885742188 Valid loss: 15.146669387817383\n",
      "Epoch: 554: Train loss: 9.316672325134277 Valid loss: 15.107714653015137\n",
      "Epoch: 555: Train loss: 9.301551818847656 Valid loss: 15.068857192993164\n",
      "Epoch: 556: Train loss: 9.28646469116211 Valid loss: 15.030115127563477\n",
      "Epoch: 557: Train loss: 9.271409034729004 Valid loss: 14.991480827331543\n",
      "Epoch: 558: Train loss: 9.256388664245605 Valid loss: 14.95295238494873\n",
      "Epoch: 559: Train loss: 9.241409301757812 Valid loss: 14.914525032043457\n",
      "Epoch: 560: Train loss: 9.226460456848145 Valid loss: 14.876203536987305\n",
      "Epoch: 561: Train loss: 9.2115478515625 Valid loss: 14.837994575500488\n",
      "Epoch: 562: Train loss: 9.196669578552246 Valid loss: 14.799882888793945\n",
      "Epoch: 563: Train loss: 9.181827545166016 Valid loss: 14.761877059936523\n",
      "Epoch: 564: Train loss: 9.167019844055176 Valid loss: 14.723989486694336\n",
      "Epoch: 565: Train loss: 9.152241706848145 Valid loss: 14.686192512512207\n",
      "Epoch: 566: Train loss: 9.137505531311035 Valid loss: 14.648503303527832\n",
      "Epoch: 567: Train loss: 9.122796058654785 Valid loss: 14.610925674438477\n",
      "Epoch: 568: Train loss: 9.108125686645508 Valid loss: 14.573443412780762\n",
      "Epoch: 569: Train loss: 9.093488693237305 Valid loss: 14.536065101623535\n",
      "Epoch: 570: Train loss: 9.078887939453125 Valid loss: 14.498786926269531\n",
      "Epoch: 571: Train loss: 9.064319610595703 Valid loss: 14.461626052856445\n",
      "Epoch: 572: Train loss: 9.049785614013672 Valid loss: 14.424556732177734\n",
      "Epoch: 573: Train loss: 9.035284996032715 Valid loss: 14.387587547302246\n",
      "Epoch: 574: Train loss: 9.020816802978516 Valid loss: 14.350738525390625\n",
      "Epoch: 575: Train loss: 9.00638484954834 Valid loss: 14.313970565795898\n",
      "Epoch: 576: Train loss: 8.991987228393555 Valid loss: 14.277318954467773\n",
      "Epoch: 577: Train loss: 8.977619171142578 Valid loss: 14.240753173828125\n",
      "Epoch: 578: Train loss: 8.96329116821289 Valid loss: 14.204301834106445\n",
      "Epoch: 579: Train loss: 8.94898796081543 Valid loss: 14.167947769165039\n",
      "Epoch: 580: Train loss: 8.934724807739258 Valid loss: 14.131702423095703\n",
      "Epoch: 581: Train loss: 8.920492172241211 Valid loss: 14.095542907714844\n",
      "Epoch: 582: Train loss: 8.906296730041504 Valid loss: 14.05949592590332\n",
      "Epoch: 583: Train loss: 8.892128944396973 Valid loss: 14.023542404174805\n",
      "Epoch: 584: Train loss: 8.877995491027832 Valid loss: 13.987691879272461\n",
      "Epoch: 585: Train loss: 8.863896369934082 Valid loss: 13.951935768127441\n",
      "Epoch: 586: Train loss: 8.849831581115723 Valid loss: 13.916282653808594\n",
      "Epoch: 587: Train loss: 8.835795402526855 Valid loss: 13.880729675292969\n",
      "Epoch: 588: Train loss: 8.821794509887695 Valid loss: 13.845279693603516\n",
      "Epoch: 589: Train loss: 8.80782699584961 Valid loss: 13.809919357299805\n",
      "Epoch: 590: Train loss: 8.79389476776123 Valid loss: 13.774660110473633\n",
      "Epoch: 591: Train loss: 8.779987335205078 Valid loss: 13.739500999450684\n",
      "Epoch: 592: Train loss: 8.76611614227295 Valid loss: 13.70444393157959\n",
      "Epoch: 593: Train loss: 8.752281188964844 Valid loss: 13.669478416442871\n",
      "Epoch: 594: Train loss: 8.738472938537598 Valid loss: 13.634608268737793\n",
      "Epoch: 595: Train loss: 8.724702835083008 Valid loss: 13.59984016418457\n",
      "Epoch: 596: Train loss: 8.710957527160645 Valid loss: 13.56517219543457\n",
      "Epoch: 597: Train loss: 8.697248458862305 Valid loss: 13.53059196472168\n",
      "Epoch: 598: Train loss: 8.683572769165039 Valid loss: 13.496110916137695\n",
      "Epoch: 599: Train loss: 8.669925689697266 Valid loss: 13.461729049682617\n",
      "Epoch: 600: Train loss: 8.656311988830566 Valid loss: 13.427434921264648\n",
      "Epoch: 601: Train loss: 8.642731666564941 Valid loss: 13.393239974975586\n",
      "Epoch: 602: Train loss: 8.629179954528809 Valid loss: 13.359147071838379\n",
      "Epoch: 603: Train loss: 8.61566162109375 Valid loss: 13.325132369995117\n",
      "Epoch: 604: Train loss: 8.602174758911133 Valid loss: 13.29123592376709\n",
      "Epoch: 605: Train loss: 8.58872127532959 Valid loss: 13.257421493530273\n",
      "Epoch: 606: Train loss: 8.575296401977539 Valid loss: 13.223705291748047\n",
      "Epoch: 607: Train loss: 8.561903953552246 Valid loss: 13.19008731842041\n",
      "Epoch: 608: Train loss: 8.548542976379395 Valid loss: 13.156553268432617\n",
      "Epoch: 609: Train loss: 8.535212516784668 Valid loss: 13.123114585876465\n",
      "Epoch: 610: Train loss: 8.521913528442383 Valid loss: 13.089775085449219\n",
      "Epoch: 611: Train loss: 8.508645057678223 Valid loss: 13.056524276733398\n",
      "Epoch: 612: Train loss: 8.495408058166504 Valid loss: 13.023370742797852\n",
      "Epoch: 613: Train loss: 8.482199668884277 Valid loss: 12.99030590057373\n",
      "Epoch: 614: Train loss: 8.469024658203125 Valid loss: 12.957331657409668\n",
      "Epoch: 615: Train loss: 8.455881118774414 Valid loss: 12.924455642700195\n",
      "Epoch: 616: Train loss: 8.442766189575195 Valid loss: 12.891666412353516\n",
      "Epoch: 617: Train loss: 8.429683685302734 Valid loss: 12.858975410461426\n",
      "Epoch: 618: Train loss: 8.416629791259766 Valid loss: 12.826362609863281\n",
      "Epoch: 619: Train loss: 8.403608322143555 Valid loss: 12.793851852416992\n",
      "Epoch: 620: Train loss: 8.39061450958252 Valid loss: 12.761438369750977\n",
      "Epoch: 621: Train loss: 8.377654075622559 Valid loss: 12.729104042053223\n",
      "Epoch: 622: Train loss: 8.36472225189209 Valid loss: 12.696863174438477\n",
      "Epoch: 623: Train loss: 8.35181999206543 Valid loss: 12.664713859558105\n",
      "Epoch: 624: Train loss: 8.338950157165527 Valid loss: 12.632658004760742\n",
      "Epoch: 625: Train loss: 8.326108932495117 Valid loss: 12.600683212280273\n",
      "Epoch: 626: Train loss: 8.313298225402832 Valid loss: 12.568798065185547\n",
      "Epoch: 627: Train loss: 8.300516128540039 Valid loss: 12.537015914916992\n",
      "Epoch: 628: Train loss: 8.287765502929688 Valid loss: 12.505311965942383\n",
      "Epoch: 629: Train loss: 8.275046348571777 Valid loss: 12.473709106445312\n",
      "Epoch: 630: Train loss: 8.262353897094727 Valid loss: 12.442184448242188\n",
      "Epoch: 631: Train loss: 8.2496919631958 Valid loss: 12.410752296447754\n",
      "Epoch: 632: Train loss: 8.237056732177734 Valid loss: 12.379413604736328\n",
      "Epoch: 633: Train loss: 8.224454879760742 Valid loss: 12.348153114318848\n",
      "Epoch: 634: Train loss: 8.211880683898926 Valid loss: 12.316983222961426\n",
      "Epoch: 635: Train loss: 8.199335098266602 Valid loss: 12.285893440246582\n",
      "Epoch: 636: Train loss: 8.186820030212402 Valid loss: 12.254907608032227\n",
      "Epoch: 637: Train loss: 8.174339294433594 Valid loss: 12.224002838134766\n",
      "Epoch: 638: Train loss: 8.16187858581543 Valid loss: 12.193172454833984\n",
      "Epoch: 639: Train loss: 8.149453163146973 Valid loss: 12.162445068359375\n",
      "Epoch: 640: Train loss: 8.137052536010742 Valid loss: 12.131795883178711\n",
      "Epoch: 641: Train loss: 8.12468147277832 Valid loss: 12.101236343383789\n",
      "Epoch: 642: Train loss: 8.11233901977539 Valid loss: 12.070769309997559\n",
      "Epoch: 643: Train loss: 8.100029945373535 Valid loss: 12.04037857055664\n",
      "Epoch: 644: Train loss: 8.087746620178223 Valid loss: 12.010078430175781\n",
      "Epoch: 645: Train loss: 8.075491905212402 Valid loss: 11.979866027832031\n",
      "Epoch: 646: Train loss: 8.063265800476074 Valid loss: 11.949728012084961\n",
      "Epoch: 647: Train loss: 8.051067352294922 Valid loss: 11.919681549072266\n",
      "Epoch: 648: Train loss: 8.038896560668945 Valid loss: 11.889724731445312\n",
      "Epoch: 649: Train loss: 8.026758193969727 Valid loss: 11.859848022460938\n",
      "Epoch: 650: Train loss: 8.014650344848633 Valid loss: 11.830049514770508\n",
      "Epoch: 651: Train loss: 8.0025634765625 Valid loss: 11.800342559814453\n",
      "Epoch: 652: Train loss: 7.990507125854492 Valid loss: 11.770727157592773\n",
      "Epoch: 653: Train loss: 7.97847843170166 Valid loss: 11.741189002990723\n",
      "Epoch: 654: Train loss: 7.966482162475586 Valid loss: 11.711727142333984\n",
      "Epoch: 655: Train loss: 7.954508304595947 Valid loss: 11.68236255645752\n",
      "Epoch: 656: Train loss: 7.942563533782959 Valid loss: 11.653068542480469\n",
      "Epoch: 657: Train loss: 7.930649757385254 Valid loss: 11.623861312866211\n",
      "Epoch: 658: Train loss: 7.91876220703125 Valid loss: 11.594734191894531\n",
      "Epoch: 659: Train loss: 7.90690279006958 Valid loss: 11.565696716308594\n",
      "Epoch: 660: Train loss: 7.895071983337402 Valid loss: 11.536739349365234\n",
      "Epoch: 661: Train loss: 7.883265972137451 Valid loss: 11.507866859436035\n",
      "Epoch: 662: Train loss: 7.871490478515625 Valid loss: 11.47906494140625\n",
      "Epoch: 663: Train loss: 7.859738826751709 Valid loss: 11.450357437133789\n",
      "Epoch: 664: Train loss: 7.848013877868652 Valid loss: 11.421720504760742\n",
      "Epoch: 665: Train loss: 7.836324214935303 Valid loss: 11.39316463470459\n",
      "Epoch: 666: Train loss: 7.824652671813965 Valid loss: 11.364700317382812\n",
      "Epoch: 667: Train loss: 7.813015460968018 Valid loss: 11.336313247680664\n",
      "Epoch: 668: Train loss: 7.801401138305664 Valid loss: 11.308006286621094\n",
      "Epoch: 669: Train loss: 7.789815425872803 Valid loss: 11.279773712158203\n",
      "Epoch: 670: Train loss: 7.778255939483643 Valid loss: 11.251629829406738\n",
      "Epoch: 671: Train loss: 7.7667236328125 Valid loss: 11.223563194274902\n",
      "Epoch: 672: Train loss: 7.755215167999268 Valid loss: 11.195581436157227\n",
      "Epoch: 673: Train loss: 7.743739604949951 Valid loss: 11.167673110961914\n",
      "Epoch: 674: Train loss: 7.7322869300842285 Valid loss: 11.139839172363281\n",
      "Epoch: 675: Train loss: 7.720864295959473 Valid loss: 11.112095832824707\n",
      "Epoch: 676: Train loss: 7.709465980529785 Valid loss: 11.084419250488281\n",
      "Epoch: 677: Train loss: 7.698091506958008 Valid loss: 11.05683708190918\n",
      "Epoch: 678: Train loss: 7.686751365661621 Valid loss: 11.029317855834961\n",
      "Epoch: 679: Train loss: 7.6754302978515625 Valid loss: 11.001884460449219\n",
      "Epoch: 680: Train loss: 7.664137840270996 Valid loss: 10.974531173706055\n",
      "Epoch: 681: Train loss: 7.652871131896973 Valid loss: 10.947257041931152\n",
      "Epoch: 682: Train loss: 7.641632080078125 Valid loss: 10.920059204101562\n",
      "Epoch: 683: Train loss: 7.63041877746582 Valid loss: 10.892934799194336\n",
      "Epoch: 684: Train loss: 7.619232177734375 Valid loss: 10.865880966186523\n",
      "Epoch: 685: Train loss: 7.608072757720947 Valid loss: 10.838927268981934\n",
      "Epoch: 686: Train loss: 7.5969367027282715 Valid loss: 10.812030792236328\n",
      "Epoch: 687: Train loss: 7.58582878112793 Valid loss: 10.785215377807617\n",
      "Epoch: 688: Train loss: 7.574745178222656 Valid loss: 10.75848388671875\n",
      "Epoch: 689: Train loss: 7.563692092895508 Valid loss: 10.731823921203613\n",
      "Epoch: 690: Train loss: 7.552656173706055 Valid loss: 10.705238342285156\n",
      "Epoch: 691: Train loss: 7.541654586791992 Valid loss: 10.678731918334961\n",
      "Epoch: 692: Train loss: 7.530673503875732 Valid loss: 10.652292251586914\n",
      "Epoch: 693: Train loss: 7.519716739654541 Valid loss: 10.625940322875977\n",
      "Epoch: 694: Train loss: 7.5087890625 Valid loss: 10.59965991973877\n",
      "Epoch: 695: Train loss: 7.497886657714844 Valid loss: 10.573455810546875\n",
      "Epoch: 696: Train loss: 7.487011432647705 Valid loss: 10.547323226928711\n",
      "Epoch: 697: Train loss: 7.476158142089844 Valid loss: 10.521270751953125\n",
      "Epoch: 698: Train loss: 7.46533203125 Valid loss: 10.495285034179688\n",
      "Epoch: 699: Train loss: 7.454526901245117 Valid loss: 10.469375610351562\n",
      "Epoch: 700: Train loss: 7.443752765655518 Valid loss: 10.443546295166016\n",
      "Epoch: 701: Train loss: 7.43300199508667 Valid loss: 10.417795181274414\n",
      "Epoch: 702: Train loss: 7.422276973724365 Valid loss: 10.392111778259277\n",
      "Epoch: 703: Train loss: 7.411576271057129 Valid loss: 10.366504669189453\n",
      "Epoch: 704: Train loss: 7.400900840759277 Valid loss: 10.340965270996094\n",
      "Epoch: 705: Train loss: 7.3902459144592285 Valid loss: 10.315500259399414\n",
      "Epoch: 706: Train loss: 7.3796234130859375 Valid loss: 10.290114402770996\n",
      "Epoch: 707: Train loss: 7.369019508361816 Valid loss: 10.264791488647461\n",
      "Epoch: 708: Train loss: 7.35844612121582 Valid loss: 10.23954963684082\n",
      "Epoch: 709: Train loss: 7.347893714904785 Valid loss: 10.214385986328125\n",
      "Epoch: 710: Train loss: 7.337366580963135 Valid loss: 10.189282417297363\n",
      "Epoch: 711: Train loss: 7.326863765716553 Valid loss: 10.16425895690918\n",
      "Epoch: 712: Train loss: 7.31638240814209 Valid loss: 10.13930892944336\n",
      "Epoch: 713: Train loss: 7.30592679977417 Valid loss: 10.114421844482422\n",
      "Epoch: 714: Train loss: 7.295499801635742 Valid loss: 10.089607238769531\n",
      "Epoch: 715: Train loss: 7.285093307495117 Valid loss: 10.064873695373535\n",
      "Epoch: 716: Train loss: 7.274713516235352 Valid loss: 10.040209770202637\n",
      "Epoch: 717: Train loss: 7.264357566833496 Valid loss: 10.015604019165039\n",
      "Epoch: 718: Train loss: 7.254025936126709 Valid loss: 9.99108600616455\n",
      "Epoch: 719: Train loss: 7.24371862411499 Valid loss: 9.966630935668945\n",
      "Epoch: 720: Train loss: 7.233432292938232 Valid loss: 9.942243576049805\n",
      "Epoch: 721: Train loss: 7.223171710968018 Valid loss: 9.917932510375977\n",
      "Epoch: 722: Train loss: 7.212935447692871 Valid loss: 9.893689155578613\n",
      "Epoch: 723: Train loss: 7.202724933624268 Valid loss: 9.869514465332031\n",
      "Epoch: 724: Train loss: 7.192533493041992 Valid loss: 9.845415115356445\n",
      "Epoch: 725: Train loss: 7.182373046875 Valid loss: 9.821382522583008\n",
      "Epoch: 726: Train loss: 7.172229290008545 Valid loss: 9.797420501708984\n",
      "Epoch: 727: Train loss: 7.162111759185791 Valid loss: 9.773529052734375\n",
      "Epoch: 728: Train loss: 7.15201997756958 Valid loss: 9.749706268310547\n",
      "Epoch: 729: Train loss: 7.141950607299805 Valid loss: 9.725945472717285\n",
      "Epoch: 730: Train loss: 7.131901264190674 Valid loss: 9.70225715637207\n",
      "Epoch: 731: Train loss: 7.121880531311035 Valid loss: 9.678640365600586\n",
      "Epoch: 732: Train loss: 7.111879825592041 Valid loss: 9.65509033203125\n",
      "Epoch: 733: Train loss: 7.101903438568115 Valid loss: 9.631617546081543\n",
      "Epoch: 734: Train loss: 7.091951370239258 Valid loss: 9.608194351196289\n",
      "Epoch: 735: Train loss: 7.08201789855957 Valid loss: 9.584844589233398\n",
      "Epoch: 736: Train loss: 7.072111129760742 Valid loss: 9.56156063079834\n",
      "Epoch: 737: Train loss: 7.062230110168457 Valid loss: 9.538355827331543\n",
      "Epoch: 738: Train loss: 7.052369117736816 Valid loss: 9.51522159576416\n",
      "Epoch: 739: Train loss: 7.042532920837402 Valid loss: 9.49213981628418\n",
      "Epoch: 740: Train loss: 7.032716751098633 Valid loss: 9.469127655029297\n",
      "Epoch: 741: Train loss: 7.022922039031982 Valid loss: 9.446186065673828\n",
      "Epoch: 742: Train loss: 7.013154029846191 Valid loss: 9.423311233520508\n",
      "Epoch: 743: Train loss: 7.003410339355469 Valid loss: 9.400504112243652\n",
      "Epoch: 744: Train loss: 6.993683338165283 Valid loss: 9.37775993347168\n",
      "Epoch: 745: Train loss: 6.983983516693115 Valid loss: 9.355083465576172\n",
      "Epoch: 746: Train loss: 6.974308490753174 Valid loss: 9.332475662231445\n",
      "Epoch: 747: Train loss: 6.96465539932251 Valid loss: 9.30992317199707\n",
      "Epoch: 748: Train loss: 6.955018043518066 Valid loss: 9.287455558776855\n",
      "Epoch: 749: Train loss: 6.945408344268799 Valid loss: 9.26504898071289\n",
      "Epoch: 750: Train loss: 6.935823440551758 Valid loss: 9.24270248413086\n",
      "Epoch: 751: Train loss: 6.926257133483887 Valid loss: 9.220417022705078\n",
      "Epoch: 752: Train loss: 6.916712284088135 Valid loss: 9.198205947875977\n",
      "Epoch: 753: Train loss: 6.907189846038818 Valid loss: 9.17604923248291\n",
      "Epoch: 754: Train loss: 6.897691249847412 Valid loss: 9.153960227966309\n",
      "Epoch: 755: Train loss: 6.888218402862549 Valid loss: 9.131938934326172\n",
      "Epoch: 756: Train loss: 6.878762245178223 Valid loss: 9.1099853515625\n",
      "Epoch: 757: Train loss: 6.869331359863281 Valid loss: 9.088088035583496\n",
      "Epoch: 758: Train loss: 6.859920978546143 Valid loss: 9.06625747680664\n",
      "Epoch: 759: Train loss: 6.850532531738281 Valid loss: 9.044493675231934\n",
      "Epoch: 760: Train loss: 6.841165542602539 Valid loss: 9.022794723510742\n",
      "Epoch: 761: Train loss: 6.831821918487549 Valid loss: 9.001148223876953\n",
      "Epoch: 762: Train loss: 6.822502613067627 Valid loss: 8.97957992553711\n",
      "Epoch: 763: Train loss: 6.813198566436768 Valid loss: 8.95806884765625\n",
      "Epoch: 764: Train loss: 6.803921222686768 Valid loss: 8.936626434326172\n",
      "Epoch: 765: Train loss: 6.794661521911621 Valid loss: 8.915237426757812\n",
      "Epoch: 766: Train loss: 6.785427570343018 Valid loss: 8.893913269042969\n",
      "Epoch: 767: Train loss: 6.776212215423584 Valid loss: 8.872659683227539\n",
      "Epoch: 768: Train loss: 6.767016887664795 Valid loss: 8.851456642150879\n",
      "Epoch: 769: Train loss: 6.757844924926758 Valid loss: 8.830322265625\n",
      "Epoch: 770: Train loss: 6.7487006187438965 Valid loss: 8.809248924255371\n",
      "Epoch: 771: Train loss: 6.739570140838623 Valid loss: 8.788230895996094\n",
      "Epoch: 772: Train loss: 6.730462074279785 Valid loss: 8.76728630065918\n",
      "Epoch: 773: Train loss: 6.721374988555908 Valid loss: 8.746397972106934\n",
      "Epoch: 774: Train loss: 6.712312698364258 Valid loss: 8.725576400756836\n",
      "Epoch: 775: Train loss: 6.703268051147461 Valid loss: 8.704814910888672\n",
      "Epoch: 776: Train loss: 6.694246292114258 Valid loss: 8.684106826782227\n",
      "Epoch: 777: Train loss: 6.68524694442749 Valid loss: 8.663466453552246\n",
      "Epoch: 778: Train loss: 6.676265239715576 Valid loss: 8.642890930175781\n",
      "Epoch: 779: Train loss: 6.667305946350098 Valid loss: 8.622367858886719\n",
      "Epoch: 780: Train loss: 6.658369064331055 Valid loss: 8.601905822753906\n",
      "Epoch: 781: Train loss: 6.649451732635498 Valid loss: 8.581507682800293\n",
      "Epoch: 782: Train loss: 6.640553951263428 Valid loss: 8.561163902282715\n",
      "Epoch: 783: Train loss: 6.631674766540527 Valid loss: 8.54088306427002\n",
      "Epoch: 784: Train loss: 6.62282133102417 Valid loss: 8.52066421508789\n",
      "Epoch: 785: Train loss: 6.613986492156982 Valid loss: 8.500507354736328\n",
      "Epoch: 786: Train loss: 6.605174541473389 Valid loss: 8.4804048538208\n",
      "Epoch: 787: Train loss: 6.596380710601807 Valid loss: 8.460358619689941\n",
      "Epoch: 788: Train loss: 6.587605953216553 Valid loss: 8.44037914276123\n",
      "Epoch: 789: Train loss: 6.57885217666626 Valid loss: 8.420466423034668\n",
      "Epoch: 790: Train loss: 6.570120334625244 Valid loss: 8.400594711303711\n",
      "Epoch: 791: Train loss: 6.561408996582031 Valid loss: 8.380788803100586\n",
      "Epoch: 792: Train loss: 6.552718162536621 Valid loss: 8.361045837402344\n",
      "Epoch: 793: Train loss: 6.544050216674805 Valid loss: 8.341360092163086\n",
      "Epoch: 794: Train loss: 6.535399436950684 Valid loss: 8.321722984313965\n",
      "Epoch: 795: Train loss: 6.52677059173584 Valid loss: 8.302156448364258\n",
      "Epoch: 796: Train loss: 6.518157958984375 Valid loss: 8.28264331817627\n",
      "Epoch: 797: Train loss: 6.509568214416504 Valid loss: 8.263193130493164\n",
      "Epoch: 798: Train loss: 6.500999450683594 Valid loss: 8.243797302246094\n",
      "Epoch: 799: Train loss: 6.492443561553955 Valid loss: 8.224465370178223\n",
      "Epoch: 800: Train loss: 6.483912944793701 Valid loss: 8.205175399780273\n",
      "Epoch: 801: Train loss: 6.475404262542725 Valid loss: 8.185952186584473\n",
      "Epoch: 802: Train loss: 6.466913223266602 Valid loss: 8.166784286499023\n",
      "Epoch: 803: Train loss: 6.458446502685547 Valid loss: 8.147672653198242\n",
      "Epoch: 804: Train loss: 6.449995517730713 Valid loss: 8.128626823425293\n",
      "Epoch: 805: Train loss: 6.441563606262207 Valid loss: 8.109626770019531\n",
      "Epoch: 806: Train loss: 6.43315315246582 Valid loss: 8.090681076049805\n",
      "Epoch: 807: Train loss: 6.424757957458496 Valid loss: 8.071802139282227\n",
      "Epoch: 808: Train loss: 6.416385173797607 Valid loss: 8.052980422973633\n",
      "Epoch: 809: Train loss: 6.408034324645996 Valid loss: 8.034199714660645\n",
      "Epoch: 810: Train loss: 6.399702072143555 Valid loss: 8.015487670898438\n",
      "Epoch: 811: Train loss: 6.391388893127441 Valid loss: 7.996828556060791\n",
      "Epoch: 812: Train loss: 6.383094787597656 Valid loss: 7.978229522705078\n",
      "Epoch: 813: Train loss: 6.374819278717041 Valid loss: 7.959667682647705\n",
      "Epoch: 814: Train loss: 6.366562366485596 Valid loss: 7.94118070602417\n",
      "Epoch: 815: Train loss: 6.35832405090332 Valid loss: 7.9227423667907715\n",
      "Epoch: 816: Train loss: 6.350107192993164 Valid loss: 7.904365062713623\n",
      "Epoch: 817: Train loss: 6.341908931732178 Valid loss: 7.88603401184082\n",
      "Epoch: 818: Train loss: 6.333731174468994 Valid loss: 7.867763519287109\n",
      "Epoch: 819: Train loss: 6.3255743980407715 Valid loss: 7.84954833984375\n",
      "Epoch: 820: Train loss: 6.31743049621582 Valid loss: 7.831390380859375\n",
      "Epoch: 821: Train loss: 6.309309482574463 Valid loss: 7.813279628753662\n",
      "Epoch: 822: Train loss: 6.301205158233643 Valid loss: 7.795222282409668\n",
      "Epoch: 823: Train loss: 6.293122291564941 Valid loss: 7.777225494384766\n",
      "Epoch: 824: Train loss: 6.285060405731201 Valid loss: 7.75928258895874\n",
      "Epoch: 825: Train loss: 6.277011871337891 Valid loss: 7.741387367248535\n",
      "Epoch: 826: Train loss: 6.268985271453857 Valid loss: 7.723545074462891\n",
      "Epoch: 827: Train loss: 6.2609734535217285 Valid loss: 7.705757141113281\n",
      "Epoch: 828: Train loss: 6.252987384796143 Valid loss: 7.688034534454346\n",
      "Epoch: 829: Train loss: 6.245016098022461 Valid loss: 7.670357704162598\n",
      "Epoch: 830: Train loss: 6.237061023712158 Valid loss: 7.652730941772461\n",
      "Epoch: 831: Train loss: 6.229127883911133 Valid loss: 7.6351518630981445\n",
      "Epoch: 832: Train loss: 6.221211910247803 Valid loss: 7.617635726928711\n",
      "Epoch: 833: Train loss: 6.213315963745117 Valid loss: 7.600172519683838\n",
      "Epoch: 834: Train loss: 6.205437660217285 Valid loss: 7.582756042480469\n",
      "Epoch: 835: Train loss: 6.197576522827148 Valid loss: 7.565402984619141\n",
      "Epoch: 836: Train loss: 6.18973445892334 Valid loss: 7.548092842102051\n",
      "Epoch: 837: Train loss: 6.18191385269165 Valid loss: 7.530832290649414\n",
      "Epoch: 838: Train loss: 6.174104690551758 Valid loss: 7.513628005981445\n",
      "Epoch: 839: Train loss: 6.166321754455566 Valid loss: 7.496479511260986\n",
      "Epoch: 840: Train loss: 6.158549785614014 Valid loss: 7.479381561279297\n",
      "Epoch: 841: Train loss: 6.15079927444458 Valid loss: 7.462332725524902\n",
      "Epoch: 842: Train loss: 6.143068313598633 Valid loss: 7.445331573486328\n",
      "Epoch: 843: Train loss: 6.135351181030273 Valid loss: 7.428380966186523\n",
      "Epoch: 844: Train loss: 6.127654075622559 Valid loss: 7.411492347717285\n",
      "Epoch: 845: Train loss: 6.119974136352539 Valid loss: 7.394644260406494\n",
      "Epoch: 846: Train loss: 6.112317085266113 Valid loss: 7.3778581619262695\n",
      "Epoch: 847: Train loss: 6.10467529296875 Valid loss: 7.361115455627441\n",
      "Epoch: 848: Train loss: 6.097047805786133 Valid loss: 7.344425201416016\n",
      "Epoch: 849: Train loss: 6.089440822601318 Valid loss: 7.327776908874512\n",
      "Epoch: 850: Train loss: 6.081851959228516 Valid loss: 7.311193466186523\n",
      "Epoch: 851: Train loss: 6.074276447296143 Valid loss: 7.294660568237305\n",
      "Epoch: 852: Train loss: 6.066725730895996 Valid loss: 7.278168678283691\n",
      "Epoch: 853: Train loss: 6.059187889099121 Valid loss: 7.261731147766113\n",
      "Epoch: 854: Train loss: 6.051670551300049 Valid loss: 7.245344161987305\n",
      "Epoch: 855: Train loss: 6.044172286987305 Valid loss: 7.229010105133057\n",
      "Epoch: 856: Train loss: 6.03668737411499 Valid loss: 7.212717533111572\n",
      "Epoch: 857: Train loss: 6.029221534729004 Valid loss: 7.196475505828857\n",
      "Epoch: 858: Train loss: 6.021771430969238 Valid loss: 7.180285453796387\n",
      "Epoch: 859: Train loss: 6.014337539672852 Valid loss: 7.164150238037109\n",
      "Epoch: 860: Train loss: 6.006926536560059 Valid loss: 7.1480560302734375\n",
      "Epoch: 861: Train loss: 5.9995293617248535 Valid loss: 7.132011890411377\n",
      "Epoch: 862: Train loss: 5.992147445678711 Valid loss: 7.116021156311035\n",
      "Epoch: 863: Train loss: 5.984785556793213 Valid loss: 7.100071907043457\n",
      "Epoch: 864: Train loss: 5.977442741394043 Valid loss: 7.084177017211914\n",
      "Epoch: 865: Train loss: 5.970115661621094 Valid loss: 7.068325042724609\n",
      "Epoch: 866: Train loss: 5.962800979614258 Valid loss: 7.052527904510498\n",
      "Epoch: 867: Train loss: 5.955508232116699 Valid loss: 7.036778450012207\n",
      "Epoch: 868: Train loss: 5.948231220245361 Valid loss: 7.021081924438477\n",
      "Epoch: 869: Train loss: 5.940973281860352 Valid loss: 7.005420684814453\n",
      "Epoch: 870: Train loss: 5.933732032775879 Valid loss: 6.989809513092041\n",
      "Epoch: 871: Train loss: 5.926503658294678 Valid loss: 6.974259376525879\n",
      "Epoch: 872: Train loss: 5.9192962646484375 Valid loss: 6.958752155303955\n",
      "Epoch: 873: Train loss: 5.912101745605469 Valid loss: 6.943285942077637\n",
      "Epoch: 874: Train loss: 5.904926776885986 Valid loss: 6.927868843078613\n",
      "Epoch: 875: Train loss: 5.897768020629883 Valid loss: 6.912492752075195\n",
      "Epoch: 876: Train loss: 5.890625953674316 Valid loss: 6.897177219390869\n",
      "Epoch: 877: Train loss: 5.8835015296936035 Valid loss: 6.881906032562256\n",
      "Epoch: 878: Train loss: 5.876394271850586 Valid loss: 6.866675853729248\n",
      "Epoch: 879: Train loss: 5.869302749633789 Valid loss: 6.851490020751953\n",
      "Epoch: 880: Train loss: 5.8622260093688965 Valid loss: 6.836358547210693\n",
      "Epoch: 881: Train loss: 5.855166912078857 Valid loss: 6.821271896362305\n",
      "Epoch: 882: Train loss: 5.848124027252197 Valid loss: 6.806230068206787\n",
      "Epoch: 883: Train loss: 5.841099262237549 Valid loss: 6.791230201721191\n",
      "Epoch: 884: Train loss: 5.834088325500488 Valid loss: 6.776287078857422\n",
      "Epoch: 885: Train loss: 5.827093124389648 Valid loss: 6.761387348175049\n",
      "Epoch: 886: Train loss: 5.820118427276611 Valid loss: 6.746532440185547\n",
      "Epoch: 887: Train loss: 5.81315803527832 Valid loss: 6.731710433959961\n",
      "Epoch: 888: Train loss: 5.806211471557617 Valid loss: 6.716949462890625\n",
      "Epoch: 889: Train loss: 5.799283504486084 Valid loss: 6.702233791351318\n",
      "Epoch: 890: Train loss: 5.792374610900879 Valid loss: 6.687558650970459\n",
      "Epoch: 891: Train loss: 5.785475730895996 Valid loss: 6.672928810119629\n",
      "Epoch: 892: Train loss: 5.778597354888916 Valid loss: 6.658337116241455\n",
      "Epoch: 893: Train loss: 5.771736145019531 Valid loss: 6.643803596496582\n",
      "Epoch: 894: Train loss: 5.764888763427734 Valid loss: 6.629317283630371\n",
      "Epoch: 895: Train loss: 5.758054733276367 Valid loss: 6.614866733551025\n",
      "Epoch: 896: Train loss: 5.751242637634277 Valid loss: 6.600464820861816\n",
      "Epoch: 897: Train loss: 5.744440078735352 Valid loss: 6.586109161376953\n",
      "Epoch: 898: Train loss: 5.737658977508545 Valid loss: 6.571803569793701\n",
      "Epoch: 899: Train loss: 5.730886459350586 Valid loss: 6.557523727416992\n",
      "Epoch: 900: Train loss: 5.724137783050537 Valid loss: 6.543295860290527\n",
      "Epoch: 901: Train loss: 5.717398643493652 Valid loss: 6.52911376953125\n",
      "Epoch: 902: Train loss: 5.7106781005859375 Valid loss: 6.514983654022217\n",
      "Epoch: 903: Train loss: 5.703975200653076 Valid loss: 6.500890731811523\n",
      "Epoch: 904: Train loss: 5.697285175323486 Valid loss: 6.486837387084961\n",
      "Epoch: 905: Train loss: 5.69060754776001 Valid loss: 6.4728193283081055\n",
      "Epoch: 906: Train loss: 5.6839494705200195 Valid loss: 6.458863258361816\n",
      "Epoch: 907: Train loss: 5.677310943603516 Valid loss: 6.444945812225342\n",
      "Epoch: 908: Train loss: 5.670682907104492 Valid loss: 6.431065559387207\n",
      "Epoch: 909: Train loss: 5.664071083068848 Valid loss: 6.417232513427734\n",
      "Epoch: 910: Train loss: 5.657473564147949 Valid loss: 6.403448104858398\n",
      "Epoch: 911: Train loss: 5.650892734527588 Valid loss: 6.389708995819092\n",
      "Epoch: 912: Train loss: 5.64432954788208 Valid loss: 6.375994682312012\n",
      "Epoch: 913: Train loss: 5.6377787590026855 Valid loss: 6.362344741821289\n",
      "Epoch: 914: Train loss: 5.631242752075195 Valid loss: 6.348719596862793\n",
      "Epoch: 915: Train loss: 5.624722003936768 Valid loss: 6.3351545333862305\n",
      "Epoch: 916: Train loss: 5.618219375610352 Valid loss: 6.3216166496276855\n",
      "Epoch: 917: Train loss: 5.611732006072998 Valid loss: 6.3081159591674805\n",
      "Epoch: 918: Train loss: 5.605255126953125 Valid loss: 6.29467248916626\n",
      "Epoch: 919: Train loss: 5.598794937133789 Valid loss: 6.281276702880859\n",
      "Epoch: 920: Train loss: 5.592353820800781 Valid loss: 6.267906665802002\n",
      "Epoch: 921: Train loss: 5.585920333862305 Valid loss: 6.254583358764648\n",
      "Epoch: 922: Train loss: 5.579508304595947 Valid loss: 6.241308689117432\n",
      "Epoch: 923: Train loss: 5.5731072425842285 Valid loss: 6.228068828582764\n",
      "Epoch: 924: Train loss: 5.5667266845703125 Valid loss: 6.214876651763916\n",
      "Epoch: 925: Train loss: 5.560357093811035 Valid loss: 6.201719760894775\n",
      "Epoch: 926: Train loss: 5.554005146026611 Valid loss: 6.188600063323975\n",
      "Epoch: 927: Train loss: 5.547664642333984 Valid loss: 6.175536632537842\n",
      "Epoch: 928: Train loss: 5.541338920593262 Valid loss: 6.16249942779541\n",
      "Epoch: 929: Train loss: 5.535028457641602 Valid loss: 6.14951229095459\n",
      "Epoch: 930: Train loss: 5.5287322998046875 Valid loss: 6.136560440063477\n",
      "Epoch: 931: Train loss: 5.522455215454102 Valid loss: 6.123652458190918\n",
      "Epoch: 932: Train loss: 5.516188621520996 Valid loss: 6.110788345336914\n",
      "Epoch: 933: Train loss: 5.5099358558654785 Valid loss: 6.097959518432617\n",
      "Epoch: 934: Train loss: 5.50369930267334 Valid loss: 6.0851664543151855\n",
      "Epoch: 935: Train loss: 5.497480869293213 Valid loss: 6.07242488861084\n",
      "Epoch: 936: Train loss: 5.491269588470459 Valid loss: 6.059717655181885\n",
      "Epoch: 937: Train loss: 5.485078811645508 Valid loss: 6.047055721282959\n",
      "Epoch: 938: Train loss: 5.478898525238037 Valid loss: 6.034427642822266\n",
      "Epoch: 939: Train loss: 5.47273588180542 Valid loss: 6.021841526031494\n",
      "Epoch: 940: Train loss: 5.466585159301758 Valid loss: 6.009289741516113\n",
      "Epoch: 941: Train loss: 5.460451602935791 Valid loss: 5.9967827796936035\n",
      "Epoch: 942: Train loss: 5.454329013824463 Valid loss: 5.984309673309326\n",
      "Epoch: 943: Train loss: 5.448221206665039 Valid loss: 5.971892356872559\n",
      "Epoch: 944: Train loss: 5.442131996154785 Valid loss: 5.959510803222656\n",
      "Epoch: 945: Train loss: 5.436050891876221 Valid loss: 5.947155952453613\n",
      "Epoch: 946: Train loss: 5.429991722106934 Valid loss: 5.934836387634277\n",
      "Epoch: 947: Train loss: 5.42393684387207 Valid loss: 5.922576904296875\n",
      "Epoch: 948: Train loss: 5.41790246963501 Valid loss: 5.910340309143066\n",
      "Epoch: 949: Train loss: 5.411882400512695 Valid loss: 5.898153781890869\n",
      "Epoch: 950: Train loss: 5.405872344970703 Valid loss: 5.886001110076904\n",
      "Epoch: 951: Train loss: 5.399879455566406 Valid loss: 5.873880863189697\n",
      "Epoch: 952: Train loss: 5.393898963928223 Valid loss: 5.86181640625\n",
      "Epoch: 953: Train loss: 5.38793420791626 Valid loss: 5.84977912902832\n",
      "Epoch: 954: Train loss: 5.381984233856201 Valid loss: 5.837775230407715\n",
      "Epoch: 955: Train loss: 5.376047134399414 Valid loss: 5.8258161544799805\n",
      "Epoch: 956: Train loss: 5.37012243270874 Valid loss: 5.813889503479004\n",
      "Epoch: 957: Train loss: 5.36421012878418 Valid loss: 5.802006721496582\n",
      "Epoch: 958: Train loss: 5.358316421508789 Valid loss: 5.79015588760376\n",
      "Epoch: 959: Train loss: 5.3524322509765625 Valid loss: 5.77834415435791\n",
      "Epoch: 960: Train loss: 5.346565246582031 Valid loss: 5.766575336456299\n",
      "Epoch: 961: Train loss: 5.340707302093506 Valid loss: 5.754844665527344\n",
      "Epoch: 962: Train loss: 5.334866523742676 Valid loss: 5.743146896362305\n",
      "Epoch: 963: Train loss: 5.329038619995117 Valid loss: 5.7314910888671875\n",
      "Epoch: 964: Train loss: 5.3232245445251465 Valid loss: 5.719878196716309\n",
      "Epoch: 965: Train loss: 5.31742525100708 Valid loss: 5.708292484283447\n",
      "Epoch: 966: Train loss: 5.311636924743652 Valid loss: 5.696751117706299\n",
      "Epoch: 967: Train loss: 5.305864334106445 Valid loss: 5.685234546661377\n",
      "Epoch: 968: Train loss: 5.300104141235352 Valid loss: 5.673761367797852\n",
      "Epoch: 969: Train loss: 5.2943596839904785 Valid loss: 5.662330150604248\n",
      "Epoch: 970: Train loss: 5.288625717163086 Valid loss: 5.6509294509887695\n",
      "Epoch: 971: Train loss: 5.282903671264648 Valid loss: 5.639562129974365\n",
      "Epoch: 972: Train loss: 5.2771992683410645 Valid loss: 5.628247261047363\n",
      "Epoch: 973: Train loss: 5.271503448486328 Valid loss: 5.616952896118164\n",
      "Epoch: 974: Train loss: 5.265825271606445 Valid loss: 5.605700492858887\n",
      "Epoch: 975: Train loss: 5.260159969329834 Valid loss: 5.594487190246582\n",
      "Epoch: 976: Train loss: 5.254505634307861 Valid loss: 5.583313465118408\n",
      "Epoch: 977: Train loss: 5.248865127563477 Valid loss: 5.5721755027771\n",
      "Epoch: 978: Train loss: 5.243238925933838 Valid loss: 5.561069965362549\n",
      "Epoch: 979: Train loss: 5.2376251220703125 Valid loss: 5.549994468688965\n",
      "Epoch: 980: Train loss: 5.232025146484375 Valid loss: 5.538955211639404\n",
      "Epoch: 981: Train loss: 5.226436614990234 Valid loss: 5.527957439422607\n",
      "Epoch: 982: Train loss: 5.220860004425049 Valid loss: 5.516996383666992\n",
      "Epoch: 983: Train loss: 5.215300559997559 Valid loss: 5.506064414978027\n",
      "Epoch: 984: Train loss: 5.209750652313232 Valid loss: 5.495173454284668\n",
      "Epoch: 985: Train loss: 5.204211711883545 Valid loss: 5.484314441680908\n",
      "Epoch: 986: Train loss: 5.198690414428711 Valid loss: 5.473484992980957\n",
      "Epoch: 987: Train loss: 5.193182945251465 Valid loss: 5.46270751953125\n",
      "Epoch: 988: Train loss: 5.18768310546875 Valid loss: 5.451953411102295\n",
      "Epoch: 989: Train loss: 5.182200908660889 Valid loss: 5.441242218017578\n",
      "Epoch: 990: Train loss: 5.176729679107666 Valid loss: 5.430553913116455\n",
      "Epoch: 991: Train loss: 5.171270370483398 Valid loss: 5.419906139373779\n",
      "Epoch: 992: Train loss: 5.1658244132995605 Valid loss: 5.409295558929443\n",
      "Epoch: 993: Train loss: 5.160390853881836 Valid loss: 5.39872407913208\n",
      "Epoch: 994: Train loss: 5.154969215393066 Valid loss: 5.388166427612305\n",
      "Epoch: 995: Train loss: 5.149564266204834 Valid loss: 5.377669334411621\n",
      "Epoch: 996: Train loss: 5.144169330596924 Valid loss: 5.367191314697266\n",
      "Epoch: 997: Train loss: 5.138784885406494 Valid loss: 5.356753349304199\n",
      "Epoch: 998: Train loss: 5.13341760635376 Valid loss: 5.346343517303467\n",
      "Epoch: 999: Train loss: 5.128057479858398 Valid loss: 5.335964679718018\n",
      "Epoch: 1000: Train loss: 5.122713088989258 Valid loss: 5.32563591003418\n",
      "Epoch: 1001: Train loss: 5.117380142211914 Valid loss: 5.315324783325195\n",
      "Epoch: 1002: Train loss: 5.112059593200684 Valid loss: 5.3050642013549805\n",
      "Epoch: 1003: Train loss: 5.106752395629883 Valid loss: 5.294826507568359\n",
      "Epoch: 1004: Train loss: 5.1014556884765625 Valid loss: 5.284628868103027\n",
      "Epoch: 1005: Train loss: 5.0961761474609375 Valid loss: 5.2744550704956055\n",
      "Epoch: 1006: Train loss: 5.0909037590026855 Valid loss: 5.264318943023682\n",
      "Epoch: 1007: Train loss: 5.085644245147705 Valid loss: 5.254217147827148\n",
      "Epoch: 1008: Train loss: 5.080400466918945 Valid loss: 5.244144916534424\n",
      "Epoch: 1009: Train loss: 5.075165748596191 Valid loss: 5.234106063842773\n",
      "Epoch: 1010: Train loss: 5.069943428039551 Valid loss: 5.224105358123779\n",
      "Epoch: 1011: Train loss: 5.064737319946289 Valid loss: 5.214139461517334\n",
      "Epoch: 1012: Train loss: 5.059539318084717 Valid loss: 5.20419454574585\n",
      "Epoch: 1013: Train loss: 5.054352760314941 Valid loss: 5.194293975830078\n",
      "Epoch: 1014: Train loss: 5.049178123474121 Valid loss: 5.184426307678223\n",
      "Epoch: 1015: Train loss: 5.044018268585205 Valid loss: 5.174577236175537\n",
      "Epoch: 1016: Train loss: 5.038872241973877 Valid loss: 5.164772033691406\n",
      "Epoch: 1017: Train loss: 5.033735752105713 Valid loss: 5.155003547668457\n",
      "Epoch: 1018: Train loss: 5.0286102294921875 Valid loss: 5.145258903503418\n",
      "Epoch: 1019: Train loss: 5.023497581481934 Valid loss: 5.135550498962402\n",
      "Epoch: 1020: Train loss: 5.01839542388916 Valid loss: 5.125875949859619\n",
      "Epoch: 1021: Train loss: 5.0133056640625 Valid loss: 5.116229057312012\n",
      "Epoch: 1022: Train loss: 5.008227825164795 Valid loss: 5.10661506652832\n",
      "Epoch: 1023: Train loss: 5.003162860870361 Valid loss: 5.0970377922058105\n",
      "Epoch: 1024: Train loss: 4.998109817504883 Valid loss: 5.087478160858154\n",
      "Epoch: 1025: Train loss: 4.993069648742676 Valid loss: 5.077976226806641\n",
      "Epoch: 1026: Train loss: 4.988039970397949 Valid loss: 5.068480968475342\n",
      "Epoch: 1027: Train loss: 4.98301887512207 Valid loss: 5.059022903442383\n",
      "Epoch: 1028: Train loss: 4.97801399230957 Valid loss: 5.049612045288086\n",
      "Epoch: 1029: Train loss: 4.973020553588867 Valid loss: 5.040217876434326\n",
      "Epoch: 1030: Train loss: 4.9680376052856445 Valid loss: 5.030856609344482\n",
      "Epoch: 1031: Train loss: 4.963064193725586 Valid loss: 5.021530628204346\n",
      "Epoch: 1032: Train loss: 4.958102703094482 Valid loss: 5.0122270584106445\n",
      "Epoch: 1033: Train loss: 4.953155994415283 Valid loss: 5.002960205078125\n",
      "Epoch: 1034: Train loss: 4.948217868804932 Valid loss: 4.993725776672363\n",
      "Epoch: 1035: Train loss: 4.943295955657959 Valid loss: 4.984527587890625\n",
      "Epoch: 1036: Train loss: 4.938379764556885 Valid loss: 4.9753499031066895\n",
      "Epoch: 1037: Train loss: 4.933478355407715 Valid loss: 4.966209411621094\n",
      "Epoch: 1038: Train loss: 4.928591251373291 Valid loss: 4.9571003913879395\n",
      "Epoch: 1039: Train loss: 4.923710346221924 Valid loss: 4.948016166687012\n",
      "Epoch: 1040: Train loss: 4.91884183883667 Valid loss: 4.938965797424316\n",
      "Epoch: 1041: Train loss: 4.913984298706055 Valid loss: 4.929947853088379\n",
      "Epoch: 1042: Train loss: 4.909139633178711 Valid loss: 4.920958042144775\n",
      "Epoch: 1043: Train loss: 4.904304027557373 Valid loss: 4.9120001792907715\n",
      "Epoch: 1044: Train loss: 4.899483680725098 Valid loss: 4.903066158294678\n",
      "Epoch: 1045: Train loss: 4.894671440124512 Valid loss: 4.894168376922607\n",
      "Epoch: 1046: Train loss: 4.889870643615723 Valid loss: 4.885295391082764\n",
      "Epoch: 1047: Train loss: 4.885080814361572 Valid loss: 4.87645959854126\n",
      "Epoch: 1048: Train loss: 4.880304336547852 Valid loss: 4.867649078369141\n",
      "Epoch: 1049: Train loss: 4.875537872314453 Valid loss: 4.85886812210083\n",
      "Epoch: 1050: Train loss: 4.87078332901001 Valid loss: 4.850123405456543\n",
      "Epoch: 1051: Train loss: 4.866039276123047 Valid loss: 4.841403961181641\n",
      "Epoch: 1052: Train loss: 4.861303329467773 Valid loss: 4.832719802856445\n",
      "Epoch: 1053: Train loss: 4.856584548950195 Valid loss: 4.824059963226318\n",
      "Epoch: 1054: Train loss: 4.851871013641357 Valid loss: 4.815420627593994\n",
      "Epoch: 1055: Train loss: 4.847173690795898 Valid loss: 4.806817054748535\n",
      "Epoch: 1056: Train loss: 4.842482566833496 Valid loss: 4.798242568969727\n",
      "Epoch: 1057: Train loss: 4.837804794311523 Valid loss: 4.78970193862915\n",
      "Epoch: 1058: Train loss: 4.833139419555664 Valid loss: 4.78118896484375\n",
      "Epoch: 1059: Train loss: 4.828483581542969 Valid loss: 4.772698402404785\n",
      "Epoch: 1060: Train loss: 4.8238372802734375 Valid loss: 4.7642436027526855\n",
      "Epoch: 1061: Train loss: 4.8192033767700195 Valid loss: 4.7558112144470215\n",
      "Epoch: 1062: Train loss: 4.814577579498291 Valid loss: 4.747415065765381\n",
      "Epoch: 1063: Train loss: 4.809964656829834 Valid loss: 4.739048480987549\n",
      "Epoch: 1064: Train loss: 4.805359840393066 Valid loss: 4.730706214904785\n",
      "Epoch: 1065: Train loss: 4.80076789855957 Valid loss: 4.722386837005615\n",
      "Epoch: 1066: Train loss: 4.796186923980713 Valid loss: 4.714102745056152\n",
      "Epoch: 1067: Train loss: 4.791616439819336 Valid loss: 4.70584774017334\n",
      "Epoch: 1068: Train loss: 4.787058353424072 Valid loss: 4.697617530822754\n",
      "Epoch: 1069: Train loss: 4.782510757446289 Valid loss: 4.689414978027344\n",
      "Epoch: 1070: Train loss: 4.7779693603515625 Valid loss: 4.681242942810059\n",
      "Epoch: 1071: Train loss: 4.773443222045898 Valid loss: 4.673098087310791\n",
      "Epoch: 1072: Train loss: 4.768927097320557 Valid loss: 4.664987564086914\n",
      "Epoch: 1073: Train loss: 4.764421463012695 Valid loss: 4.656900405883789\n",
      "Epoch: 1074: Train loss: 4.759925365447998 Valid loss: 4.648836612701416\n",
      "Epoch: 1075: Train loss: 4.755436897277832 Valid loss: 4.640801906585693\n",
      "Epoch: 1076: Train loss: 4.7509613037109375 Valid loss: 4.632805824279785\n",
      "Epoch: 1077: Train loss: 4.746499538421631 Valid loss: 4.6248273849487305\n",
      "Epoch: 1078: Train loss: 4.742044925689697 Valid loss: 4.616879463195801\n",
      "Epoch: 1079: Train loss: 4.737603187561035 Valid loss: 4.60895299911499\n",
      "Epoch: 1080: Train loss: 4.7331671714782715 Valid loss: 4.601055145263672\n",
      "Epoch: 1081: Train loss: 4.728745460510254 Valid loss: 4.5931901931762695\n",
      "Epoch: 1082: Train loss: 4.724333763122559 Valid loss: 4.585348606109619\n",
      "Epoch: 1083: Train loss: 4.719929218292236 Valid loss: 4.577540874481201\n",
      "Epoch: 1084: Train loss: 4.715538024902344 Valid loss: 4.56974983215332\n",
      "Epoch: 1085: Train loss: 4.711157321929932 Valid loss: 4.5619916915893555\n",
      "Epoch: 1086: Train loss: 4.706784248352051 Valid loss: 4.554266452789307\n",
      "Epoch: 1087: Train loss: 4.702425003051758 Valid loss: 4.546554088592529\n",
      "Epoch: 1088: Train loss: 4.698073387145996 Valid loss: 4.538875102996826\n",
      "Epoch: 1089: Train loss: 4.693732261657715 Valid loss: 4.531228542327881\n",
      "Epoch: 1090: Train loss: 4.689401626586914 Valid loss: 4.523597717285156\n",
      "Epoch: 1091: Train loss: 4.6850786209106445 Valid loss: 4.516010761260986\n",
      "Epoch: 1092: Train loss: 4.680768966674805 Valid loss: 4.508434772491455\n",
      "Epoch: 1093: Train loss: 4.67646598815918 Valid loss: 4.500881671905518\n",
      "Epoch: 1094: Train loss: 4.672174453735352 Valid loss: 4.493371963500977\n",
      "Epoch: 1095: Train loss: 4.667893886566162 Valid loss: 4.485872745513916\n",
      "Epoch: 1096: Train loss: 4.663626670837402 Valid loss: 4.47841739654541\n",
      "Epoch: 1097: Train loss: 4.659361839294434 Valid loss: 4.470973491668701\n",
      "Epoch: 1098: Train loss: 4.655115127563477 Valid loss: 4.463551044464111\n",
      "Epoch: 1099: Train loss: 4.6508708000183105 Valid loss: 4.45617151260376\n",
      "Epoch: 1100: Train loss: 4.64664363861084 Valid loss: 4.448802947998047\n",
      "Epoch: 1101: Train loss: 4.64241886138916 Valid loss: 4.441476345062256\n",
      "Epoch: 1102: Train loss: 4.63820743560791 Valid loss: 4.43416166305542\n",
      "Epoch: 1103: Train loss: 4.63400936126709 Valid loss: 4.426877975463867\n",
      "Epoch: 1104: Train loss: 4.62981653213501 Valid loss: 4.419616222381592\n",
      "Epoch: 1105: Train loss: 4.625635623931885 Valid loss: 4.41238260269165\n",
      "Epoch: 1106: Train loss: 4.621462345123291 Valid loss: 4.405178070068359\n",
      "Epoch: 1107: Train loss: 4.6172990798950195 Valid loss: 4.397995948791504\n",
      "Epoch: 1108: Train loss: 4.6131463050842285 Valid loss: 4.390834331512451\n",
      "Epoch: 1109: Train loss: 4.609004020690918 Valid loss: 4.383705139160156\n",
      "Epoch: 1110: Train loss: 4.604874134063721 Valid loss: 4.37659215927124\n",
      "Epoch: 1111: Train loss: 4.6007490158081055 Valid loss: 4.369514465332031\n",
      "Epoch: 1112: Train loss: 4.596632480621338 Valid loss: 4.362458229064941\n",
      "Epoch: 1113: Train loss: 4.592529296875 Valid loss: 4.355432987213135\n",
      "Epoch: 1114: Train loss: 4.588433742523193 Valid loss: 4.348423957824707\n",
      "Epoch: 1115: Train loss: 4.5843505859375 Valid loss: 4.341439723968506\n",
      "Epoch: 1116: Train loss: 4.580271244049072 Valid loss: 4.3344879150390625\n",
      "Epoch: 1117: Train loss: 4.576207637786865 Valid loss: 4.327551364898682\n",
      "Epoch: 1118: Train loss: 4.5721516609191895 Valid loss: 4.320644855499268\n",
      "Epoch: 1119: Train loss: 4.568103313446045 Valid loss: 4.3137640953063965\n",
      "Epoch: 1120: Train loss: 4.564065456390381 Valid loss: 4.306904315948486\n",
      "Epoch: 1121: Train loss: 4.560037612915039 Valid loss: 4.3000688552856445\n",
      "Epoch: 1122: Train loss: 4.5560173988342285 Valid loss: 4.293264865875244\n",
      "Epoch: 1123: Train loss: 4.552009105682373 Valid loss: 4.2864813804626465\n",
      "Epoch: 1124: Train loss: 4.548009395599365 Valid loss: 4.279717922210693\n",
      "Epoch: 1125: Train loss: 4.544018745422363 Valid loss: 4.272985458374023\n",
      "Epoch: 1126: Train loss: 4.540037631988525 Valid loss: 4.266266822814941\n",
      "Epoch: 1127: Train loss: 4.536066055297852 Valid loss: 4.259579181671143\n",
      "Epoch: 1128: Train loss: 4.53209924697876 Valid loss: 4.252911567687988\n",
      "Epoch: 1129: Train loss: 4.528148174285889 Valid loss: 4.246273040771484\n",
      "Epoch: 1130: Train loss: 4.524202823638916 Valid loss: 4.239660739898682\n",
      "Epoch: 1131: Train loss: 4.520266532897949 Valid loss: 4.233067989349365\n",
      "Epoch: 1132: Train loss: 4.516343116760254 Valid loss: 4.226505279541016\n",
      "Epoch: 1133: Train loss: 4.512423038482666 Valid loss: 4.219951152801514\n",
      "Epoch: 1134: Train loss: 4.508516788482666 Valid loss: 4.213438034057617\n",
      "Epoch: 1135: Train loss: 4.5046186447143555 Valid loss: 4.206943988800049\n",
      "Epoch: 1136: Train loss: 4.50072717666626 Valid loss: 4.200459003448486\n",
      "Epoch: 1137: Train loss: 4.496850490570068 Valid loss: 4.194014549255371\n",
      "Epoch: 1138: Train loss: 4.49297571182251 Valid loss: 4.187583923339844\n",
      "Epoch: 1139: Train loss: 4.489113807678223 Valid loss: 4.181182384490967\n",
      "Epoch: 1140: Train loss: 4.485260009765625 Valid loss: 4.174805641174316\n",
      "Epoch: 1141: Train loss: 4.481417655944824 Valid loss: 4.168447971343994\n",
      "Epoch: 1142: Train loss: 4.477581977844238 Valid loss: 4.162109851837158\n",
      "Epoch: 1143: Train loss: 4.473753929138184 Valid loss: 4.155800819396973\n",
      "Epoch: 1144: Train loss: 4.469937801361084 Valid loss: 4.149510383605957\n",
      "Epoch: 1145: Train loss: 4.466128349304199 Valid loss: 4.143239498138428\n",
      "Epoch: 1146: Train loss: 4.462328910827637 Valid loss: 4.137002468109131\n",
      "Epoch: 1147: Train loss: 4.458536148071289 Valid loss: 4.13078498840332\n",
      "Epoch: 1148: Train loss: 4.454756736755371 Valid loss: 4.124580383300781\n",
      "Epoch: 1149: Train loss: 4.450984954833984 Valid loss: 4.118409156799316\n",
      "Epoch: 1150: Train loss: 4.447221279144287 Valid loss: 4.112258434295654\n",
      "Epoch: 1151: Train loss: 4.443464279174805 Valid loss: 4.106130599975586\n",
      "Epoch: 1152: Train loss: 4.439718723297119 Valid loss: 4.100020885467529\n",
      "Epoch: 1153: Train loss: 4.435981273651123 Valid loss: 4.093935489654541\n",
      "Epoch: 1154: Train loss: 4.432251453399658 Valid loss: 4.0878729820251465\n",
      "Epoch: 1155: Train loss: 4.428529739379883 Valid loss: 4.081839084625244\n",
      "Epoch: 1156: Train loss: 4.42481803894043 Valid loss: 4.0758185386657715\n",
      "Epoch: 1157: Train loss: 4.421111583709717 Valid loss: 4.069831371307373\n",
      "Epoch: 1158: Train loss: 4.417418956756592 Valid loss: 4.0638532638549805\n",
      "Epoch: 1159: Train loss: 4.413733959197998 Valid loss: 4.0579023361206055\n",
      "Epoch: 1160: Train loss: 4.410055160522461 Valid loss: 4.051970481872559\n",
      "Epoch: 1161: Train loss: 4.406387805938721 Valid loss: 4.0460615158081055\n",
      "Epoch: 1162: Train loss: 4.402727127075195 Valid loss: 4.04017972946167\n",
      "Epoch: 1163: Train loss: 4.399073600769043 Valid loss: 4.034311771392822\n",
      "Epoch: 1164: Train loss: 4.395434379577637 Valid loss: 4.02847146987915\n",
      "Epoch: 1165: Train loss: 4.391796588897705 Valid loss: 4.0226545333862305\n",
      "Epoch: 1166: Train loss: 4.388171672821045 Valid loss: 4.016854763031006\n",
      "Epoch: 1167: Train loss: 4.3845534324646 Valid loss: 4.0110883712768555\n",
      "Epoch: 1168: Train loss: 4.38094425201416 Valid loss: 4.005329608917236\n",
      "Epoch: 1169: Train loss: 4.377342224121094 Valid loss: 3.9995882511138916\n",
      "Epoch: 1170: Train loss: 4.373749732971191 Valid loss: 3.9938745498657227\n",
      "Epoch: 1171: Train loss: 4.370164394378662 Valid loss: 3.9881885051727295\n",
      "Epoch: 1172: Train loss: 4.366589546203613 Valid loss: 3.9825098514556885\n",
      "Epoch: 1173: Train loss: 4.363022804260254 Valid loss: 3.9768638610839844\n",
      "Epoch: 1174: Train loss: 4.359465599060059 Valid loss: 3.971245050430298\n",
      "Epoch: 1175: Train loss: 4.355915546417236 Valid loss: 3.9656388759613037\n",
      "Epoch: 1176: Train loss: 4.352369785308838 Valid loss: 3.960050582885742\n",
      "Epoch: 1177: Train loss: 4.348839282989502 Valid loss: 3.9544835090637207\n",
      "Epoch: 1178: Train loss: 4.345311641693115 Valid loss: 3.948944091796875\n",
      "Epoch: 1179: Train loss: 4.341793537139893 Valid loss: 3.9434165954589844\n",
      "Epoch: 1180: Train loss: 4.338285446166992 Valid loss: 3.9379208087921143\n",
      "Epoch: 1181: Train loss: 4.334781646728516 Valid loss: 3.9324429035186768\n",
      "Epoch: 1182: Train loss: 4.331287860870361 Valid loss: 3.9269862174987793\n",
      "Epoch: 1183: Train loss: 4.327805995941162 Valid loss: 3.921541929244995\n",
      "Epoch: 1184: Train loss: 4.3243279457092285 Valid loss: 3.9161243438720703\n",
      "Epoch: 1185: Train loss: 4.320861339569092 Valid loss: 3.910728931427002\n",
      "Epoch: 1186: Train loss: 4.317401885986328 Valid loss: 3.9053494930267334\n",
      "Epoch: 1187: Train loss: 4.313947677612305 Valid loss: 3.89998722076416\n",
      "Epoch: 1188: Train loss: 4.3105034828186035 Valid loss: 3.8946516513824463\n",
      "Epoch: 1189: Train loss: 4.307065963745117 Valid loss: 3.8893327713012695\n",
      "Epoch: 1190: Train loss: 4.3036346435546875 Valid loss: 3.884040117263794\n",
      "Epoch: 1191: Train loss: 4.300217628479004 Valid loss: 3.8787639141082764\n",
      "Epoch: 1192: Train loss: 4.296804428100586 Valid loss: 3.873504400253296\n",
      "Epoch: 1193: Train loss: 4.293399810791016 Valid loss: 3.8682613372802734\n",
      "Epoch: 1194: Train loss: 4.290002822875977 Valid loss: 3.863054037094116\n",
      "Epoch: 1195: Train loss: 4.286615371704102 Valid loss: 3.8578543663024902\n",
      "Epoch: 1196: Train loss: 4.283234119415283 Valid loss: 3.852680206298828\n",
      "Epoch: 1197: Train loss: 4.279863357543945 Valid loss: 3.8475167751312256\n",
      "Epoch: 1198: Train loss: 4.276494979858398 Valid loss: 3.8423800468444824\n",
      "Epoch: 1199: Train loss: 4.273138523101807 Valid loss: 3.8372642993927\n",
      "Epoch: 1200: Train loss: 4.269792556762695 Valid loss: 3.832169771194458\n",
      "Epoch: 1201: Train loss: 4.266451358795166 Valid loss: 3.8270912170410156\n",
      "Epoch: 1202: Train loss: 4.263116836547852 Valid loss: 3.822038173675537\n",
      "Epoch: 1203: Train loss: 4.259790420532227 Valid loss: 3.8170008659362793\n",
      "Epoch: 1204: Train loss: 4.256474018096924 Valid loss: 3.8119747638702393\n",
      "Epoch: 1205: Train loss: 4.253162860870361 Valid loss: 3.8069794178009033\n",
      "Epoch: 1206: Train loss: 4.249860763549805 Valid loss: 3.80199933052063\n",
      "Epoch: 1207: Train loss: 4.246564865112305 Valid loss: 3.7970404624938965\n",
      "Epoch: 1208: Train loss: 4.2432780265808105 Valid loss: 3.7920968532562256\n",
      "Epoch: 1209: Train loss: 4.239996910095215 Valid loss: 3.7871792316436768\n",
      "Epoch: 1210: Train loss: 4.236726760864258 Valid loss: 3.7822718620300293\n",
      "Epoch: 1211: Train loss: 4.233461380004883 Valid loss: 3.7773804664611816\n",
      "Epoch: 1212: Train loss: 4.230205059051514 Valid loss: 3.772519588470459\n",
      "Epoch: 1213: Train loss: 4.226958274841309 Valid loss: 3.767678737640381\n",
      "Epoch: 1214: Train loss: 4.223717212677002 Valid loss: 3.762847900390625\n",
      "Epoch: 1215: Train loss: 4.220480918884277 Valid loss: 3.758037805557251\n",
      "Epoch: 1216: Train loss: 4.217256546020508 Valid loss: 3.7532577514648438\n",
      "Epoch: 1217: Train loss: 4.21403694152832 Valid loss: 3.7484827041625977\n",
      "Epoch: 1218: Train loss: 4.210826396942139 Valid loss: 3.743723154067993\n",
      "Epoch: 1219: Train loss: 4.20762300491333 Valid loss: 3.738983631134033\n",
      "Epoch: 1220: Train loss: 4.204427242279053 Valid loss: 3.734278917312622\n",
      "Epoch: 1221: Train loss: 4.201238632202148 Valid loss: 3.729579210281372\n",
      "Epoch: 1222: Train loss: 4.198057174682617 Valid loss: 3.7249045372009277\n",
      "Epoch: 1223: Train loss: 4.194882869720459 Valid loss: 3.720249652862549\n",
      "Epoch: 1224: Train loss: 4.191714763641357 Valid loss: 3.7156100273132324\n",
      "Epoch: 1225: Train loss: 4.188558101654053 Valid loss: 3.710974931716919\n",
      "Epoch: 1226: Train loss: 4.185406684875488 Valid loss: 3.7063846588134766\n",
      "Epoch: 1227: Train loss: 4.182260513305664 Valid loss: 3.701788902282715\n",
      "Epoch: 1228: Train loss: 4.17912483215332 Valid loss: 3.697227954864502\n",
      "Epoch: 1229: Train loss: 4.175993919372559 Valid loss: 3.6926767826080322\n",
      "Epoch: 1230: Train loss: 4.172872066497803 Valid loss: 3.688145160675049\n",
      "Epoch: 1231: Train loss: 4.169755935668945 Valid loss: 3.6836328506469727\n",
      "Epoch: 1232: Train loss: 4.166647911071777 Valid loss: 3.6791298389434814\n",
      "Epoch: 1233: Train loss: 4.163549423217773 Valid loss: 3.674652099609375\n",
      "Epoch: 1234: Train loss: 4.160454750061035 Valid loss: 3.6701879501342773\n",
      "Epoch: 1235: Train loss: 4.157367706298828 Valid loss: 3.665748357772827\n",
      "Epoch: 1236: Train loss: 4.154288291931152 Valid loss: 3.6613285541534424\n",
      "Epoch: 1237: Train loss: 4.151217937469482 Valid loss: 3.6569221019744873\n",
      "Epoch: 1238: Train loss: 4.148152828216553 Valid loss: 3.6525251865386963\n",
      "Epoch: 1239: Train loss: 4.145096302032471 Valid loss: 3.648162603378296\n",
      "Epoch: 1240: Train loss: 4.142048358917236 Valid loss: 3.6438043117523193\n",
      "Epoch: 1241: Train loss: 4.139003753662109 Valid loss: 3.6394646167755127\n",
      "Epoch: 1242: Train loss: 4.135967254638672 Valid loss: 3.6351492404937744\n",
      "Epoch: 1243: Train loss: 4.132936954498291 Valid loss: 3.630847692489624\n",
      "Epoch: 1244: Train loss: 4.129917621612549 Valid loss: 3.626569986343384\n",
      "Epoch: 1245: Train loss: 4.126902103424072 Valid loss: 3.6223015785217285\n",
      "Epoch: 1246: Train loss: 4.1238932609558105 Valid loss: 3.618046522140503\n",
      "Epoch: 1247: Train loss: 4.1208930015563965 Valid loss: 3.6138203144073486\n",
      "Epoch: 1248: Train loss: 4.117897987365723 Valid loss: 3.609598159790039\n",
      "Epoch: 1249: Train loss: 4.114912986755371 Valid loss: 3.605409622192383\n",
      "Epoch: 1250: Train loss: 4.111934185028076 Valid loss: 3.6012344360351562\n",
      "Epoch: 1251: Train loss: 4.108962059020996 Valid loss: 3.5970680713653564\n",
      "Epoch: 1252: Train loss: 4.105992317199707 Valid loss: 3.5929150581359863\n",
      "Epoch: 1253: Train loss: 4.103035926818848 Valid loss: 3.5887863636016846\n",
      "Epoch: 1254: Train loss: 4.100083351135254 Valid loss: 3.5846805572509766\n",
      "Epoch: 1255: Train loss: 4.097137928009033 Valid loss: 3.580578088760376\n",
      "Epoch: 1256: Train loss: 4.094200134277344 Valid loss: 3.5764992237091064\n",
      "Epoch: 1257: Train loss: 4.091268539428711 Valid loss: 3.572443723678589\n",
      "Epoch: 1258: Train loss: 4.088342189788818 Valid loss: 3.5683915615081787\n",
      "Epoch: 1259: Train loss: 4.08542537689209 Valid loss: 3.5643625259399414\n",
      "Epoch: 1260: Train loss: 4.082514762878418 Valid loss: 3.5603463649749756\n",
      "Epoch: 1261: Train loss: 4.07961368560791 Valid loss: 3.556353807449341\n",
      "Epoch: 1262: Train loss: 4.076715469360352 Valid loss: 3.5523743629455566\n",
      "Epoch: 1263: Train loss: 4.073824405670166 Valid loss: 3.5484180450439453\n",
      "Epoch: 1264: Train loss: 4.0709404945373535 Valid loss: 3.5444743633270264\n",
      "Epoch: 1265: Train loss: 4.068065166473389 Valid loss: 3.540544033050537\n",
      "Epoch: 1266: Train loss: 4.065191745758057 Valid loss: 3.5366268157958984\n",
      "Epoch: 1267: Train loss: 4.06233024597168 Valid loss: 3.5327322483062744\n",
      "Epoch: 1268: Train loss: 4.059471130371094 Valid loss: 3.528850793838501\n",
      "Epoch: 1269: Train loss: 4.056621074676514 Valid loss: 3.52498197555542\n",
      "Epoch: 1270: Train loss: 4.053779602050781 Valid loss: 3.5211358070373535\n",
      "Epoch: 1271: Train loss: 4.0509419441223145 Valid loss: 3.517302989959717\n",
      "Epoch: 1272: Train loss: 4.0481133460998535 Valid loss: 3.5134825706481934\n",
      "Epoch: 1273: Train loss: 4.045289039611816 Valid loss: 3.5096845626831055\n",
      "Epoch: 1274: Train loss: 4.042468070983887 Valid loss: 3.505894422531128\n",
      "Epoch: 1275: Train loss: 4.03965950012207 Valid loss: 3.502131700515747\n",
      "Epoch: 1276: Train loss: 4.036855697631836 Valid loss: 3.4983768463134766\n",
      "Epoch: 1277: Train loss: 4.034058094024658 Valid loss: 3.4946441650390625\n",
      "Epoch: 1278: Train loss: 4.031266689300537 Valid loss: 3.490933656692505\n",
      "Epoch: 1279: Train loss: 4.028482437133789 Valid loss: 3.4872212409973145\n",
      "Epoch: 1280: Train loss: 4.025705337524414 Valid loss: 3.4835307598114014\n",
      "Epoch: 1281: Train loss: 4.022934436798096 Valid loss: 3.4798526763916016\n",
      "Epoch: 1282: Train loss: 4.020166873931885 Valid loss: 3.4761922359466553\n",
      "Epoch: 1283: Train loss: 4.0174102783203125 Valid loss: 3.4725534915924072\n",
      "Epoch: 1284: Train loss: 4.014656066894531 Valid loss: 3.4689273834228516\n",
      "Epoch: 1285: Train loss: 4.011911392211914 Valid loss: 3.465318202972412\n",
      "Epoch: 1286: Train loss: 4.009172439575195 Valid loss: 3.461716413497925\n",
      "Epoch: 1287: Train loss: 4.006439208984375 Valid loss: 3.4581313133239746\n",
      "Epoch: 1288: Train loss: 4.003712177276611 Valid loss: 3.45456862449646\n",
      "Epoch: 1289: Train loss: 4.000992774963379 Valid loss: 3.4510178565979004\n",
      "Epoch: 1290: Train loss: 3.9982800483703613 Valid loss: 3.44748854637146\n",
      "Epoch: 1291: Train loss: 3.995570659637451 Valid loss: 3.4439620971679688\n",
      "Epoch: 1292: Train loss: 3.992870330810547 Valid loss: 3.4404571056365967\n",
      "Epoch: 1293: Train loss: 3.9901742935180664 Valid loss: 3.4369640350341797\n",
      "Epoch: 1294: Train loss: 3.98748517036438 Valid loss: 3.4334921836853027\n",
      "Epoch: 1295: Train loss: 3.9848055839538574 Valid loss: 3.4300377368927\n",
      "Epoch: 1296: Train loss: 3.982128620147705 Valid loss: 3.4265851974487305\n",
      "Epoch: 1297: Train loss: 3.9794600009918213 Valid loss: 3.423163890838623\n",
      "Epoch: 1298: Train loss: 3.976793050765991 Valid loss: 3.4197444915771484\n",
      "Epoch: 1299: Train loss: 3.974135398864746 Valid loss: 3.416351318359375\n",
      "Epoch: 1300: Train loss: 3.9714884757995605 Valid loss: 3.4129700660705566\n",
      "Epoch: 1301: Train loss: 3.968841552734375 Valid loss: 3.409590482711792\n",
      "Epoch: 1302: Train loss: 3.966202974319458 Valid loss: 3.4062321186065674\n",
      "Epoch: 1303: Train loss: 3.9635684490203857 Valid loss: 3.402895212173462\n",
      "Epoch: 1304: Train loss: 3.960944652557373 Valid loss: 3.3995747566223145\n",
      "Epoch: 1305: Train loss: 3.9583218097686768 Valid loss: 3.3962559700012207\n",
      "Epoch: 1306: Train loss: 3.955709218978882 Valid loss: 3.392967939376831\n",
      "Epoch: 1307: Train loss: 3.953101634979248 Valid loss: 3.389681577682495\n",
      "Epoch: 1308: Train loss: 3.950500965118408 Valid loss: 3.3864166736602783\n",
      "Epoch: 1309: Train loss: 3.947901725769043 Valid loss: 3.3831627368927\n",
      "Epoch: 1310: Train loss: 3.945312976837158 Valid loss: 3.379915237426758\n",
      "Epoch: 1311: Train loss: 3.9427309036254883 Valid loss: 3.3766987323760986\n",
      "Epoch: 1312: Train loss: 3.940152406692505 Valid loss: 3.373493194580078\n",
      "Epoch: 1313: Train loss: 3.937580108642578 Valid loss: 3.370288848876953\n",
      "Epoch: 1314: Train loss: 3.9350128173828125 Valid loss: 3.367105484008789\n",
      "Epoch: 1315: Train loss: 3.9324560165405273 Valid loss: 3.363943338394165\n",
      "Epoch: 1316: Train loss: 3.929903745651245 Valid loss: 3.3607821464538574\n",
      "Epoch: 1317: Train loss: 3.9273529052734375 Valid loss: 3.357651472091675\n",
      "Epoch: 1318: Train loss: 3.9248106479644775 Valid loss: 3.3545124530792236\n",
      "Epoch: 1319: Train loss: 3.922276735305786 Valid loss: 3.3514037132263184\n",
      "Epoch: 1320: Train loss: 3.9197468757629395 Valid loss: 3.3483059406280518\n",
      "Epoch: 1321: Train loss: 3.917219400405884 Valid loss: 3.345228910446167\n",
      "Epoch: 1322: Train loss: 3.914703369140625 Valid loss: 3.3421623706817627\n",
      "Epoch: 1323: Train loss: 3.912189483642578 Valid loss: 3.339106798171997\n",
      "Epoch: 1324: Train loss: 3.909684181213379 Valid loss: 3.336071729660034\n",
      "Epoch: 1325: Train loss: 3.9071836471557617 Valid loss: 3.333037853240967\n",
      "Epoch: 1326: Train loss: 3.904688596725464 Valid loss: 3.330024003982544\n",
      "Epoch: 1327: Train loss: 3.9021992683410645 Valid loss: 3.3270211219787598\n",
      "Epoch: 1328: Train loss: 3.8997180461883545 Valid loss: 3.32403826713562\n",
      "Epoch: 1329: Train loss: 3.8972396850585938 Valid loss: 3.321056365966797\n",
      "Epoch: 1330: Train loss: 3.894768714904785 Valid loss: 3.3180947303771973\n",
      "Epoch: 1331: Train loss: 3.892302989959717 Valid loss: 3.3151533603668213\n",
      "Epoch: 1332: Train loss: 3.8898441791534424 Valid loss: 3.312222480773926\n",
      "Epoch: 1333: Train loss: 3.8873889446258545 Valid loss: 3.3093020915985107\n",
      "Epoch: 1334: Train loss: 3.8849401473999023 Valid loss: 3.3064019680023193\n",
      "Epoch: 1335: Train loss: 3.882495880126953 Valid loss: 3.303502082824707\n",
      "Epoch: 1336: Train loss: 3.880058765411377 Valid loss: 3.3006322383880615\n",
      "Epoch: 1337: Train loss: 3.87762713432312 Valid loss: 3.297762870788574\n",
      "Epoch: 1338: Train loss: 3.8752033710479736 Valid loss: 3.2949037551879883\n",
      "Epoch: 1339: Train loss: 3.87278151512146 Valid loss: 3.292064905166626\n",
      "Epoch: 1340: Train loss: 3.8703668117523193 Valid loss: 3.28924560546875\n",
      "Epoch: 1341: Train loss: 3.86795973777771 Valid loss: 3.2864267826080322\n",
      "Epoch: 1342: Train loss: 3.8655552864074707 Valid loss: 3.283637762069702\n",
      "Epoch: 1343: Train loss: 3.8631591796875 Valid loss: 3.280843734741211\n",
      "Epoch: 1344: Train loss: 3.8607678413391113 Valid loss: 3.2780747413635254\n",
      "Epoch: 1345: Train loss: 3.858379364013672 Valid loss: 3.275310516357422\n",
      "Epoch: 1346: Train loss: 3.8559985160827637 Valid loss: 3.272571325302124\n",
      "Epoch: 1347: Train loss: 3.8536250591278076 Valid loss: 3.2698371410369873\n",
      "Epoch: 1348: Train loss: 3.8512561321258545 Valid loss: 3.2671077251434326\n",
      "Epoch: 1349: Train loss: 3.8488893508911133 Valid loss: 3.2644031047821045\n",
      "Epoch: 1350: Train loss: 3.846531391143799 Valid loss: 3.2617032527923584\n",
      "Epoch: 1351: Train loss: 3.8441789150238037 Valid loss: 3.2590084075927734\n",
      "Epoch: 1352: Train loss: 3.8418307304382324 Valid loss: 3.256352186203003\n",
      "Epoch: 1353: Train loss: 3.839487314224243 Valid loss: 3.2536866664886475\n",
      "Epoch: 1354: Train loss: 3.8371517658233643 Valid loss: 3.2510502338409424\n",
      "Epoch: 1355: Train loss: 3.8348193168640137 Valid loss: 3.248413562774658\n",
      "Epoch: 1356: Train loss: 3.832496166229248 Valid loss: 3.2457964420318604\n",
      "Epoch: 1357: Train loss: 3.830174446105957 Valid loss: 3.2431888580322266\n",
      "Epoch: 1358: Train loss: 3.8278591632843018 Valid loss: 3.240590810775757\n",
      "Epoch: 1359: Train loss: 3.8255510330200195 Valid loss: 3.2380170822143555\n",
      "Epoch: 1360: Train loss: 3.8232460021972656 Valid loss: 3.235438108444214\n",
      "Epoch: 1361: Train loss: 3.8209469318389893 Valid loss: 3.2328734397888184\n",
      "Epoch: 1362: Train loss: 3.8186542987823486 Valid loss: 3.230332851409912\n",
      "Epoch: 1363: Train loss: 3.8163630962371826 Valid loss: 3.227796792984009\n",
      "Epoch: 1364: Train loss: 3.814082145690918 Valid loss: 3.2252798080444336\n",
      "Epoch: 1365: Train loss: 3.81180477142334 Valid loss: 3.2227723598480225\n",
      "Epoch: 1366: Train loss: 3.80953311920166 Valid loss: 3.220273971557617\n",
      "Epoch: 1367: Train loss: 3.8072657585144043 Valid loss: 3.217789888381958\n",
      "Epoch: 1368: Train loss: 3.8050053119659424 Valid loss: 3.215319871902466\n",
      "Epoch: 1369: Train loss: 3.802748680114746 Valid loss: 3.2128636837005615\n",
      "Epoch: 1370: Train loss: 3.8004963397979736 Valid loss: 3.21041202545166\n",
      "Epoch: 1371: Train loss: 3.798251152038574 Valid loss: 3.207979202270508\n",
      "Epoch: 1372: Train loss: 3.796010732650757 Valid loss: 3.2055556774139404\n",
      "Epoch: 1373: Train loss: 3.793775796890259 Valid loss: 3.203150749206543\n",
      "Epoch: 1374: Train loss: 3.79154372215271 Valid loss: 3.200745105743408\n",
      "Epoch: 1375: Train loss: 3.789322853088379 Valid loss: 3.1983487606048584\n",
      "Epoch: 1376: Train loss: 3.7871038913726807 Valid loss: 3.1959805488586426\n",
      "Epoch: 1377: Train loss: 3.78488826751709 Valid loss: 3.1936211585998535\n",
      "Epoch: 1378: Train loss: 3.782679557800293 Valid loss: 3.1912615299224854\n",
      "Epoch: 1379: Train loss: 3.7804722785949707 Valid loss: 3.188920021057129\n",
      "Epoch: 1380: Train loss: 3.7782740592956543 Valid loss: 3.1865971088409424\n",
      "Epoch: 1381: Train loss: 3.7760798931121826 Valid loss: 3.1842784881591797\n",
      "Epoch: 1382: Train loss: 3.773893117904663 Valid loss: 3.1819732189178467\n",
      "Epoch: 1383: Train loss: 3.77170729637146 Valid loss: 3.1796720027923584\n",
      "Epoch: 1384: Train loss: 3.76953125 Valid loss: 3.177399158477783\n",
      "Epoch: 1385: Train loss: 3.76735520362854 Valid loss: 3.1751251220703125\n",
      "Epoch: 1386: Train loss: 3.7651889324188232 Valid loss: 3.1728644371032715\n",
      "Epoch: 1387: Train loss: 3.763025999069214 Valid loss: 3.1706125736236572\n",
      "Epoch: 1388: Train loss: 3.7608652114868164 Valid loss: 3.168379068374634\n",
      "Epoch: 1389: Train loss: 3.7587127685546875 Valid loss: 3.166168451309204\n",
      "Epoch: 1390: Train loss: 3.7565646171569824 Valid loss: 3.163947343826294\n",
      "Epoch: 1391: Train loss: 3.7544240951538086 Valid loss: 3.1617491245269775\n",
      "Epoch: 1392: Train loss: 3.7522850036621094 Valid loss: 3.159559488296509\n",
      "Epoch: 1393: Train loss: 3.7501509189605713 Valid loss: 3.1573686599731445\n",
      "Epoch: 1394: Train loss: 3.7480239868164062 Valid loss: 3.155210256576538\n",
      "Epoch: 1395: Train loss: 3.745899200439453 Valid loss: 3.153041124343872\n",
      "Epoch: 1396: Train loss: 3.7437820434570312 Valid loss: 3.150904655456543\n",
      "Epoch: 1397: Train loss: 3.741668701171875 Valid loss: 3.14876651763916\n",
      "Epoch: 1398: Train loss: 3.7395591735839844 Valid loss: 3.146641731262207\n",
      "Epoch: 1399: Train loss: 3.7374563217163086 Valid loss: 3.1445348262786865\n",
      "Epoch: 1400: Train loss: 3.7353591918945312 Valid loss: 3.1424412727355957\n",
      "Epoch: 1401: Train loss: 3.7332653999328613 Valid loss: 3.140346050262451\n",
      "Epoch: 1402: Train loss: 3.731173515319824 Valid loss: 3.1382734775543213\n",
      "Epoch: 1403: Train loss: 3.729091167449951 Valid loss: 3.1361899375915527\n",
      "Epoch: 1404: Train loss: 3.7270114421844482 Valid loss: 3.134148120880127\n",
      "Epoch: 1405: Train loss: 3.7249386310577393 Valid loss: 3.1320955753326416\n",
      "Epoch: 1406: Train loss: 3.722867250442505 Valid loss: 3.1300556659698486\n",
      "Epoch: 1407: Train loss: 3.7208056449890137 Valid loss: 3.128033399581909\n",
      "Epoch: 1408: Train loss: 3.718745708465576 Valid loss: 3.126024007797241\n",
      "Epoch: 1409: Train loss: 3.716688394546509 Valid loss: 3.1240129470825195\n",
      "Epoch: 1410: Train loss: 3.71463942527771 Valid loss: 3.1220242977142334\n",
      "Epoch: 1411: Train loss: 3.7125935554504395 Valid loss: 3.1200342178344727\n",
      "Epoch: 1412: Train loss: 3.710554361343384 Valid loss: 3.1180760860443115\n",
      "Epoch: 1413: Train loss: 3.7085180282592773 Valid loss: 3.1161160469055176\n",
      "Epoch: 1414: Train loss: 3.7064852714538574 Valid loss: 3.114159107208252\n",
      "Epoch: 1415: Train loss: 3.7044589519500732 Valid loss: 3.112229108810425\n",
      "Epoch: 1416: Train loss: 3.7024383544921875 Valid loss: 3.110302448272705\n",
      "Epoch: 1417: Train loss: 3.700423240661621 Valid loss: 3.1083786487579346\n",
      "Epoch: 1418: Train loss: 3.698410987854004 Valid loss: 3.1064720153808594\n",
      "Epoch: 1419: Train loss: 3.6964035034179688 Valid loss: 3.1045687198638916\n",
      "Epoch: 1420: Train loss: 3.694399118423462 Valid loss: 3.102682590484619\n",
      "Epoch: 1421: Train loss: 3.6924006938934326 Valid loss: 3.100799083709717\n",
      "Epoch: 1422: Train loss: 3.69040846824646 Valid loss: 3.098947525024414\n",
      "Epoch: 1423: Train loss: 3.6884193420410156 Valid loss: 3.0970888137817383\n",
      "Epoch: 1424: Train loss: 3.6864335536956787 Valid loss: 3.095233201980591\n",
      "Epoch: 1425: Train loss: 3.6844568252563477 Valid loss: 3.0934042930603027\n",
      "Epoch: 1426: Train loss: 3.6824803352355957 Valid loss: 3.0915780067443848\n",
      "Epoch: 1427: Train loss: 3.6805131435394287 Valid loss: 3.089769124984741\n",
      "Epoch: 1428: Train loss: 3.678544759750366 Valid loss: 3.0879578590393066\n",
      "Epoch: 1429: Train loss: 3.6765856742858887 Valid loss: 3.0861635208129883\n",
      "Epoch: 1430: Train loss: 3.6746275424957275 Valid loss: 3.0843722820281982\n",
      "Epoch: 1431: Train loss: 3.6726760864257812 Valid loss: 3.0825979709625244\n",
      "Epoch: 1432: Train loss: 3.670728921890259 Valid loss: 3.0808448791503906\n",
      "Epoch: 1433: Train loss: 3.668787717819214 Valid loss: 3.079089879989624\n",
      "Epoch: 1434: Train loss: 3.6668484210968018 Valid loss: 3.0773422718048096\n",
      "Epoch: 1435: Train loss: 3.664912700653076 Valid loss: 3.0756115913391113\n",
      "Epoch: 1436: Train loss: 3.6629834175109863 Valid loss: 3.073883533477783\n",
      "Epoch: 1437: Train loss: 3.6610591411590576 Valid loss: 3.07216215133667\n",
      "Epoch: 1438: Train loss: 3.6591389179229736 Valid loss: 3.070462465286255\n",
      "Epoch: 1439: Train loss: 3.65722393989563 Valid loss: 3.068770408630371\n",
      "Epoch: 1440: Train loss: 3.6553144454956055 Valid loss: 3.067080497741699\n",
      "Epoch: 1441: Train loss: 3.653407573699951 Valid loss: 3.0653979778289795\n",
      "Epoch: 1442: Train loss: 3.6515040397644043 Valid loss: 3.0637364387512207\n",
      "Epoch: 1443: Train loss: 3.6496057510375977 Valid loss: 3.062087059020996\n",
      "Epoch: 1444: Train loss: 3.647710084915161 Valid loss: 3.0604398250579834\n",
      "Epoch: 1445: Train loss: 3.6458232402801514 Valid loss: 3.058804512023926\n",
      "Epoch: 1446: Train loss: 3.643937826156616 Valid loss: 3.057176113128662\n",
      "Epoch: 1447: Train loss: 3.6420576572418213 Valid loss: 3.0555548667907715\n",
      "Epoch: 1448: Train loss: 3.6401829719543457 Valid loss: 3.053954601287842\n",
      "Epoch: 1449: Train loss: 3.638309955596924 Valid loss: 3.052356719970703\n",
      "Epoch: 1450: Train loss: 3.6364428997039795 Valid loss: 3.0507612228393555\n",
      "Epoch: 1451: Train loss: 3.63458251953125 Valid loss: 3.0491957664489746\n",
      "Epoch: 1452: Train loss: 3.632723808288574 Valid loss: 3.047618865966797\n",
      "Epoch: 1453: Train loss: 3.6308698654174805 Valid loss: 3.04605770111084\n",
      "Epoch: 1454: Train loss: 3.6290206909179688 Valid loss: 3.044494152069092\n",
      "Epoch: 1455: Train loss: 3.62717342376709 Valid loss: 3.0429611206054688\n",
      "Epoch: 1456: Train loss: 3.625332832336426 Valid loss: 3.0414302349090576\n",
      "Epoch: 1457: Train loss: 3.62349534034729 Valid loss: 3.0399110317230225\n",
      "Epoch: 1458: Train loss: 3.62166428565979 Valid loss: 3.038393259048462\n",
      "Epoch: 1459: Train loss: 3.61983585357666 Valid loss: 3.0368916988372803\n",
      "Epoch: 1460: Train loss: 3.618011951446533 Valid loss: 3.0353925228118896\n",
      "Epoch: 1461: Train loss: 3.616194009780884 Valid loss: 3.0338995456695557\n",
      "Epoch: 1462: Train loss: 3.6143765449523926 Valid loss: 3.0324277877807617\n",
      "Epoch: 1463: Train loss: 3.612565040588379 Valid loss: 3.0309624671936035\n",
      "Epoch: 1464: Train loss: 3.6107568740844727 Valid loss: 3.029503583908081\n",
      "Epoch: 1465: Train loss: 3.608954429626465 Valid loss: 3.0280516147613525\n",
      "Epoch: 1466: Train loss: 3.6071581840515137 Valid loss: 3.0266056060791016\n",
      "Epoch: 1467: Train loss: 3.605363130569458 Valid loss: 3.0251853466033936\n",
      "Epoch: 1468: Train loss: 3.6035733222961426 Valid loss: 3.023757219314575\n",
      "Epoch: 1469: Train loss: 3.60178542137146 Valid loss: 3.0223448276519775\n",
      "Epoch: 1470: Train loss: 3.6000051498413086 Valid loss: 3.020944356918335\n",
      "Epoch: 1471: Train loss: 3.5982258319854736 Valid loss: 3.019545078277588\n",
      "Epoch: 1472: Train loss: 3.5964550971984863 Valid loss: 3.0181617736816406\n",
      "Epoch: 1473: Train loss: 3.5946829319000244 Valid loss: 3.016780138015747\n",
      "Epoch: 1474: Train loss: 3.5929200649261475 Valid loss: 3.015413999557495\n",
      "Epoch: 1475: Train loss: 3.5911576747894287 Valid loss: 3.014054298400879\n",
      "Epoch: 1476: Train loss: 3.5893990993499756 Valid loss: 3.0127007961273193\n",
      "Epoch: 1477: Train loss: 3.5876481533050537 Valid loss: 3.0113534927368164\n",
      "Epoch: 1478: Train loss: 3.5858993530273438 Valid loss: 3.0100221633911133\n",
      "Epoch: 1479: Train loss: 3.584153175354004 Valid loss: 3.0087013244628906\n",
      "Epoch: 1480: Train loss: 3.582413673400879 Valid loss: 3.0073776245117188\n",
      "Epoch: 1481: Train loss: 3.5806782245635986 Valid loss: 3.0060741901397705\n",
      "Epoch: 1482: Train loss: 3.578946113586426 Valid loss: 3.0047719478607178\n",
      "Epoch: 1483: Train loss: 3.5772171020507812 Valid loss: 3.003485679626465\n",
      "Epoch: 1484: Train loss: 3.575491189956665 Valid loss: 3.002200126647949\n",
      "Epoch: 1485: Train loss: 3.5737714767456055 Valid loss: 3.0009307861328125\n",
      "Epoch: 1486: Train loss: 3.5720558166503906 Valid loss: 2.999667167663574\n",
      "Epoch: 1487: Train loss: 3.5703418254852295 Valid loss: 2.9984047412872314\n",
      "Epoch: 1488: Train loss: 3.568634510040283 Valid loss: 2.9971673488616943\n",
      "Epoch: 1489: Train loss: 3.5669314861297607 Valid loss: 2.9959218502044678\n",
      "Epoch: 1490: Train loss: 3.565229654312134 Valid loss: 2.994687080383301\n",
      "Epoch: 1491: Train loss: 3.563532590866089 Valid loss: 2.9934580326080322\n",
      "Epoch: 1492: Train loss: 3.561842203140259 Valid loss: 2.9922492504119873\n",
      "Epoch: 1493: Train loss: 3.5601513385772705 Valid loss: 2.9910459518432617\n",
      "Epoch: 1494: Train loss: 3.5584678649902344 Valid loss: 2.9898488521575928\n",
      "Epoch: 1495: Train loss: 3.5567877292633057 Valid loss: 2.988662004470825\n",
      "Epoch: 1496: Train loss: 3.5551106929779053 Valid loss: 2.98747181892395\n",
      "Epoch: 1497: Train loss: 3.5534393787384033 Valid loss: 2.9863014221191406\n",
      "Epoch: 1498: Train loss: 3.5517683029174805 Valid loss: 2.9851372241973877\n",
      "Epoch: 1499: Train loss: 3.550103187561035 Valid loss: 2.983988046646118\n",
      "Epoch: 1500: Train loss: 3.5484423637390137 Valid loss: 2.982835054397583\n",
      "Epoch: 1501: Train loss: 3.546785354614258 Valid loss: 2.9816925525665283\n",
      "Epoch: 1502: Train loss: 3.5451323986053467 Valid loss: 2.980560779571533\n",
      "Epoch: 1503: Train loss: 3.5434820652008057 Valid loss: 2.9794294834136963\n",
      "Epoch: 1504: Train loss: 3.541837453842163 Valid loss: 2.9783177375793457\n",
      "Epoch: 1505: Train loss: 3.5401954650878906 Valid loss: 2.9772117137908936\n",
      "Epoch: 1506: Train loss: 3.5385584831237793 Valid loss: 2.976111650466919\n",
      "Epoch: 1507: Train loss: 3.5369250774383545 Valid loss: 2.9750165939331055\n",
      "Epoch: 1508: Train loss: 3.5352935791015625 Valid loss: 2.9739325046539307\n",
      "Epoch: 1509: Train loss: 3.533665418624878 Valid loss: 2.972853660583496\n",
      "Epoch: 1510: Train loss: 3.532043218612671 Valid loss: 2.971785306930542\n",
      "Epoch: 1511: Train loss: 3.530424118041992 Valid loss: 2.970722198486328\n",
      "Epoch: 1512: Train loss: 3.528810501098633 Valid loss: 2.9696738719940186\n",
      "Epoch: 1513: Train loss: 3.5271964073181152 Valid loss: 2.9686262607574463\n",
      "Epoch: 1514: Train loss: 3.525589942932129 Valid loss: 2.96759295463562\n",
      "Epoch: 1515: Train loss: 3.5239875316619873 Valid loss: 2.9665610790252686\n",
      "Epoch: 1516: Train loss: 3.522385597229004 Valid loss: 2.9655346870422363\n",
      "Epoch: 1517: Train loss: 3.520789384841919 Valid loss: 2.9645323753356934\n",
      "Epoch: 1518: Train loss: 3.5191969871520996 Valid loss: 2.9635164737701416\n",
      "Epoch: 1519: Train loss: 3.5176076889038086 Valid loss: 2.962519645690918\n",
      "Epoch: 1520: Train loss: 3.5160231590270996 Valid loss: 2.9615378379821777\n",
      "Epoch: 1521: Train loss: 3.514439821243286 Valid loss: 2.96054744720459\n",
      "Epoch: 1522: Train loss: 3.512864112854004 Valid loss: 2.959567070007324\n",
      "Epoch: 1523: Train loss: 3.5112874507904053 Valid loss: 2.9586007595062256\n",
      "Epoch: 1524: Train loss: 3.5097174644470215 Valid loss: 2.9576351642608643\n",
      "Epoch: 1525: Train loss: 3.5081517696380615 Valid loss: 2.95668888092041\n",
      "Epoch: 1526: Train loss: 3.506586790084839 Valid loss: 2.9557430744171143\n",
      "Epoch: 1527: Train loss: 3.5050272941589355 Valid loss: 2.954807758331299\n",
      "Epoch: 1528: Train loss: 3.503472328186035 Valid loss: 2.953881025314331\n",
      "Epoch: 1529: Train loss: 3.501920461654663 Valid loss: 2.9529504776000977\n",
      "Epoch: 1530: Train loss: 3.500371217727661 Valid loss: 2.9520344734191895\n",
      "Epoch: 1531: Train loss: 3.498825788497925 Valid loss: 2.9511237144470215\n",
      "Epoch: 1532: Train loss: 3.4972848892211914 Valid loss: 2.9502179622650146\n",
      "Epoch: 1533: Train loss: 3.4957478046417236 Valid loss: 2.9493308067321777\n",
      "Epoch: 1534: Train loss: 3.494213342666626 Valid loss: 2.9484403133392334\n",
      "Epoch: 1535: Train loss: 3.492682695388794 Valid loss: 2.947558879852295\n",
      "Epoch: 1536: Train loss: 3.491154909133911 Valid loss: 2.9466919898986816\n",
      "Epoch: 1537: Train loss: 3.4896316528320312 Valid loss: 2.945829391479492\n",
      "Epoch: 1538: Train loss: 3.488110065460205 Valid loss: 2.944972276687622\n",
      "Epoch: 1539: Train loss: 3.4865972995758057 Valid loss: 2.944124698638916\n",
      "Epoch: 1540: Train loss: 3.48508358001709 Valid loss: 2.943282127380371\n",
      "Epoch: 1541: Train loss: 3.4835729598999023 Valid loss: 2.942448854446411\n",
      "Epoch: 1542: Train loss: 3.482065200805664 Valid loss: 2.9416208267211914\n",
      "Epoch: 1543: Train loss: 3.480565071105957 Valid loss: 2.9407968521118164\n",
      "Epoch: 1544: Train loss: 3.4790658950805664 Valid loss: 2.9399831295013428\n",
      "Epoch: 1545: Train loss: 3.477572441101074 Valid loss: 2.939173936843872\n",
      "Epoch: 1546: Train loss: 3.4760773181915283 Valid loss: 2.9383645057678223\n",
      "Epoch: 1547: Train loss: 3.4745912551879883 Valid loss: 2.9375741481781006\n",
      "Epoch: 1548: Train loss: 3.4731051921844482 Valid loss: 2.936784029006958\n",
      "Epoch: 1549: Train loss: 3.4716248512268066 Valid loss: 2.9360122680664062\n",
      "Epoch: 1550: Train loss: 3.470146417617798 Valid loss: 2.935241222381592\n",
      "Epoch: 1551: Train loss: 3.4686710834503174 Valid loss: 2.934478759765625\n",
      "Epoch: 1552: Train loss: 3.46720027923584 Valid loss: 2.9337213039398193\n",
      "Epoch: 1553: Train loss: 3.4657320976257324 Valid loss: 2.9329640865325928\n",
      "Epoch: 1554: Train loss: 3.4642698764801025 Valid loss: 2.932220220565796\n",
      "Epoch: 1555: Train loss: 3.4628093242645264 Valid loss: 2.9314815998077393\n",
      "Epoch: 1556: Train loss: 3.4613513946533203 Valid loss: 2.9307470321655273\n",
      "Epoch: 1557: Train loss: 3.4598987102508545 Valid loss: 2.9300215244293213\n",
      "Epoch: 1558: Train loss: 3.4584474563598633 Valid loss: 2.9293103218078613\n",
      "Epoch: 1559: Train loss: 3.4569990634918213 Valid loss: 2.9285895824432373\n",
      "Epoch: 1560: Train loss: 3.4555580615997314 Valid loss: 2.927887439727783\n",
      "Epoch: 1561: Train loss: 3.454115867614746 Valid loss: 2.927194595336914\n",
      "Epoch: 1562: Train loss: 3.4526779651641846 Valid loss: 2.9265058040618896\n",
      "Epoch: 1563: Train loss: 3.4512436389923096 Valid loss: 2.9258174896240234\n",
      "Epoch: 1564: Train loss: 3.4498143196105957 Valid loss: 2.9251463413238525\n",
      "Epoch: 1565: Train loss: 3.448385238647461 Valid loss: 2.924471139907837\n",
      "Epoch: 1566: Train loss: 3.4469645023345947 Valid loss: 2.923814058303833\n",
      "Epoch: 1567: Train loss: 3.4455432891845703 Valid loss: 2.9231526851654053\n",
      "Epoch: 1568: Train loss: 3.4441256523132324 Valid loss: 2.922508478164673\n",
      "Epoch: 1569: Train loss: 3.44271183013916 Valid loss: 2.9218695163726807\n",
      "Epoch: 1570: Train loss: 3.441300392150879 Valid loss: 2.9212210178375244\n",
      "Epoch: 1571: Train loss: 3.439894437789917 Valid loss: 2.920595169067383\n",
      "Epoch: 1572: Train loss: 3.438490629196167 Valid loss: 2.919968843460083\n",
      "Epoch: 1573: Train loss: 3.4370899200439453 Valid loss: 2.919346809387207\n",
      "Epoch: 1574: Train loss: 3.4356918334960938 Valid loss: 2.918728828430176\n",
      "Epoch: 1575: Train loss: 3.434298515319824 Valid loss: 2.918123960494995\n",
      "Epoch: 1576: Train loss: 3.4329090118408203 Valid loss: 2.9175243377685547\n",
      "Epoch: 1577: Train loss: 3.431520462036133 Valid loss: 2.9169373512268066\n",
      "Epoch: 1578: Train loss: 3.430135726928711 Valid loss: 2.9163548946380615\n",
      "Epoch: 1579: Train loss: 3.428755044937134 Valid loss: 2.915767192840576\n",
      "Epoch: 1580: Train loss: 3.4273786544799805 Valid loss: 2.9151930809020996\n",
      "Epoch: 1581: Train loss: 3.4260034561157227 Valid loss: 2.9146323204040527\n",
      "Epoch: 1582: Train loss: 3.424633502960205 Valid loss: 2.914071559906006\n",
      "Epoch: 1583: Train loss: 3.4232640266418457 Valid loss: 2.9135146141052246\n",
      "Epoch: 1584: Train loss: 3.4218978881835938 Valid loss: 2.912966012954712\n",
      "Epoch: 1585: Train loss: 3.4205355644226074 Valid loss: 2.912421464920044\n",
      "Epoch: 1586: Train loss: 3.4191794395446777 Valid loss: 2.911885976791382\n",
      "Epoch: 1587: Train loss: 3.4178221225738525 Valid loss: 2.9113543033599854\n",
      "Epoch: 1588: Train loss: 3.416471242904663 Valid loss: 2.9108405113220215\n",
      "Epoch: 1589: Train loss: 3.4151203632354736 Valid loss: 2.9103126525878906\n",
      "Epoch: 1590: Train loss: 3.413774251937866 Valid loss: 2.909802198410034\n",
      "Epoch: 1591: Train loss: 3.412431478500366 Valid loss: 2.9092953205108643\n",
      "Epoch: 1592: Train loss: 3.4110918045043945 Valid loss: 2.9087975025177\n",
      "Epoch: 1593: Train loss: 3.4097554683685303 Valid loss: 2.908313035964966\n",
      "Epoch: 1594: Train loss: 3.408421516418457 Valid loss: 2.907823085784912\n",
      "Epoch: 1595: Train loss: 3.4070932865142822 Valid loss: 2.9073362350463867\n",
      "Epoch: 1596: Train loss: 3.4057650566101074 Valid loss: 2.906858444213867\n",
      "Epoch: 1597: Train loss: 3.4044387340545654 Valid loss: 2.9063854217529297\n",
      "Epoch: 1598: Train loss: 3.4031193256378174 Valid loss: 2.905928373336792\n",
      "Epoch: 1599: Train loss: 3.401798963546753 Valid loss: 2.9054675102233887\n",
      "Epoch: 1600: Train loss: 3.400486469268799 Valid loss: 2.9050235748291016\n",
      "Epoch: 1601: Train loss: 3.399174928665161 Valid loss: 2.904574155807495\n",
      "Epoch: 1602: Train loss: 3.39786434173584 Valid loss: 2.9041330814361572\n",
      "Epoch: 1603: Train loss: 3.3965604305267334 Valid loss: 2.903695583343506\n",
      "Epoch: 1604: Train loss: 3.3952577114105225 Valid loss: 2.9032719135284424\n",
      "Epoch: 1605: Train loss: 3.393956422805786 Valid loss: 2.902851104736328\n",
      "Epoch: 1606: Train loss: 3.3926608562469482 Valid loss: 2.9024436473846436\n",
      "Epoch: 1607: Train loss: 3.391366720199585 Valid loss: 2.9020261764526367\n",
      "Epoch: 1608: Train loss: 3.390075206756592 Valid loss: 2.9016122817993164\n",
      "Epoch: 1609: Train loss: 3.388789415359497 Valid loss: 2.9012153148651123\n",
      "Epoch: 1610: Train loss: 3.387505054473877 Valid loss: 2.900822401046753\n",
      "Epoch: 1611: Train loss: 3.386223316192627 Valid loss: 2.9004335403442383\n",
      "Epoch: 1612: Train loss: 3.384943962097168 Valid loss: 2.9000566005706787\n",
      "Epoch: 1613: Train loss: 3.3836684226989746 Valid loss: 2.899674892425537\n",
      "Epoch: 1614: Train loss: 3.382397174835205 Valid loss: 2.899301290512085\n",
      "Epoch: 1615: Train loss: 3.381126880645752 Valid loss: 2.898944854736328\n",
      "Epoch: 1616: Train loss: 3.3798582553863525 Valid loss: 2.8985824584960938\n",
      "Epoch: 1617: Train loss: 3.378594398498535 Valid loss: 2.898223876953125\n",
      "Epoch: 1618: Train loss: 3.377333879470825 Valid loss: 2.8978729248046875\n",
      "Epoch: 1619: Train loss: 3.376077890396118 Valid loss: 2.8975400924682617\n",
      "Epoch: 1620: Train loss: 3.374823808670044 Valid loss: 2.8972008228302\n",
      "Epoch: 1621: Train loss: 3.373569965362549 Valid loss: 2.8968698978424072\n",
      "Epoch: 1622: Train loss: 3.3723225593566895 Valid loss: 2.896542549133301\n",
      "Epoch: 1623: Train loss: 3.3710744380950928 Valid loss: 2.8962182998657227\n",
      "Epoch: 1624: Train loss: 3.3698344230651855 Valid loss: 2.89589786529541\n",
      "Epoch: 1625: Train loss: 3.368591785430908 Valid loss: 2.895599603652954\n",
      "Epoch: 1626: Train loss: 3.367354154586792 Valid loss: 2.895294427871704\n",
      "Epoch: 1627: Train loss: 3.3661203384399414 Valid loss: 2.894989013671875\n",
      "Epoch: 1628: Train loss: 3.36488938331604 Valid loss: 2.894695281982422\n",
      "Epoch: 1629: Train loss: 3.3636586666107178 Valid loss: 2.8944106101989746\n",
      "Epoch: 1630: Train loss: 3.3624348640441895 Valid loss: 2.8941237926483154\n",
      "Epoch: 1631: Train loss: 3.3612115383148193 Valid loss: 2.8938491344451904\n",
      "Epoch: 1632: Train loss: 3.3599913120269775 Valid loss: 2.893569231033325\n",
      "Epoch: 1633: Train loss: 3.3587734699249268 Valid loss: 2.8933115005493164\n",
      "Epoch: 1634: Train loss: 3.3575596809387207 Valid loss: 2.8930563926696777\n",
      "Epoch: 1635: Train loss: 3.3563475608825684 Valid loss: 2.8927907943725586\n",
      "Epoch: 1636: Train loss: 3.3551361560821533 Valid loss: 2.8925423622131348\n",
      "Epoch: 1637: Train loss: 3.3539321422576904 Valid loss: 2.892306089401245\n",
      "Epoch: 1638: Train loss: 3.3527278900146484 Valid loss: 2.892059803009033\n",
      "Epoch: 1639: Train loss: 3.3515281677246094 Valid loss: 2.8918251991271973\n",
      "Epoch: 1640: Train loss: 3.3503308296203613 Valid loss: 2.8915982246398926\n",
      "Epoch: 1641: Train loss: 3.3491358757019043 Valid loss: 2.8913657665252686\n",
      "Epoch: 1642: Train loss: 3.3479442596435547 Valid loss: 2.8911454677581787\n",
      "Epoch: 1643: Train loss: 3.3467533588409424 Valid loss: 2.8909285068511963\n",
      "Epoch: 1644: Train loss: 3.3455698490142822 Valid loss: 2.890723943710327\n",
      "Epoch: 1645: Train loss: 3.3443844318389893 Valid loss: 2.8905134201049805\n",
      "Epoch: 1646: Train loss: 3.343203544616699 Valid loss: 2.8903093338012695\n",
      "Epoch: 1647: Train loss: 3.3420252799987793 Valid loss: 2.890117883682251\n",
      "Epoch: 1648: Train loss: 3.3408517837524414 Valid loss: 2.889925956726074\n",
      "Epoch: 1649: Train loss: 3.3396787643432617 Valid loss: 2.889744997024536\n",
      "Epoch: 1650: Train loss: 3.338508129119873 Valid loss: 2.8895726203918457\n",
      "Epoch: 1651: Train loss: 3.3373422622680664 Valid loss: 2.8893935680389404\n",
      "Epoch: 1652: Train loss: 3.3361752033233643 Valid loss: 2.8892223834991455\n",
      "Epoch: 1653: Train loss: 3.3350136280059814 Valid loss: 2.8890540599823\n",
      "Epoch: 1654: Train loss: 3.333857536315918 Valid loss: 2.8888978958129883\n",
      "Epoch: 1655: Train loss: 3.332698345184326 Valid loss: 2.8887441158294678\n",
      "Epoch: 1656: Train loss: 3.3315467834472656 Valid loss: 2.888584613800049\n",
      "Epoch: 1657: Train loss: 3.330396890640259 Valid loss: 2.888446807861328\n",
      "Epoch: 1658: Train loss: 3.3292455673217773 Valid loss: 2.8883018493652344\n",
      "Epoch: 1659: Train loss: 3.32810115814209 Valid loss: 2.888169765472412\n",
      "Epoch: 1660: Train loss: 3.3269596099853516 Valid loss: 2.88804030418396\n",
      "Epoch: 1661: Train loss: 3.325817823410034 Valid loss: 2.887913465499878\n",
      "Epoch: 1662: Train loss: 3.3246796131134033 Valid loss: 2.8877995014190674\n",
      "Epoch: 1663: Train loss: 3.3235461711883545 Valid loss: 2.887678623199463\n",
      "Epoch: 1664: Train loss: 3.3224148750305176 Valid loss: 2.8875696659088135\n",
      "Epoch: 1665: Train loss: 3.321284294128418 Valid loss: 2.887463331222534\n",
      "Epoch: 1666: Train loss: 3.3201539516448975 Valid loss: 2.887364625930786\n",
      "Epoch: 1667: Train loss: 3.3190319538116455 Valid loss: 2.887259006500244\n",
      "Epoch: 1668: Train loss: 3.3179101943969727 Valid loss: 2.8871703147888184\n",
      "Epoch: 1669: Train loss: 3.3167924880981445 Valid loss: 2.8870849609375\n",
      "Epoch: 1670: Train loss: 3.3156750202178955 Valid loss: 2.8869967460632324\n",
      "Epoch: 1671: Train loss: 3.3145604133605957 Valid loss: 2.886920690536499\n",
      "Epoch: 1672: Train loss: 3.3134520053863525 Valid loss: 2.886843204498291\n",
      "Epoch: 1673: Train loss: 3.312342643737793 Valid loss: 2.886776924133301\n",
      "Epoch: 1674: Train loss: 3.311235189437866 Valid loss: 2.886709213256836\n",
      "Epoch: 1675: Train loss: 3.310131072998047 Valid loss: 2.8866443634033203\n",
      "Epoch: 1676: Train loss: 3.309030055999756 Valid loss: 2.8865952491760254\n",
      "Epoch: 1677: Train loss: 3.307934284210205 Valid loss: 2.886549472808838\n",
      "Epoch: 1678: Train loss: 3.306838035583496 Valid loss: 2.8864920139312744\n",
      "Epoch: 1679: Train loss: 3.3057425022125244 Valid loss: 2.88645601272583\n",
      "Epoch: 1680: Train loss: 3.3046560287475586 Valid loss: 2.886417865753174\n",
      "Epoch: 1681: Train loss: 3.3035647869110107 Valid loss: 2.8863911628723145\n",
      "Epoch: 1682: Train loss: 3.302478551864624 Valid loss: 2.8863539695739746\n",
      "Epoch: 1683: Train loss: 3.3013968467712402 Valid loss: 2.8863277435302734\n",
      "Epoch: 1684: Train loss: 3.3003158569335938 Valid loss: 2.8863086700439453\n",
      "Epoch: 1685: Train loss: 3.2992398738861084 Valid loss: 2.8863019943237305\n",
      "Epoch: 1686: Train loss: 3.298163414001465 Valid loss: 2.8862924575805664\n",
      "Epoch: 1687: Train loss: 3.297089099884033 Valid loss: 2.8862764835357666\n",
      "Epoch: 1688: Train loss: 3.296020030975342 Valid loss: 2.886277198791504\n",
      "Epoch: 1689: Train loss: 3.294952869415283 Valid loss: 2.886279344558716\n",
      "Epoch: 1690: Train loss: 3.293886423110962 Valid loss: 2.8862946033477783\n",
      "Epoch: 1691: Train loss: 3.2928247451782227 Valid loss: 2.886303186416626\n",
      "Epoch: 1692: Train loss: 3.2917630672454834 Valid loss: 2.886322498321533\n",
      "Epoch: 1693: Train loss: 3.2907052040100098 Valid loss: 2.8863446712493896\n",
      "Epoch: 1694: Train loss: 3.289649486541748 Valid loss: 2.8863697052001953\n",
      "Epoch: 1695: Train loss: 3.2885961532592773 Valid loss: 2.8863961696624756\n",
      "Epoch: 1696: Train loss: 3.287545919418335 Valid loss: 2.886425256729126\n",
      "Epoch: 1697: Train loss: 3.286496877670288 Valid loss: 2.8864660263061523\n",
      "Epoch: 1698: Train loss: 3.285452127456665 Valid loss: 2.886509418487549\n",
      "Epoch: 1699: Train loss: 3.2844090461730957 Valid loss: 2.8865549564361572\n",
      "Epoch: 1700: Train loss: 3.2833683490753174 Valid loss: 2.8866024017333984\n",
      "Epoch: 1701: Train loss: 3.282329797744751 Valid loss: 2.8866524696350098\n",
      "Epoch: 1702: Train loss: 3.2812936305999756 Valid loss: 2.8867053985595703\n",
      "Epoch: 1703: Train loss: 3.2802581787109375 Valid loss: 2.8867790699005127\n",
      "Epoch: 1704: Train loss: 3.2792272567749023 Valid loss: 2.8868408203125\n",
      "Epoch: 1705: Train loss: 3.2781996726989746 Valid loss: 2.886914014816284\n",
      "Epoch: 1706: Train loss: 3.2771739959716797 Valid loss: 2.886983871459961\n",
      "Epoch: 1707: Train loss: 3.2761480808258057 Valid loss: 2.887057304382324\n",
      "Epoch: 1708: Train loss: 3.2751262187957764 Valid loss: 2.887146234512329\n",
      "Epoch: 1709: Train loss: 3.274108409881592 Valid loss: 2.887237071990967\n",
      "Epoch: 1710: Train loss: 3.273090124130249 Valid loss: 2.887326955795288\n",
      "Epoch: 1711: Train loss: 3.272075653076172 Valid loss: 2.8874266147613525\n",
      "Epoch: 1712: Train loss: 3.271064043045044 Valid loss: 2.8875210285186768\n",
      "Epoch: 1713: Train loss: 3.270054817199707 Valid loss: 2.887624740600586\n",
      "Epoch: 1714: Train loss: 3.269047737121582 Valid loss: 2.8877227306365967\n",
      "Epoch: 1715: Train loss: 3.26804256439209 Valid loss: 2.8878402709960938\n",
      "Epoch: 1716: Train loss: 3.2670412063598633 Valid loss: 2.887956142425537\n",
      "Epoch: 1717: Train loss: 3.266038417816162 Valid loss: 2.8880746364593506\n",
      "Epoch: 1718: Train loss: 3.265040397644043 Valid loss: 2.8881988525390625\n",
      "Epoch: 1719: Train loss: 3.2640469074249268 Valid loss: 2.8883252143859863\n",
      "Epoch: 1720: Train loss: 3.2630527019500732 Valid loss: 2.888458728790283\n",
      "Epoch: 1721: Train loss: 3.262061834335327 Valid loss: 2.8885984420776367\n",
      "Epoch: 1722: Train loss: 3.2610740661621094 Valid loss: 2.8887319564819336\n",
      "Epoch: 1723: Train loss: 3.2600879669189453 Valid loss: 2.888867139816284\n",
      "Epoch: 1724: Train loss: 3.259101152420044 Valid loss: 2.8890132904052734\n",
      "Epoch: 1725: Train loss: 3.258120536804199 Valid loss: 2.8891704082489014\n",
      "Epoch: 1726: Train loss: 3.2571401596069336 Valid loss: 2.889324903488159\n",
      "Epoch: 1727: Train loss: 3.25616455078125 Valid loss: 2.889477491378784\n",
      "Epoch: 1728: Train loss: 3.255188465118408 Valid loss: 2.889641046524048\n",
      "Epoch: 1729: Train loss: 3.254215955734253 Valid loss: 2.889810562133789\n",
      "Epoch: 1730: Train loss: 3.2532458305358887 Valid loss: 2.889974355697632\n",
      "Epoch: 1731: Train loss: 3.2522783279418945 Valid loss: 2.890148639678955\n",
      "Epoch: 1732: Train loss: 3.251312255859375 Valid loss: 2.8903331756591797\n",
      "Epoch: 1733: Train loss: 3.2503490447998047 Valid loss: 2.8905112743377686\n",
      "Epoch: 1734: Train loss: 3.249384880065918 Valid loss: 2.8906915187835693\n",
      "Epoch: 1735: Train loss: 3.248427391052246 Valid loss: 2.8908774852752686\n",
      "Epoch: 1736: Train loss: 3.247471809387207 Valid loss: 2.8910748958587646\n",
      "Epoch: 1737: Train loss: 3.2465174198150635 Valid loss: 2.8912601470947266\n",
      "Epoch: 1738: Train loss: 3.245563507080078 Valid loss: 2.8914709091186523\n",
      "Epoch: 1739: Train loss: 3.2446136474609375 Valid loss: 2.8916735649108887\n",
      "Epoch: 1740: Train loss: 3.243666410446167 Valid loss: 2.891878366470337\n",
      "Epoch: 1741: Train loss: 3.2427194118499756 Valid loss: 2.8920845985412598\n",
      "Epoch: 1742: Train loss: 3.241776704788208 Valid loss: 2.8922982215881348\n",
      "Epoch: 1743: Train loss: 3.2408361434936523 Valid loss: 2.892509937286377\n",
      "Epoch: 1744: Train loss: 3.239894390106201 Valid loss: 2.892730236053467\n",
      "Epoch: 1745: Train loss: 3.2389602661132812 Valid loss: 2.8929622173309326\n",
      "Epoch: 1746: Train loss: 3.238022804260254 Valid loss: 2.893187999725342\n",
      "Epoch: 1747: Train loss: 3.2370924949645996 Valid loss: 2.89341402053833\n",
      "Epoch: 1748: Train loss: 3.236161231994629 Valid loss: 2.8936569690704346\n",
      "Epoch: 1749: Train loss: 3.235232353210449 Valid loss: 2.893892288208008\n",
      "Epoch: 1750: Train loss: 3.234306812286377 Valid loss: 2.8941330909729004\n",
      "Epoch: 1751: Train loss: 3.233383893966675 Valid loss: 2.894385814666748\n",
      "Epoch: 1752: Train loss: 3.2324612140655518 Valid loss: 2.8946313858032227\n",
      "Epoch: 1753: Train loss: 3.2315402030944824 Valid loss: 2.8948864936828613\n",
      "Epoch: 1754: Train loss: 3.2306222915649414 Valid loss: 2.8951356410980225\n",
      "Epoch: 1755: Train loss: 3.229708671569824 Valid loss: 2.8953895568847656\n",
      "Epoch: 1756: Train loss: 3.228797197341919 Valid loss: 2.8956546783447266\n",
      "Epoch: 1757: Train loss: 3.2278852462768555 Valid loss: 2.8959176540374756\n",
      "Epoch: 1758: Train loss: 3.2269749641418457 Valid loss: 2.8961994647979736\n",
      "Epoch: 1759: Train loss: 3.226067304611206 Valid loss: 2.89646577835083\n",
      "Epoch: 1760: Train loss: 3.2251620292663574 Valid loss: 2.896742343902588\n",
      "Epoch: 1761: Train loss: 3.224260091781616 Valid loss: 2.897020101547241\n",
      "Epoch: 1762: Train loss: 3.2233612537384033 Valid loss: 2.8973045349121094\n",
      "Epoch: 1763: Train loss: 3.2224621772766113 Valid loss: 2.897594690322876\n",
      "Epoch: 1764: Train loss: 3.2215657234191895 Valid loss: 2.8978781700134277\n",
      "Epoch: 1765: Train loss: 3.2206714153289795 Valid loss: 2.89817214012146\n",
      "Epoch: 1766: Train loss: 3.219780445098877 Valid loss: 2.8984670639038086\n",
      "Epoch: 1767: Train loss: 3.218890905380249 Valid loss: 2.8987679481506348\n",
      "Epoch: 1768: Train loss: 3.2180025577545166 Valid loss: 2.89907169342041\n",
      "Epoch: 1769: Train loss: 3.217116594314575 Valid loss: 2.8993849754333496\n",
      "Epoch: 1770: Train loss: 3.216231346130371 Valid loss: 2.899690866470337\n",
      "Epoch: 1771: Train loss: 3.21535062789917 Valid loss: 2.9000046253204346\n",
      "Epoch: 1772: Train loss: 3.2144711017608643 Valid loss: 2.900318145751953\n",
      "Epoch: 1773: Train loss: 3.213593006134033 Valid loss: 2.9006428718566895\n",
      "Epoch: 1774: Train loss: 3.2127163410186768 Valid loss: 2.9009602069854736\n",
      "Epoch: 1775: Train loss: 3.211845874786377 Valid loss: 2.9012868404388428\n",
      "Epoch: 1776: Train loss: 3.2109735012054443 Valid loss: 2.9016213417053223\n",
      "Epoch: 1777: Train loss: 3.210103750228882 Valid loss: 2.901947259902954\n",
      "Epoch: 1778: Train loss: 3.2092363834381104 Valid loss: 2.9022834300994873\n",
      "Epoch: 1779: Train loss: 3.20837140083313 Valid loss: 2.902620553970337\n",
      "Epoch: 1780: Train loss: 3.2075071334838867 Valid loss: 2.9029507637023926\n",
      "Epoch: 1781: Train loss: 3.2066471576690674 Valid loss: 2.9033050537109375\n",
      "Epoch: 1782: Train loss: 3.2057857513427734 Valid loss: 2.9036521911621094\n",
      "Epoch: 1783: Train loss: 3.204930305480957 Valid loss: 2.9040005207061768\n",
      "Epoch: 1784: Train loss: 3.204075813293457 Valid loss: 2.9043588638305664\n",
      "Epoch: 1785: Train loss: 3.203220844268799 Valid loss: 2.9047152996063232\n",
      "Epoch: 1786: Train loss: 3.2023701667785645 Valid loss: 2.905081272125244\n",
      "Epoch: 1787: Train loss: 3.2015206813812256 Valid loss: 2.905439615249634\n",
      "Epoch: 1788: Train loss: 3.200672149658203 Valid loss: 2.905799150466919\n",
      "Epoch: 1789: Train loss: 3.1998274326324463 Valid loss: 2.906174421310425\n",
      "Epoch: 1790: Train loss: 3.198983669281006 Valid loss: 2.906550884246826\n",
      "Epoch: 1791: Train loss: 3.1981430053710938 Valid loss: 2.90691876411438\n",
      "Epoch: 1792: Train loss: 3.197303533554077 Valid loss: 2.907296895980835\n",
      "Epoch: 1793: Train loss: 3.1964666843414307 Valid loss: 2.907682418823242\n",
      "Epoch: 1794: Train loss: 3.1956305503845215 Valid loss: 2.908068895339966\n",
      "Epoch: 1795: Train loss: 3.1947975158691406 Valid loss: 2.908447265625\n",
      "Epoch: 1796: Train loss: 3.1939661502838135 Valid loss: 2.9088356494903564\n",
      "Epoch: 1797: Train loss: 3.193136215209961 Valid loss: 2.9092392921447754\n",
      "Epoch: 1798: Train loss: 3.1923067569732666 Valid loss: 2.909635066986084\n",
      "Epoch: 1799: Train loss: 3.191481351852417 Valid loss: 2.9100325107574463\n",
      "Epoch: 1800: Train loss: 3.1906583309173584 Valid loss: 2.9104299545288086\n",
      "Epoch: 1801: Train loss: 3.189835548400879 Valid loss: 2.910844087600708\n",
      "Epoch: 1802: Train loss: 3.1890151500701904 Valid loss: 2.9112396240234375\n",
      "Epoch: 1803: Train loss: 3.1881961822509766 Valid loss: 2.911654472351074\n",
      "Epoch: 1804: Train loss: 3.1873815059661865 Valid loss: 2.912062406539917\n",
      "Epoch: 1805: Train loss: 3.1865663528442383 Valid loss: 2.9124855995178223\n",
      "Epoch: 1806: Train loss: 3.1857542991638184 Valid loss: 2.9128997325897217\n",
      "Epoch: 1807: Train loss: 3.184943199157715 Valid loss: 2.913316249847412\n",
      "Epoch: 1808: Train loss: 3.1841349601745605 Valid loss: 2.9137468338012695\n",
      "Epoch: 1809: Train loss: 3.1833276748657227 Valid loss: 2.914170265197754\n",
      "Epoch: 1810: Train loss: 3.1825244426727295 Valid loss: 2.9146034717559814\n",
      "Epoch: 1811: Train loss: 3.181720495223999 Valid loss: 2.9150373935699463\n",
      "Epoch: 1812: Train loss: 3.1809194087982178 Valid loss: 2.915477752685547\n",
      "Epoch: 1813: Train loss: 3.180119514465332 Valid loss: 2.9159183502197266\n",
      "Epoch: 1814: Train loss: 3.17932391166687 Valid loss: 2.916350841522217\n",
      "Epoch: 1815: Train loss: 3.1785268783569336 Valid loss: 2.916799545288086\n",
      "Epoch: 1816: Train loss: 3.177734136581421 Valid loss: 2.917248249053955\n",
      "Epoch: 1817: Train loss: 3.176942825317383 Valid loss: 2.917689085006714\n",
      "Epoch: 1818: Train loss: 3.176151752471924 Valid loss: 2.9181458950042725\n",
      "Epoch: 1819: Train loss: 3.175363779067993 Valid loss: 2.918602705001831\n",
      "Epoch: 1820: Train loss: 3.174576759338379 Valid loss: 2.9190516471862793\n",
      "Epoch: 1821: Train loss: 3.1737935543060303 Valid loss: 2.9195187091827393\n",
      "Epoch: 1822: Train loss: 3.1730103492736816 Valid loss: 2.9199838638305664\n",
      "Epoch: 1823: Train loss: 3.1722280979156494 Valid loss: 2.9204487800598145\n",
      "Epoch: 1824: Train loss: 3.171449899673462 Valid loss: 2.9209063053131104\n",
      "Epoch: 1825: Train loss: 3.1706724166870117 Valid loss: 2.921388626098633\n",
      "Epoch: 1826: Train loss: 3.1698968410491943 Valid loss: 2.9218616485595703\n",
      "Epoch: 1827: Train loss: 3.1691250801086426 Valid loss: 2.9223361015319824\n",
      "Epoch: 1828: Train loss: 3.16835355758667 Valid loss: 2.922816753387451\n",
      "Epoch: 1829: Train loss: 3.1675827503204346 Valid loss: 2.923297643661499\n",
      "Epoch: 1830: Train loss: 3.166815757751465 Valid loss: 2.9237780570983887\n",
      "Epoch: 1831: Train loss: 3.1660497188568115 Valid loss: 2.924267292022705\n",
      "Epoch: 1832: Train loss: 3.1652843952178955 Valid loss: 2.924755096435547\n",
      "Epoch: 1833: Train loss: 3.164522171020508 Valid loss: 2.925243854522705\n",
      "Epoch: 1834: Train loss: 3.163759708404541 Valid loss: 2.925739049911499\n",
      "Epoch: 1835: Train loss: 3.1629996299743652 Valid loss: 2.9262351989746094\n",
      "Epoch: 1836: Train loss: 3.1622416973114014 Valid loss: 2.926731586456299\n",
      "Epoch: 1837: Train loss: 3.1614878177642822 Valid loss: 2.92724347114563\n",
      "Epoch: 1838: Train loss: 3.1607325077056885 Valid loss: 2.9277379512786865\n",
      "Epoch: 1839: Train loss: 3.159982919692993 Valid loss: 2.928241729736328\n",
      "Epoch: 1840: Train loss: 3.1592321395874023 Valid loss: 2.9287517070770264\n",
      "Epoch: 1841: Train loss: 3.158482551574707 Valid loss: 2.9292712211608887\n",
      "Epoch: 1842: Train loss: 3.1577374935150146 Valid loss: 2.929781198501587\n",
      "Epoch: 1843: Train loss: 3.1569900512695312 Valid loss: 2.930299758911133\n",
      "Epoch: 1844: Train loss: 3.156247615814209 Valid loss: 2.9308252334594727\n",
      "Epoch: 1845: Train loss: 3.1555070877075195 Valid loss: 2.9313437938690186\n",
      "Epoch: 1846: Train loss: 3.1547656059265137 Valid loss: 2.9318675994873047\n",
      "Epoch: 1847: Train loss: 3.154029369354248 Valid loss: 2.9323837757110596\n",
      "Epoch: 1848: Train loss: 3.153290271759033 Valid loss: 2.9329237937927246\n",
      "Epoch: 1849: Train loss: 3.152555227279663 Valid loss: 2.9334545135498047\n",
      "Epoch: 1850: Train loss: 3.1518232822418213 Valid loss: 2.9339864253997803\n",
      "Epoch: 1851: Train loss: 3.151092529296875 Valid loss: 2.9345240592956543\n",
      "Epoch: 1852: Train loss: 3.150360584259033 Valid loss: 2.9350619316101074\n",
      "Epoch: 1853: Train loss: 3.149635076522827 Valid loss: 2.935600757598877\n",
      "Epoch: 1854: Train loss: 3.1489076614379883 Valid loss: 2.936136245727539\n",
      "Epoch: 1855: Train loss: 3.1481828689575195 Valid loss: 2.936690092086792\n",
      "Epoch: 1856: Train loss: 3.147461414337158 Valid loss: 2.937225341796875\n",
      "Epoch: 1857: Train loss: 3.1467392444610596 Valid loss: 2.937777042388916\n",
      "Epoch: 1858: Train loss: 3.1460185050964355 Valid loss: 2.938328742980957\n",
      "Epoch: 1859: Train loss: 3.1453025341033936 Valid loss: 2.9388864040374756\n",
      "Epoch: 1860: Train loss: 3.1445863246917725 Valid loss: 2.939443826675415\n",
      "Epoch: 1861: Train loss: 3.1438729763031006 Valid loss: 2.939992904663086\n",
      "Epoch: 1862: Train loss: 3.1431589126586914 Valid loss: 2.9405574798583984\n",
      "Epoch: 1863: Train loss: 3.1424474716186523 Valid loss: 2.9411306381225586\n",
      "Epoch: 1864: Train loss: 3.141737937927246 Valid loss: 2.9417009353637695\n",
      "Epoch: 1865: Train loss: 3.141030788421631 Valid loss: 2.9422638416290283\n",
      "Epoch: 1866: Train loss: 3.140324592590332 Valid loss: 2.9428346157073975\n",
      "Epoch: 1867: Train loss: 3.139620065689087 Valid loss: 2.9434027671813965\n",
      "Epoch: 1868: Train loss: 3.138916492462158 Valid loss: 2.9439783096313477\n",
      "Epoch: 1869: Train loss: 3.1382153034210205 Valid loss: 2.9445526599884033\n",
      "Epoch: 1870: Train loss: 3.1375162601470947 Valid loss: 2.945136785507202\n",
      "Epoch: 1871: Train loss: 3.1368179321289062 Valid loss: 2.9457106590270996\n",
      "Epoch: 1872: Train loss: 3.136122703552246 Valid loss: 2.9462995529174805\n",
      "Epoch: 1873: Train loss: 3.1354281902313232 Valid loss: 2.9468798637390137\n",
      "Epoch: 1874: Train loss: 3.134734869003296 Valid loss: 2.947467088699341\n",
      "Epoch: 1875: Train loss: 3.134042263031006 Valid loss: 2.9480624198913574\n",
      "Epoch: 1876: Train loss: 3.133352041244507 Valid loss: 2.948647975921631\n",
      "Epoch: 1877: Train loss: 3.1326658725738525 Valid loss: 2.9492413997650146\n",
      "Epoch: 1878: Train loss: 3.1319801807403564 Valid loss: 2.949842929840088\n",
      "Epoch: 1879: Train loss: 3.1312925815582275 Valid loss: 2.95044207572937\n",
      "Epoch: 1880: Train loss: 3.1306097507476807 Valid loss: 2.9510397911071777\n",
      "Epoch: 1881: Train loss: 3.1299283504486084 Valid loss: 2.9516444206237793\n",
      "Epoch: 1882: Train loss: 3.1292474269866943 Valid loss: 2.952249526977539\n",
      "Epoch: 1883: Train loss: 3.1285696029663086 Valid loss: 2.952853202819824\n",
      "Epoch: 1884: Train loss: 3.1278913021087646 Valid loss: 2.9534640312194824\n",
      "Epoch: 1885: Train loss: 3.1272168159484863 Valid loss: 2.9540648460388184\n",
      "Epoch: 1886: Train loss: 3.126542806625366 Valid loss: 2.9546902179718018\n",
      "Epoch: 1887: Train loss: 3.125870704650879 Valid loss: 2.9552974700927734\n",
      "Epoch: 1888: Train loss: 3.125199317932129 Valid loss: 2.9559199810028076\n",
      "Epoch: 1889: Train loss: 3.124530792236328 Valid loss: 2.9565417766571045\n",
      "Epoch: 1890: Train loss: 3.1238622665405273 Valid loss: 2.9571540355682373\n",
      "Epoch: 1891: Train loss: 3.123196840286255 Valid loss: 2.957772731781006\n",
      "Epoch: 1892: Train loss: 3.122532367706299 Valid loss: 2.958399534225464\n",
      "Epoch: 1893: Train loss: 3.1218690872192383 Valid loss: 2.9590249061584473\n",
      "Epoch: 1894: Train loss: 3.1212072372436523 Valid loss: 2.959657907485962\n",
      "Epoch: 1895: Train loss: 3.1205482482910156 Valid loss: 2.9602882862091064\n",
      "Epoch: 1896: Train loss: 3.1198883056640625 Valid loss: 2.960918426513672\n",
      "Epoch: 1897: Train loss: 3.119232416152954 Valid loss: 2.961562156677246\n",
      "Epoch: 1898: Train loss: 3.1185755729675293 Valid loss: 2.962198257446289\n",
      "Epoch: 1899: Train loss: 3.117924213409424 Valid loss: 2.9628243446350098\n",
      "Epoch: 1900: Train loss: 3.117272138595581 Valid loss: 2.9634740352630615\n",
      "Epoch: 1901: Train loss: 3.116621971130371 Valid loss: 2.964115619659424\n",
      "Epoch: 1902: Train loss: 3.1159732341766357 Valid loss: 2.9647536277770996\n",
      "Epoch: 1903: Train loss: 3.1153228282928467 Valid loss: 2.9653995037078857\n",
      "Epoch: 1904: Train loss: 3.1146767139434814 Valid loss: 2.9660515785217285\n",
      "Epoch: 1905: Train loss: 3.114032745361328 Valid loss: 2.966703414916992\n",
      "Epoch: 1906: Train loss: 3.1133882999420166 Valid loss: 2.967352867126465\n",
      "Epoch: 1907: Train loss: 3.112748384475708 Valid loss: 2.968001365661621\n",
      "Epoch: 1908: Train loss: 3.1121087074279785 Valid loss: 2.9686646461486816\n",
      "Epoch: 1909: Train loss: 3.1114702224731445 Valid loss: 2.969318151473999\n",
      "Epoch: 1910: Train loss: 3.110833168029785 Valid loss: 2.9699864387512207\n",
      "Epoch: 1911: Train loss: 3.110196590423584 Valid loss: 2.9706363677978516\n",
      "Epoch: 1912: Train loss: 3.1095614433288574 Valid loss: 2.971301555633545\n",
      "Epoch: 1913: Train loss: 3.108928918838501 Valid loss: 2.971975088119507\n",
      "Epoch: 1914: Train loss: 3.1082983016967773 Valid loss: 2.972630500793457\n",
      "Epoch: 1915: Train loss: 3.107670307159424 Valid loss: 2.9733004570007324\n",
      "Epoch: 1916: Train loss: 3.107041120529175 Valid loss: 2.973968982696533\n",
      "Epoch: 1917: Train loss: 3.106414318084717 Valid loss: 2.9746439456939697\n",
      "Epoch: 1918: Train loss: 3.1057896614074707 Valid loss: 2.9753191471099854\n",
      "Epoch: 1919: Train loss: 3.1051650047302246 Valid loss: 2.9759907722473145\n",
      "Epoch: 1920: Train loss: 3.104541301727295 Valid loss: 2.976670742034912\n",
      "Epoch: 1921: Train loss: 3.10392165184021 Valid loss: 2.977346897125244\n",
      "Epoch: 1922: Train loss: 3.103302240371704 Valid loss: 2.978031635284424\n",
      "Epoch: 1923: Train loss: 3.10268497467041 Valid loss: 2.97871470451355\n",
      "Epoch: 1924: Train loss: 3.102067708969116 Valid loss: 2.9793944358825684\n",
      "Epoch: 1925: Train loss: 3.1014530658721924 Valid loss: 2.9800822734832764\n",
      "Epoch: 1926: Train loss: 3.1008386611938477 Valid loss: 2.9807679653167725\n",
      "Epoch: 1927: Train loss: 3.100226640701294 Valid loss: 2.9814603328704834\n",
      "Epoch: 1928: Train loss: 3.099616050720215 Valid loss: 2.982151985168457\n",
      "Epoch: 1929: Train loss: 3.099006175994873 Valid loss: 2.9828405380249023\n",
      "Epoch: 1930: Train loss: 3.0983986854553223 Valid loss: 2.9835364818573\n",
      "Epoch: 1931: Train loss: 3.0977914333343506 Valid loss: 2.984229803085327\n",
      "Epoch: 1932: Train loss: 3.09718656539917 Valid loss: 2.984921932220459\n",
      "Epoch: 1933: Train loss: 3.096583366394043 Valid loss: 2.9856197834014893\n",
      "Epoch: 1934: Train loss: 3.0959808826446533 Valid loss: 2.986333131790161\n",
      "Epoch: 1935: Train loss: 3.0953803062438965 Valid loss: 2.9870352745056152\n",
      "Epoch: 1936: Train loss: 3.0947797298431396 Valid loss: 2.987744092941284\n",
      "Epoch: 1937: Train loss: 3.094181776046753 Valid loss: 2.9884424209594727\n",
      "Epoch: 1938: Train loss: 3.093583583831787 Valid loss: 2.989147424697876\n",
      "Epoch: 1939: Train loss: 3.092988967895508 Valid loss: 2.9898593425750732\n",
      "Epoch: 1940: Train loss: 3.0923948287963867 Valid loss: 2.9905776977539062\n",
      "Epoch: 1941: Train loss: 3.091804027557373 Valid loss: 2.9912779331207275\n",
      "Epoch: 1942: Train loss: 3.0912115573883057 Valid loss: 2.9919915199279785\n",
      "Epoch: 1943: Train loss: 3.0906217098236084 Valid loss: 2.992713212966919\n",
      "Epoch: 1944: Train loss: 3.0900332927703857 Valid loss: 2.9934229850769043\n",
      "Epoch: 1945: Train loss: 3.089446783065796 Valid loss: 2.9941420555114746\n",
      "Epoch: 1946: Train loss: 3.0888612270355225 Valid loss: 2.9948651790618896\n",
      "Epoch: 1947: Train loss: 3.0882771015167236 Valid loss: 2.995579242706299\n",
      "Epoch: 1948: Train loss: 3.087693929672241 Valid loss: 2.9963080883026123\n",
      "Epoch: 1949: Train loss: 3.08711314201355 Valid loss: 2.9970343112945557\n",
      "Epoch: 1950: Train loss: 3.086531639099121 Valid loss: 2.9977588653564453\n",
      "Epoch: 1951: Train loss: 3.0859527587890625 Valid loss: 2.998488664627075\n",
      "Epoch: 1952: Train loss: 3.085374116897583 Valid loss: 2.999225616455078\n",
      "Epoch: 1953: Train loss: 3.084798574447632 Valid loss: 2.999950408935547\n",
      "Epoch: 1954: Train loss: 3.0842254161834717 Valid loss: 3.0006847381591797\n",
      "Epoch: 1955: Train loss: 3.0836496353149414 Valid loss: 3.0014145374298096\n",
      "Epoch: 1956: Train loss: 3.0830788612365723 Valid loss: 3.0021514892578125\n",
      "Epoch: 1957: Train loss: 3.0825071334838867 Valid loss: 3.0028867721557617\n",
      "Epoch: 1958: Train loss: 3.081939220428467 Valid loss: 3.003628730773926\n",
      "Epoch: 1959: Train loss: 3.081369161605835 Valid loss: 3.004366874694824\n",
      "Epoch: 1960: Train loss: 3.080803871154785 Valid loss: 3.0051114559173584\n",
      "Epoch: 1961: Train loss: 3.0802371501922607 Valid loss: 3.005845308303833\n",
      "Epoch: 1962: Train loss: 3.0796725749969482 Valid loss: 3.006593704223633\n",
      "Epoch: 1963: Train loss: 3.0791103839874268 Valid loss: 3.0073328018188477\n",
      "Epoch: 1964: Train loss: 3.0785489082336426 Valid loss: 3.008085012435913\n",
      "Epoch: 1965: Train loss: 3.077988624572754 Valid loss: 3.0088279247283936\n",
      "Epoch: 1966: Train loss: 3.0774285793304443 Valid loss: 3.0095772743225098\n",
      "Epoch: 1967: Train loss: 3.076873302459717 Valid loss: 3.0103306770324707\n",
      "Epoch: 1968: Train loss: 3.0763139724731445 Valid loss: 3.0110838413238525\n",
      "Epoch: 1969: Train loss: 3.0757603645324707 Valid loss: 3.0118408203125\n",
      "Epoch: 1970: Train loss: 3.075206756591797 Valid loss: 3.012589931488037\n",
      "Epoch: 1971: Train loss: 3.074653148651123 Valid loss: 3.0133519172668457\n",
      "Epoch: 1972: Train loss: 3.0741031169891357 Valid loss: 3.014103412628174\n",
      "Epoch: 1973: Train loss: 3.073552370071411 Valid loss: 3.0148611068725586\n",
      "Epoch: 1974: Train loss: 3.073004722595215 Valid loss: 3.015634536743164\n",
      "Epoch: 1975: Train loss: 3.072458028793335 Valid loss: 3.0163960456848145\n",
      "Epoch: 1976: Train loss: 3.0719118118286133 Valid loss: 3.0171544551849365\n",
      "Epoch: 1977: Train loss: 3.07136607170105 Valid loss: 3.0179197788238525\n",
      "Epoch: 1978: Train loss: 3.0708234310150146 Valid loss: 3.0186831951141357\n",
      "Epoch: 1979: Train loss: 3.0702805519104004 Valid loss: 3.0194504261016846\n",
      "Epoch: 1980: Train loss: 3.0697402954101562 Valid loss: 3.0202255249023438\n",
      "Epoch: 1981: Train loss: 3.0692014694213867 Valid loss: 3.0209884643554688\n",
      "Epoch: 1982: Train loss: 3.06866192817688 Valid loss: 3.021759033203125\n",
      "Epoch: 1983: Train loss: 3.068125009536743 Valid loss: 3.0225348472595215\n",
      "Epoch: 1984: Train loss: 3.0675878524780273 Valid loss: 3.0233075618743896\n",
      "Epoch: 1985: Train loss: 3.0670528411865234 Valid loss: 3.0240867137908936\n",
      "Epoch: 1986: Train loss: 3.066519260406494 Valid loss: 3.0248632431030273\n",
      "Epoch: 1987: Train loss: 3.065988779067993 Valid loss: 3.0256378650665283\n",
      "Epoch: 1988: Train loss: 3.0654571056365967 Valid loss: 3.026418924331665\n",
      "Epoch: 1989: Train loss: 3.064927101135254 Valid loss: 3.0272035598754883\n",
      "Epoch: 1990: Train loss: 3.0644006729125977 Valid loss: 3.0279879570007324\n",
      "Epoch: 1991: Train loss: 3.0638723373413086 Valid loss: 3.0287587642669678\n",
      "Epoch: 1992: Train loss: 3.0633463859558105 Valid loss: 3.0295467376708984\n",
      "Epoch: 1993: Train loss: 3.0628204345703125 Valid loss: 3.030331611633301\n",
      "Epoch: 1994: Train loss: 3.062298059463501 Valid loss: 3.0311203002929688\n",
      "Epoch: 1995: Train loss: 3.061774730682373 Valid loss: 3.0319085121154785\n",
      "Epoch: 1996: Train loss: 3.061253070831299 Valid loss: 3.0327017307281494\n",
      "Epoch: 1997: Train loss: 3.060734987258911 Valid loss: 3.033482313156128\n",
      "Epoch: 1998: Train loss: 3.0602145195007324 Valid loss: 3.034280300140381\n",
      "Epoch: 1999: Train loss: 3.0596983432769775 Valid loss: 3.035064220428467\n",
      "Epoch: 2000: Train loss: 3.0591824054718018 Valid loss: 3.0358641147613525\n",
      "Epoch: 2001: Train loss: 3.0586676597595215 Valid loss: 3.0366621017456055\n",
      "Epoch: 2002: Train loss: 3.058152437210083 Valid loss: 3.037454605102539\n",
      "Epoch: 2003: Train loss: 3.057640790939331 Valid loss: 3.0382556915283203\n",
      "Epoch: 2004: Train loss: 3.057128667831421 Valid loss: 3.0390431880950928\n",
      "Epoch: 2005: Train loss: 3.056617498397827 Valid loss: 3.0398542881011963\n",
      "Epoch: 2006: Train loss: 3.0561094284057617 Valid loss: 3.0406463146209717\n",
      "Epoch: 2007: Train loss: 3.0556013584136963 Valid loss: 3.0414512157440186\n",
      "Epoch: 2008: Train loss: 3.0550947189331055 Valid loss: 3.042262077331543\n",
      "Epoch: 2009: Train loss: 3.0545883178710938 Valid loss: 3.043062210083008\n",
      "Epoch: 2010: Train loss: 3.054084300994873 Valid loss: 3.04386830329895\n",
      "Epoch: 2011: Train loss: 3.0535805225372314 Valid loss: 3.044663667678833\n",
      "Epoch: 2012: Train loss: 3.05307936668396 Valid loss: 3.045480966567993\n",
      "Epoch: 2013: Train loss: 3.052579164505005 Valid loss: 3.0462870597839355\n",
      "Epoch: 2014: Train loss: 3.052079200744629 Valid loss: 3.047098159790039\n",
      "Epoch: 2015: Train loss: 3.0515806674957275 Valid loss: 3.0479073524475098\n",
      "Epoch: 2016: Train loss: 3.0510830879211426 Valid loss: 3.0487213134765625\n",
      "Epoch: 2017: Train loss: 3.0505876541137695 Valid loss: 3.0495336055755615\n",
      "Epoch: 2018: Train loss: 3.0500926971435547 Valid loss: 3.0503413677215576\n",
      "Epoch: 2019: Train loss: 3.0495996475219727 Valid loss: 3.0511631965637207\n",
      "Epoch: 2020: Train loss: 3.049105644226074 Valid loss: 3.051976442337036\n",
      "Epoch: 2021: Train loss: 3.0486135482788086 Valid loss: 3.0528011322021484\n",
      "Epoch: 2022: Train loss: 3.048123359680176 Valid loss: 3.0536134243011475\n",
      "Epoch: 2023: Train loss: 3.0476348400115967 Valid loss: 3.0544333457946777\n",
      "Epoch: 2024: Train loss: 3.047147512435913 Valid loss: 3.0552496910095215\n",
      "Epoch: 2025: Train loss: 3.0466597080230713 Valid loss: 3.0560789108276367\n",
      "Epoch: 2026: Train loss: 3.046173095703125 Valid loss: 3.056898355484009\n",
      "Epoch: 2027: Train loss: 3.0456888675689697 Valid loss: 3.057722330093384\n",
      "Epoch: 2028: Train loss: 3.0452041625976562 Valid loss: 3.0585432052612305\n",
      "Epoch: 2029: Train loss: 3.0447230339050293 Valid loss: 3.059370279312134\n",
      "Epoch: 2030: Train loss: 3.044240713119507 Valid loss: 3.0601940155029297\n",
      "Epoch: 2031: Train loss: 3.0437605381011963 Valid loss: 3.0610222816467285\n",
      "Epoch: 2032: Train loss: 3.0432825088500977 Valid loss: 3.0618491172790527\n",
      "Epoch: 2033: Train loss: 3.0428035259246826 Valid loss: 3.0626723766326904\n",
      "Epoch: 2034: Train loss: 3.042327404022217 Valid loss: 3.0635080337524414\n",
      "Epoch: 2035: Train loss: 3.0418508052825928 Valid loss: 3.064335346221924\n",
      "Epoch: 2036: Train loss: 3.0413780212402344 Valid loss: 3.065174102783203\n",
      "Epoch: 2037: Train loss: 3.040904998779297 Valid loss: 3.0660107135772705\n",
      "Epoch: 2038: Train loss: 3.040432929992676 Valid loss: 3.0668349266052246\n",
      "Epoch: 2039: Train loss: 3.0399608612060547 Valid loss: 3.067674160003662\n",
      "Epoch: 2040: Train loss: 3.0394887924194336 Valid loss: 3.068510055541992\n",
      "Epoch: 2041: Train loss: 3.03902006149292 Valid loss: 3.0693492889404297\n",
      "Epoch: 2042: Train loss: 3.038553237915039 Valid loss: 3.0701961517333984\n",
      "Epoch: 2043: Train loss: 3.038085699081421 Valid loss: 3.0710301399230957\n",
      "Epoch: 2044: Train loss: 3.0376195907592773 Valid loss: 3.071868896484375\n",
      "Epoch: 2045: Train loss: 3.0371572971343994 Valid loss: 3.0727052688598633\n",
      "Epoch: 2046: Train loss: 3.036691427230835 Valid loss: 3.0735466480255127\n",
      "Epoch: 2047: Train loss: 3.0362281799316406 Valid loss: 3.0743939876556396\n",
      "Epoch: 2048: Train loss: 3.035768747329712 Valid loss: 3.07523775100708\n",
      "Epoch: 2049: Train loss: 3.0353074073791504 Valid loss: 3.0760796070098877\n",
      "Epoch: 2050: Train loss: 3.0348477363586426 Valid loss: 3.076932430267334\n",
      "Epoch: 2051: Train loss: 3.034390449523926 Valid loss: 3.0777740478515625\n",
      "Epoch: 2052: Train loss: 3.0339319705963135 Valid loss: 3.0786123275756836\n",
      "Epoch: 2053: Train loss: 3.0334768295288086 Valid loss: 3.079465866088867\n",
      "Epoch: 2054: Train loss: 3.0330207347869873 Valid loss: 3.0803143978118896\n",
      "Epoch: 2055: Train loss: 3.032567024230957 Valid loss: 3.0811610221862793\n",
      "Epoch: 2056: Train loss: 3.032114028930664 Valid loss: 3.082012176513672\n",
      "Epoch: 2057: Train loss: 3.031662702560425 Valid loss: 3.0828704833984375\n",
      "Epoch: 2058: Train loss: 3.031212091445923 Valid loss: 3.083730697631836\n",
      "Epoch: 2059: Train loss: 3.03076171875 Valid loss: 3.0845718383789062\n",
      "Epoch: 2060: Train loss: 3.0303142070770264 Valid loss: 3.0854263305664062\n",
      "Epoch: 2061: Train loss: 3.0298643112182617 Valid loss: 3.086286783218384\n",
      "Epoch: 2062: Train loss: 3.0294182300567627 Valid loss: 3.0871360301971436\n",
      "Epoch: 2063: Train loss: 3.0289738178253174 Valid loss: 3.0879898071289062\n",
      "Epoch: 2064: Train loss: 3.028529644012451 Valid loss: 3.0888476371765137\n",
      "Epoch: 2065: Train loss: 3.028083324432373 Valid loss: 3.089719533920288\n",
      "Epoch: 2066: Train loss: 3.027642250061035 Valid loss: 3.0905721187591553\n",
      "Epoch: 2067: Train loss: 3.027200937271118 Valid loss: 3.091428756713867\n",
      "Epoch: 2068: Train loss: 3.0267608165740967 Valid loss: 3.0922904014587402\n",
      "Epoch: 2069: Train loss: 3.0263209342956543 Valid loss: 3.093156337738037\n",
      "Epoch: 2070: Train loss: 3.025881290435791 Valid loss: 3.094019889831543\n",
      "Epoch: 2071: Train loss: 3.0254456996917725 Valid loss: 3.094878673553467\n",
      "Epoch: 2072: Train loss: 3.0250089168548584 Valid loss: 3.0957448482513428\n",
      "Epoch: 2073: Train loss: 3.024574041366577 Valid loss: 3.0966057777404785\n",
      "Epoch: 2074: Train loss: 3.0241386890411377 Valid loss: 3.0974740982055664\n",
      "Epoch: 2075: Train loss: 3.023707151412964 Valid loss: 3.0983357429504395\n",
      "Epoch: 2076: Train loss: 3.0232748985290527 Valid loss: 3.099203586578369\n",
      "Epoch: 2077: Train loss: 3.022844076156616 Valid loss: 3.100069284439087\n",
      "Epoch: 2078: Train loss: 3.0224132537841797 Valid loss: 3.1009390354156494\n",
      "Epoch: 2079: Train loss: 3.021984338760376 Valid loss: 3.1018052101135254\n",
      "Epoch: 2080: Train loss: 3.0215556621551514 Valid loss: 3.1026840209960938\n",
      "Epoch: 2081: Train loss: 3.0211286544799805 Valid loss: 3.103543281555176\n",
      "Epoch: 2082: Train loss: 3.020702600479126 Valid loss: 3.1044161319732666\n",
      "Epoch: 2083: Train loss: 3.020277261734009 Valid loss: 3.1052920818328857\n",
      "Epoch: 2084: Train loss: 3.019852876663208 Valid loss: 3.106165647506714\n",
      "Epoch: 2085: Train loss: 3.019429922103882 Valid loss: 3.1070356369018555\n",
      "Epoch: 2086: Train loss: 3.019007682800293 Valid loss: 3.1079020500183105\n",
      "Epoch: 2087: Train loss: 3.0185861587524414 Valid loss: 3.1087822914123535\n",
      "Epoch: 2088: Train loss: 3.0181658267974854 Valid loss: 3.109658718109131\n",
      "Epoch: 2089: Train loss: 3.0177462100982666 Valid loss: 3.1105473041534424\n",
      "Epoch: 2090: Train loss: 3.0173280239105225 Valid loss: 3.111415386199951\n",
      "Epoch: 2091: Train loss: 3.0169119834899902 Valid loss: 3.1122889518737793\n",
      "Epoch: 2092: Train loss: 3.0164942741394043 Valid loss: 3.113166570663452\n",
      "Epoch: 2093: Train loss: 3.016078472137451 Valid loss: 3.114048957824707\n",
      "Epoch: 2094: Train loss: 3.0156641006469727 Valid loss: 3.114928722381592\n",
      "Epoch: 2095: Train loss: 3.0152509212493896 Valid loss: 3.11580491065979\n",
      "Epoch: 2096: Train loss: 3.0148391723632812 Valid loss: 3.116694450378418\n",
      "Epoch: 2097: Train loss: 3.01442551612854 Valid loss: 3.117570161819458\n",
      "Epoch: 2098: Train loss: 3.0140180587768555 Valid loss: 3.118452548980713\n",
      "Epoch: 2099: Train loss: 3.0136070251464844 Valid loss: 3.1193299293518066\n",
      "Epoch: 2100: Train loss: 3.0131993293762207 Valid loss: 3.1202220916748047\n",
      "Epoch: 2101: Train loss: 3.0127909183502197 Valid loss: 3.121108055114746\n",
      "Epoch: 2102: Train loss: 3.0123846530914307 Valid loss: 3.1219918727874756\n",
      "Epoch: 2103: Train loss: 3.0119786262512207 Valid loss: 3.1228718757629395\n",
      "Epoch: 2104: Train loss: 3.0115745067596436 Valid loss: 3.1237571239471436\n",
      "Epoch: 2105: Train loss: 3.011168956756592 Valid loss: 3.124645709991455\n",
      "Epoch: 2106: Train loss: 3.0107669830322266 Valid loss: 3.125530242919922\n",
      "Epoch: 2107: Train loss: 3.0103628635406494 Valid loss: 3.1264190673828125\n",
      "Epoch: 2108: Train loss: 3.009962797164917 Valid loss: 3.127305507659912\n",
      "Epoch: 2109: Train loss: 3.0095627307891846 Valid loss: 3.1281962394714355\n",
      "Epoch: 2110: Train loss: 3.009161949157715 Valid loss: 3.129092216491699\n",
      "Epoch: 2111: Train loss: 3.0087640285491943 Valid loss: 3.1299822330474854\n",
      "Epoch: 2112: Train loss: 3.0083673000335693 Valid loss: 3.1308631896972656\n",
      "Epoch: 2113: Train loss: 3.0079729557037354 Valid loss: 3.131756067276001\n",
      "Epoch: 2114: Train loss: 3.007575750350952 Valid loss: 3.132652997970581\n",
      "Epoch: 2115: Train loss: 3.007180690765381 Valid loss: 3.1335463523864746\n",
      "Epoch: 2116: Train loss: 3.006788969039917 Valid loss: 3.134443759918213\n",
      "Epoch: 2117: Train loss: 3.0063960552215576 Valid loss: 3.1353282928466797\n",
      "Epoch: 2118: Train loss: 3.00600266456604 Valid loss: 3.1362195014953613\n",
      "Epoch: 2119: Train loss: 3.00561261177063 Valid loss: 3.1371138095855713\n",
      "Epoch: 2120: Train loss: 3.005221366882324 Valid loss: 3.138014554977417\n",
      "Epoch: 2121: Train loss: 3.0048322677612305 Valid loss: 3.1389169692993164\n",
      "Epoch: 2122: Train loss: 3.0044445991516113 Valid loss: 3.1398088932037354\n",
      "Epoch: 2123: Train loss: 3.004056215286255 Valid loss: 3.140695810317993\n",
      "Epoch: 2124: Train loss: 3.0036723613739014 Valid loss: 3.141597270965576\n",
      "Epoch: 2125: Train loss: 3.0032854080200195 Valid loss: 3.1424925327301025\n",
      "Epoch: 2126: Train loss: 3.0029025077819824 Valid loss: 3.143393039703369\n",
      "Epoch: 2127: Train loss: 3.0025172233581543 Valid loss: 3.1442975997924805\n",
      "Epoch: 2128: Train loss: 3.0021355152130127 Valid loss: 3.145190715789795\n",
      "Epoch: 2129: Train loss: 3.001753330230713 Valid loss: 3.146096706390381\n",
      "Epoch: 2130: Train loss: 3.0013723373413086 Valid loss: 3.1469993591308594\n",
      "Epoch: 2131: Train loss: 3.0009920597076416 Valid loss: 3.1478967666625977\n",
      "Epoch: 2132: Train loss: 3.000612735748291 Valid loss: 3.1487979888916016\n",
      "Epoch: 2133: Train loss: 3.0002355575561523 Valid loss: 3.1497035026550293\n",
      "Epoch: 2134: Train loss: 2.999857187271118 Valid loss: 3.150606155395508\n",
      "Epoch: 2135: Train loss: 2.999481439590454 Valid loss: 3.1515145301818848\n",
      "Epoch: 2136: Train loss: 2.99910569190979 Valid loss: 3.1524083614349365\n",
      "Epoch: 2137: Train loss: 2.998730182647705 Valid loss: 3.1533141136169434\n",
      "Epoch: 2138: Train loss: 2.9983558654785156 Valid loss: 3.15421724319458\n",
      "Epoch: 2139: Train loss: 2.997983694076538 Valid loss: 3.1551167964935303\n",
      "Epoch: 2140: Train loss: 2.9976110458374023 Valid loss: 3.156029224395752\n",
      "Epoch: 2141: Train loss: 2.997237205505371 Valid loss: 3.156938076019287\n",
      "Epoch: 2142: Train loss: 2.9968690872192383 Valid loss: 3.157841682434082\n",
      "Epoch: 2143: Train loss: 2.9964990615844727 Valid loss: 3.1587491035461426\n",
      "Epoch: 2144: Train loss: 2.9961302280426025 Valid loss: 3.159660577774048\n",
      "Epoch: 2145: Train loss: 2.995762825012207 Valid loss: 3.1605682373046875\n",
      "Epoch: 2146: Train loss: 2.9953954219818115 Valid loss: 3.1614718437194824\n",
      "Epoch: 2147: Train loss: 2.995030641555786 Valid loss: 3.162381172180176\n",
      "Epoch: 2148: Train loss: 2.9946649074554443 Valid loss: 3.1632862091064453\n",
      "Epoch: 2149: Train loss: 2.9942984580993652 Valid loss: 3.164194107055664\n",
      "Epoch: 2150: Train loss: 2.9939355850219727 Valid loss: 3.1651058197021484\n",
      "Epoch: 2151: Train loss: 2.9935719966888428 Valid loss: 3.166022777557373\n",
      "Epoch: 2152: Train loss: 2.9932119846343994 Valid loss: 3.166935920715332\n",
      "Epoch: 2153: Train loss: 2.9928512573242188 Valid loss: 3.167837381362915\n",
      "Epoch: 2154: Train loss: 2.9924914836883545 Valid loss: 3.1687581539154053\n",
      "Epoch: 2155: Train loss: 2.992130994796753 Valid loss: 3.1696672439575195\n",
      "Epoch: 2156: Train loss: 2.991771697998047 Valid loss: 3.1705803871154785\n",
      "Epoch: 2157: Train loss: 2.9914159774780273 Valid loss: 3.1714882850646973\n",
      "Epoch: 2158: Train loss: 2.9910571575164795 Valid loss: 3.17240047454834\n",
      "Epoch: 2159: Train loss: 2.9907026290893555 Valid loss: 3.173309564590454\n",
      "Epoch: 2160: Train loss: 2.990347385406494 Valid loss: 3.17423152923584\n",
      "Epoch: 2161: Train loss: 2.989992618560791 Valid loss: 3.175140857696533\n",
      "Epoch: 2162: Train loss: 2.9896388053894043 Valid loss: 3.1760613918304443\n",
      "Epoch: 2163: Train loss: 2.9892866611480713 Valid loss: 3.176969289779663\n",
      "Epoch: 2164: Train loss: 2.9889349937438965 Valid loss: 3.1778903007507324\n",
      "Epoch: 2165: Train loss: 2.9885826110839844 Valid loss: 3.1788055896759033\n",
      "Epoch: 2166: Train loss: 2.988233804702759 Valid loss: 3.1797266006469727\n",
      "Epoch: 2167: Train loss: 2.987884521484375 Valid loss: 3.18064284324646\n",
      "Epoch: 2168: Train loss: 2.9875361919403076 Valid loss: 3.1815555095672607\n",
      "Epoch: 2169: Train loss: 2.9871885776519775 Valid loss: 3.182464599609375\n",
      "Epoch: 2170: Train loss: 2.986839771270752 Valid loss: 3.183384895324707\n",
      "Epoch: 2171: Train loss: 2.986495018005371 Valid loss: 3.184299945831299\n",
      "Epoch: 2172: Train loss: 2.98614764213562 Valid loss: 3.185227870941162\n",
      "Epoch: 2173: Train loss: 2.985804319381714 Valid loss: 3.186142921447754\n",
      "Epoch: 2174: Train loss: 2.985460042953491 Valid loss: 3.1870617866516113\n",
      "Epoch: 2175: Train loss: 2.985116958618164 Valid loss: 3.187979221343994\n",
      "Epoch: 2176: Train loss: 2.9847750663757324 Valid loss: 3.188899040222168\n",
      "Epoch: 2177: Train loss: 2.9844350814819336 Valid loss: 3.1898226737976074\n",
      "Epoch: 2178: Train loss: 2.984092950820923 Valid loss: 3.190742254257202\n",
      "Epoch: 2179: Train loss: 2.9837541580200195 Valid loss: 3.1916568279266357\n",
      "Epoch: 2180: Train loss: 2.9834163188934326 Valid loss: 3.1925840377807617\n",
      "Epoch: 2181: Train loss: 2.983077049255371 Valid loss: 3.1935062408447266\n",
      "Epoch: 2182: Train loss: 2.9827382564544678 Valid loss: 3.1944332122802734\n",
      "Epoch: 2183: Train loss: 2.9824018478393555 Valid loss: 3.1953482627868652\n",
      "Epoch: 2184: Train loss: 2.982067346572876 Valid loss: 3.196265935897827\n",
      "Epoch: 2185: Train loss: 2.9817309379577637 Valid loss: 3.197188377380371\n",
      "Epoch: 2186: Train loss: 2.9813992977142334 Valid loss: 3.1981148719787598\n",
      "Epoch: 2187: Train loss: 2.9810640811920166 Valid loss: 3.1990370750427246\n",
      "Epoch: 2188: Train loss: 2.9807324409484863 Valid loss: 3.1999621391296387\n",
      "Epoch: 2189: Train loss: 2.9803974628448486 Valid loss: 3.200882911682129\n",
      "Epoch: 2190: Train loss: 2.9800682067871094 Valid loss: 3.2018167972564697\n",
      "Epoch: 2191: Train loss: 2.97973895072937 Valid loss: 3.2027359008789062\n",
      "Epoch: 2192: Train loss: 2.97940731048584 Valid loss: 3.2036590576171875\n",
      "Epoch: 2193: Train loss: 2.979079246520996 Valid loss: 3.204580307006836\n",
      "Epoch: 2194: Train loss: 2.9787509441375732 Valid loss: 3.2055039405822754\n",
      "Epoch: 2195: Train loss: 2.9784250259399414 Valid loss: 3.206432819366455\n",
      "Epoch: 2196: Train loss: 2.978097677230835 Valid loss: 3.2073562145233154\n",
      "Epoch: 2197: Train loss: 2.977771043777466 Valid loss: 3.2082834243774414\n",
      "Epoch: 2198: Train loss: 2.9774484634399414 Valid loss: 3.2092065811157227\n",
      "Epoch: 2199: Train loss: 2.977123260498047 Valid loss: 3.2101335525512695\n",
      "Epoch: 2200: Train loss: 2.976799964904785 Valid loss: 3.2110626697540283\n",
      "Epoch: 2201: Train loss: 2.976475715637207 Valid loss: 3.2119877338409424\n",
      "Epoch: 2202: Train loss: 2.976154088973999 Valid loss: 3.212916851043701\n",
      "Epoch: 2203: Train loss: 2.975832939147949 Valid loss: 3.2138490676879883\n",
      "Epoch: 2204: Train loss: 2.9755125045776367 Valid loss: 3.214768409729004\n",
      "Epoch: 2205: Train loss: 2.9751927852630615 Valid loss: 3.2156925201416016\n",
      "Epoch: 2206: Train loss: 2.974874973297119 Valid loss: 3.2166218757629395\n",
      "Epoch: 2207: Train loss: 2.974555253982544 Valid loss: 3.2175452709198\n",
      "Epoch: 2208: Train loss: 2.974238157272339 Valid loss: 3.218472719192505\n",
      "Epoch: 2209: Train loss: 2.9739208221435547 Valid loss: 3.219405174255371\n",
      "Epoch: 2210: Train loss: 2.9736056327819824 Valid loss: 3.220339775085449\n",
      "Epoch: 2211: Train loss: 2.9732909202575684 Valid loss: 3.2212610244750977\n",
      "Epoch: 2212: Train loss: 2.9729747772216797 Valid loss: 3.222195625305176\n",
      "Epoch: 2213: Train loss: 2.9726622104644775 Valid loss: 3.2231240272521973\n",
      "Epoch: 2214: Train loss: 2.97234845161438 Valid loss: 3.2240567207336426\n",
      "Epoch: 2215: Train loss: 2.972036123275757 Valid loss: 3.224984884262085\n",
      "Epoch: 2216: Train loss: 2.97172474861145 Valid loss: 3.2259154319763184\n",
      "Epoch: 2217: Train loss: 2.9714136123657227 Valid loss: 3.226841926574707\n",
      "Epoch: 2218: Train loss: 2.9711036682128906 Valid loss: 3.227771759033203\n",
      "Epoch: 2219: Train loss: 2.970794439315796 Valid loss: 3.2287065982818604\n",
      "Epoch: 2220: Train loss: 2.970486640930176 Valid loss: 3.2296295166015625\n",
      "Epoch: 2221: Train loss: 2.9701766967773438 Valid loss: 3.230556011199951\n",
      "Epoch: 2222: Train loss: 2.969870090484619 Valid loss: 3.2314934730529785\n",
      "Epoch: 2223: Train loss: 2.96956467628479 Valid loss: 3.2324271202087402\n",
      "Epoch: 2224: Train loss: 2.9692580699920654 Valid loss: 3.2333555221557617\n",
      "Epoch: 2225: Train loss: 2.9689531326293945 Valid loss: 3.2342872619628906\n",
      "Epoch: 2226: Train loss: 2.9686498641967773 Valid loss: 3.235213279724121\n",
      "Epoch: 2227: Train loss: 2.9683451652526855 Valid loss: 3.236144542694092\n",
      "Epoch: 2228: Train loss: 2.9680402278900146 Valid loss: 3.237079381942749\n",
      "Epoch: 2229: Train loss: 2.967738389968872 Valid loss: 3.2380175590515137\n",
      "Epoch: 2230: Train loss: 2.9674363136291504 Valid loss: 3.238950490951538\n",
      "Epoch: 2231: Train loss: 2.967137336730957 Valid loss: 3.239877700805664\n",
      "Epoch: 2232: Train loss: 2.9668357372283936 Valid loss: 3.2408175468444824\n",
      "Epoch: 2233: Train loss: 2.966536521911621 Valid loss: 3.24174427986145\n",
      "Epoch: 2234: Train loss: 2.966238021850586 Valid loss: 3.2426819801330566\n",
      "Epoch: 2235: Train loss: 2.9659407138824463 Valid loss: 3.243617534637451\n",
      "Epoch: 2236: Train loss: 2.965641498565674 Valid loss: 3.244553327560425\n",
      "Epoch: 2237: Train loss: 2.9653446674346924 Valid loss: 3.2454848289489746\n",
      "Epoch: 2238: Train loss: 2.9650495052337646 Valid loss: 3.2464146614074707\n",
      "Epoch: 2239: Train loss: 2.9647531509399414 Valid loss: 3.247346878051758\n",
      "Epoch: 2240: Train loss: 2.964460849761963 Valid loss: 3.248274803161621\n",
      "Epoch: 2241: Train loss: 2.9641666412353516 Valid loss: 3.2492058277130127\n",
      "Epoch: 2242: Train loss: 2.9638724327087402 Valid loss: 3.250131607055664\n",
      "Epoch: 2243: Train loss: 2.963581085205078 Valid loss: 3.251070022583008\n",
      "Epoch: 2244: Train loss: 2.9632887840270996 Valid loss: 3.2520041465759277\n",
      "Epoch: 2245: Train loss: 2.9629969596862793 Valid loss: 3.2529420852661133\n",
      "Epoch: 2246: Train loss: 2.9627068042755127 Valid loss: 3.253873825073242\n",
      "Epoch: 2247: Train loss: 2.9624178409576416 Valid loss: 3.254807949066162\n",
      "Epoch: 2248: Train loss: 2.9621288776397705 Valid loss: 3.255739450454712\n",
      "Epoch: 2249: Train loss: 2.961839199066162 Valid loss: 3.256674289703369\n",
      "Epoch: 2250: Train loss: 2.961552858352661 Valid loss: 3.2576112747192383\n",
      "Epoch: 2251: Train loss: 2.9612646102905273 Valid loss: 3.258542537689209\n",
      "Epoch: 2252: Train loss: 2.960977792739868 Valid loss: 3.2594799995422363\n",
      "Epoch: 2253: Train loss: 2.960693359375 Valid loss: 3.26041841506958\n",
      "Epoch: 2254: Train loss: 2.9604084491729736 Valid loss: 3.2613537311553955\n",
      "Epoch: 2255: Train loss: 2.9601240158081055 Valid loss: 3.262282133102417\n",
      "Epoch: 2256: Train loss: 2.9598398208618164 Valid loss: 3.2632229328155518\n",
      "Epoch: 2257: Train loss: 2.9595553874969482 Valid loss: 3.264159679412842\n",
      "Epoch: 2258: Train loss: 2.9592745304107666 Valid loss: 3.2650890350341797\n",
      "Epoch: 2259: Train loss: 2.9589920043945312 Valid loss: 3.266023635864258\n",
      "Epoch: 2260: Train loss: 2.958712100982666 Valid loss: 3.266953945159912\n",
      "Epoch: 2261: Train loss: 2.958430767059326 Valid loss: 3.267895221710205\n",
      "Epoch: 2262: Train loss: 2.95815110206604 Valid loss: 3.2688231468200684\n",
      "Epoch: 2263: Train loss: 2.9578700065612793 Valid loss: 3.2697620391845703\n",
      "Epoch: 2264: Train loss: 2.9575934410095215 Valid loss: 3.2706971168518066\n",
      "Epoch: 2265: Train loss: 2.957315683364868 Valid loss: 3.271636962890625\n",
      "Epoch: 2266: Train loss: 2.957037925720215 Valid loss: 3.2725770473480225\n",
      "Epoch: 2267: Train loss: 2.9567601680755615 Valid loss: 3.273512840270996\n",
      "Epoch: 2268: Train loss: 2.9564857482910156 Valid loss: 3.2744460105895996\n",
      "Epoch: 2269: Train loss: 2.956210136413574 Valid loss: 3.2753796577453613\n",
      "Epoch: 2270: Train loss: 2.9559340476989746 Valid loss: 3.276318073272705\n",
      "Epoch: 2271: Train loss: 2.955660343170166 Valid loss: 3.2772586345672607\n",
      "Epoch: 2272: Train loss: 2.9553871154785156 Valid loss: 3.2781872749328613\n",
      "Epoch: 2273: Train loss: 2.9551148414611816 Valid loss: 3.2791266441345215\n",
      "Epoch: 2274: Train loss: 2.9548418521881104 Valid loss: 3.280062198638916\n",
      "Epoch: 2275: Train loss: 2.9545719623565674 Valid loss: 3.2809901237487793\n",
      "Epoch: 2276: Train loss: 2.9542996883392334 Valid loss: 3.2819228172302246\n",
      "Epoch: 2277: Train loss: 2.9540305137634277 Valid loss: 3.2828593254089355\n",
      "Epoch: 2278: Train loss: 2.9537620544433594 Valid loss: 3.283798933029175\n",
      "Epoch: 2279: Train loss: 2.953491687774658 Valid loss: 3.2847342491149902\n",
      "Epoch: 2280: Train loss: 2.9532225131988525 Valid loss: 3.2856714725494385\n",
      "Epoch: 2281: Train loss: 2.9529542922973633 Valid loss: 3.286604404449463\n",
      "Epoch: 2282: Train loss: 2.9526889324188232 Valid loss: 3.287548542022705\n",
      "Epoch: 2283: Train loss: 2.952423334121704 Valid loss: 3.288480281829834\n",
      "Epoch: 2284: Train loss: 2.9521570205688477 Valid loss: 3.289414405822754\n",
      "Epoch: 2285: Train loss: 2.951892137527466 Valid loss: 3.290351390838623\n",
      "Epoch: 2286: Train loss: 2.9516263008117676 Valid loss: 3.291282892227173\n",
      "Epoch: 2287: Train loss: 2.9513630867004395 Valid loss: 3.2922191619873047\n",
      "Epoch: 2288: Train loss: 2.951098918914795 Valid loss: 3.2931571006774902\n",
      "Epoch: 2289: Train loss: 2.9508352279663086 Valid loss: 3.2940924167633057\n",
      "Epoch: 2290: Train loss: 2.9505739212036133 Valid loss: 3.295029401779175\n",
      "Epoch: 2291: Train loss: 2.950312614440918 Valid loss: 3.2959699630737305\n",
      "Epoch: 2292: Train loss: 2.9500513076782227 Valid loss: 3.2968969345092773\n",
      "Epoch: 2293: Train loss: 2.949791431427002 Valid loss: 3.297834634780884\n",
      "Epoch: 2294: Train loss: 2.949531316757202 Valid loss: 3.2987680435180664\n",
      "Epoch: 2295: Train loss: 2.949273109436035 Valid loss: 3.2997050285339355\n",
      "Epoch: 2296: Train loss: 2.94901442527771 Valid loss: 3.300635814666748\n",
      "Epoch: 2297: Train loss: 2.9487578868865967 Valid loss: 3.301571846008301\n",
      "Epoch: 2298: Train loss: 2.948500871658325 Valid loss: 3.302509307861328\n",
      "Epoch: 2299: Train loss: 2.9482438564300537 Valid loss: 3.303441047668457\n",
      "Epoch: 2300: Train loss: 2.9479870796203613 Valid loss: 3.3043699264526367\n",
      "Epoch: 2301: Train loss: 2.9477317333221436 Valid loss: 3.3053083419799805\n",
      "Epoch: 2302: Train loss: 2.947476387023926 Valid loss: 3.30625057220459\n",
      "Epoch: 2303: Train loss: 2.9472227096557617 Valid loss: 3.3071932792663574\n",
      "Epoch: 2304: Train loss: 2.9469692707061768 Valid loss: 3.30812406539917\n",
      "Epoch: 2305: Train loss: 2.9467153549194336 Valid loss: 3.3090577125549316\n",
      "Epoch: 2306: Train loss: 2.9464635848999023 Valid loss: 3.309995174407959\n",
      "Epoch: 2307: Train loss: 2.9462108612060547 Valid loss: 3.310932159423828\n",
      "Epoch: 2308: Train loss: 2.945960760116577 Valid loss: 3.311859130859375\n",
      "Epoch: 2309: Train loss: 2.945709228515625 Valid loss: 3.3127970695495605\n",
      "Epoch: 2310: Train loss: 2.9454586505889893 Valid loss: 3.3137378692626953\n",
      "Epoch: 2311: Train loss: 2.945207357406616 Valid loss: 3.3146729469299316\n",
      "Epoch: 2312: Train loss: 2.944959878921509 Valid loss: 3.315605401992798\n",
      "Epoch: 2313: Train loss: 2.9447107315063477 Valid loss: 3.316537857055664\n",
      "Epoch: 2314: Train loss: 2.944462776184082 Valid loss: 3.317474842071533\n",
      "Epoch: 2315: Train loss: 2.944216728210449 Valid loss: 3.318406105041504\n",
      "Epoch: 2316: Train loss: 2.943969964981079 Valid loss: 3.3193421363830566\n",
      "Epoch: 2317: Train loss: 2.9437246322631836 Valid loss: 3.3202710151672363\n",
      "Epoch: 2318: Train loss: 2.9434776306152344 Valid loss: 3.32120418548584\n",
      "Epoch: 2319: Train loss: 2.943233013153076 Valid loss: 3.322139263153076\n",
      "Epoch: 2320: Train loss: 2.9429893493652344 Valid loss: 3.3230788707733154\n",
      "Epoch: 2321: Train loss: 2.942744493484497 Valid loss: 3.324005126953125\n",
      "Epoch: 2322: Train loss: 2.942502975463867 Valid loss: 3.324942111968994\n",
      "Epoch: 2323: Train loss: 2.9422595500946045 Valid loss: 3.325870990753174\n",
      "Epoch: 2324: Train loss: 2.9420177936553955 Valid loss: 3.326812267303467\n",
      "Epoch: 2325: Train loss: 2.9417757987976074 Valid loss: 3.3277487754821777\n",
      "Epoch: 2326: Train loss: 2.941535234451294 Valid loss: 3.3286795616149902\n",
      "Epoch: 2327: Train loss: 2.9412930011749268 Valid loss: 3.32961368560791\n",
      "Epoch: 2328: Train loss: 2.941054105758667 Valid loss: 3.3305418491363525\n",
      "Epoch: 2329: Train loss: 2.940814971923828 Valid loss: 3.331472873687744\n",
      "Epoch: 2330: Train loss: 2.9405760765075684 Valid loss: 3.332408905029297\n",
      "Epoch: 2331: Train loss: 2.9403393268585205 Valid loss: 3.333346366882324\n",
      "Epoch: 2332: Train loss: 2.9401004314422607 Valid loss: 3.334270477294922\n",
      "Epoch: 2333: Train loss: 2.939862012863159 Valid loss: 3.3352067470550537\n",
      "Epoch: 2334: Train loss: 2.9396276473999023 Valid loss: 3.336137056350708\n",
      "Epoch: 2335: Train loss: 2.93938946723938 Valid loss: 3.3370628356933594\n",
      "Epoch: 2336: Train loss: 2.939154624938965 Valid loss: 3.3379979133605957\n",
      "Epoch: 2337: Train loss: 2.9389214515686035 Valid loss: 3.338930130004883\n",
      "Epoch: 2338: Train loss: 2.9386866092681885 Valid loss: 3.339872360229492\n",
      "Epoch: 2339: Train loss: 2.9384522438049316 Valid loss: 3.3408010005950928\n",
      "Epoch: 2340: Train loss: 2.938218116760254 Valid loss: 3.3417296409606934\n",
      "Epoch: 2341: Train loss: 2.9379868507385254 Valid loss: 3.342662811279297\n",
      "Epoch: 2342: Train loss: 2.9377548694610596 Valid loss: 3.343599319458008\n",
      "Epoch: 2343: Train loss: 2.937523365020752 Valid loss: 3.34452223777771\n",
      "Epoch: 2344: Train loss: 2.937290906906128 Valid loss: 3.3454558849334717\n",
      "Epoch: 2345: Train loss: 2.9370603561401367 Valid loss: 3.3463850021362305\n",
      "Epoch: 2346: Train loss: 2.9368324279785156 Valid loss: 3.3473172187805176\n",
      "Epoch: 2347: Train loss: 2.9366016387939453 Valid loss: 3.348252534866333\n",
      "Epoch: 2348: Train loss: 2.936373710632324 Valid loss: 3.349181652069092\n",
      "Epoch: 2349: Train loss: 2.9361464977264404 Valid loss: 3.350106716156006\n",
      "Epoch: 2350: Train loss: 2.935918092727661 Valid loss: 3.3510398864746094\n",
      "Epoch: 2351: Train loss: 2.9356894493103027 Valid loss: 3.351968765258789\n",
      "Epoch: 2352: Train loss: 2.9354610443115234 Valid loss: 3.352900743484497\n",
      "Epoch: 2353: Train loss: 2.9352378845214844 Valid loss: 3.3538355827331543\n",
      "Epoch: 2354: Train loss: 2.9350109100341797 Valid loss: 3.354764938354492\n",
      "Epoch: 2355: Train loss: 2.9347846508026123 Valid loss: 3.355696678161621\n",
      "Epoch: 2356: Train loss: 2.9345600605010986 Valid loss: 3.356624126434326\n",
      "Epoch: 2357: Train loss: 2.9343366622924805 Valid loss: 3.357544422149658\n",
      "Epoch: 2358: Train loss: 2.934113025665283 Valid loss: 3.358468770980835\n",
      "Epoch: 2359: Train loss: 2.9338905811309814 Valid loss: 3.359396457672119\n",
      "Epoch: 2360: Train loss: 2.933669090270996 Valid loss: 3.3603272438049316\n",
      "Epoch: 2361: Train loss: 2.9334471225738525 Valid loss: 3.3612585067749023\n",
      "Epoch: 2362: Train loss: 2.9332261085510254 Valid loss: 3.3621931076049805\n",
      "Epoch: 2363: Train loss: 2.933001756668091 Valid loss: 3.3631234169006348\n",
      "Epoch: 2364: Train loss: 2.9327824115753174 Valid loss: 3.3640475273132324\n",
      "Epoch: 2365: Train loss: 2.932562828063965 Valid loss: 3.3649730682373047\n",
      "Epoch: 2366: Train loss: 2.9323432445526123 Valid loss: 3.365894079208374\n",
      "Epoch: 2367: Train loss: 2.932124137878418 Valid loss: 3.3668270111083984\n",
      "Epoch: 2368: Train loss: 2.9319067001342773 Valid loss: 3.3677539825439453\n",
      "Epoch: 2369: Train loss: 2.9316887855529785 Valid loss: 3.3686766624450684\n",
      "Epoch: 2370: Train loss: 2.9314699172973633 Valid loss: 3.36960768699646\n",
      "Epoch: 2371: Train loss: 2.9312520027160645 Valid loss: 3.3705341815948486\n",
      "Epoch: 2372: Train loss: 2.931037664413452 Valid loss: 3.3714637756347656\n",
      "Epoch: 2373: Train loss: 2.930821180343628 Valid loss: 3.372386932373047\n",
      "Epoch: 2374: Train loss: 2.9306039810180664 Valid loss: 3.3733134269714355\n",
      "Epoch: 2375: Train loss: 2.930389404296875 Valid loss: 3.3742427825927734\n",
      "Epoch: 2376: Train loss: 2.9301767349243164 Valid loss: 3.3751583099365234\n",
      "Epoch: 2377: Train loss: 2.9299628734588623 Valid loss: 3.376084804534912\n",
      "Epoch: 2378: Train loss: 2.9297492504119873 Valid loss: 3.3770065307617188\n",
      "Epoch: 2379: Train loss: 2.929537534713745 Valid loss: 3.3779380321502686\n",
      "Epoch: 2380: Train loss: 2.9293246269226074 Valid loss: 3.37886381149292\n",
      "Epoch: 2381: Train loss: 2.929112672805786 Valid loss: 3.3797922134399414\n",
      "Epoch: 2382: Train loss: 2.9289016723632812 Valid loss: 3.3807144165039062\n",
      "Epoch: 2383: Train loss: 2.92868971824646 Valid loss: 3.3816416263580322\n",
      "Epoch: 2384: Train loss: 2.928478956222534 Valid loss: 3.3825607299804688\n",
      "Epoch: 2385: Train loss: 2.9282686710357666 Valid loss: 3.3834829330444336\n",
      "Epoch: 2386: Train loss: 2.9280595779418945 Valid loss: 3.384402275085449\n",
      "Epoch: 2387: Train loss: 2.927851915359497 Valid loss: 3.3853373527526855\n",
      "Epoch: 2388: Train loss: 2.927640914916992 Valid loss: 3.386260509490967\n",
      "Epoch: 2389: Train loss: 2.9274353981018066 Valid loss: 3.3871848583221436\n",
      "Epoch: 2390: Train loss: 2.927227258682251 Valid loss: 3.3881030082702637\n",
      "Epoch: 2391: Train loss: 2.9270198345184326 Valid loss: 3.389025926589966\n",
      "Epoch: 2392: Train loss: 2.926814317703247 Valid loss: 3.3899426460266113\n",
      "Epoch: 2393: Train loss: 2.926607370376587 Valid loss: 3.390861988067627\n",
      "Epoch: 2394: Train loss: 2.9264028072357178 Valid loss: 3.391782760620117\n",
      "Epoch: 2395: Train loss: 2.926196336746216 Valid loss: 3.3927063941955566\n",
      "Epoch: 2396: Train loss: 2.925992488861084 Valid loss: 3.393625259399414\n",
      "Epoch: 2397: Train loss: 2.925788164138794 Valid loss: 3.394545078277588\n",
      "Epoch: 2398: Train loss: 2.9255833625793457 Valid loss: 3.3954696655273438\n",
      "Epoch: 2399: Train loss: 2.9253804683685303 Valid loss: 3.396388292312622\n",
      "Epoch: 2400: Train loss: 2.9251792430877686 Valid loss: 3.3973004817962646\n",
      "Epoch: 2401: Train loss: 2.9249768257141113 Valid loss: 3.3982322216033936\n",
      "Epoch: 2402: Train loss: 2.9247753620147705 Valid loss: 3.3991498947143555\n",
      "Epoch: 2403: Train loss: 2.924572706222534 Valid loss: 3.400068759918213\n",
      "Epoch: 2404: Train loss: 2.924372673034668 Valid loss: 3.4009904861450195\n",
      "Epoch: 2405: Train loss: 2.924171209335327 Valid loss: 3.4019076824188232\n",
      "Epoch: 2406: Train loss: 2.923973560333252 Valid loss: 3.402827739715576\n",
      "Epoch: 2407: Train loss: 2.9237730503082275 Valid loss: 3.4037418365478516\n",
      "Epoch: 2408: Train loss: 2.9235737323760986 Valid loss: 3.404658317565918\n",
      "Epoch: 2409: Train loss: 2.9233763217926025 Valid loss: 3.405585289001465\n",
      "Epoch: 2410: Train loss: 2.92317795753479 Valid loss: 3.406496524810791\n",
      "Epoch: 2411: Train loss: 2.9229812622070312 Valid loss: 3.407412528991699\n",
      "Epoch: 2412: Train loss: 2.922781229019165 Valid loss: 3.408329486846924\n",
      "Epoch: 2413: Train loss: 2.9225852489471436 Valid loss: 3.4092421531677246\n",
      "Epoch: 2414: Train loss: 2.9223899841308594 Valid loss: 3.4101574420928955\n",
      "Epoch: 2415: Train loss: 2.9221935272216797 Valid loss: 3.4110827445983887\n",
      "Epoch: 2416: Train loss: 2.921999216079712 Valid loss: 3.412001609802246\n",
      "Epoch: 2417: Train loss: 2.921804189682007 Valid loss: 3.412914276123047\n",
      "Epoch: 2418: Train loss: 2.921609401702881 Valid loss: 3.413830280303955\n",
      "Epoch: 2419: Train loss: 2.921417236328125 Valid loss: 3.414742946624756\n",
      "Epoch: 2420: Train loss: 2.9212234020233154 Valid loss: 3.4156551361083984\n",
      "Epoch: 2421: Train loss: 2.921029806137085 Valid loss: 3.4165701866149902\n",
      "Epoch: 2422: Train loss: 2.920835256576538 Valid loss: 3.4174892902374268\n",
      "Epoch: 2423: Train loss: 2.920644521713257 Valid loss: 3.418400526046753\n",
      "Epoch: 2424: Train loss: 2.9204518795013428 Valid loss: 3.419323682785034\n",
      "Epoch: 2425: Train loss: 2.9202609062194824 Valid loss: 3.420231580734253\n",
      "Epoch: 2426: Train loss: 2.920071601867676 Valid loss: 3.4211440086364746\n",
      "Epoch: 2427: Train loss: 2.9198827743530273 Valid loss: 3.4220499992370605\n",
      "Epoch: 2428: Train loss: 2.9196910858154297 Valid loss: 3.422964096069336\n",
      "Epoch: 2429: Train loss: 2.919501304626465 Valid loss: 3.4238829612731934\n",
      "Epoch: 2430: Train loss: 2.919312000274658 Valid loss: 3.424795150756836\n",
      "Epoch: 2431: Train loss: 2.9191248416900635 Valid loss: 3.4257030487060547\n",
      "Epoch: 2432: Train loss: 2.9189348220825195 Valid loss: 3.426612138748169\n",
      "Epoch: 2433: Train loss: 2.918747901916504 Valid loss: 3.427516460418701\n",
      "Epoch: 2434: Train loss: 2.9185597896575928 Valid loss: 3.428438186645508\n",
      "Epoch: 2435: Train loss: 2.9183740615844727 Valid loss: 3.4293460845947266\n",
      "Epoch: 2436: Train loss: 2.9181902408599854 Valid loss: 3.4302566051483154\n",
      "Epoch: 2437: Train loss: 2.9180028438568115 Valid loss: 3.4311609268188477\n",
      "Epoch: 2438: Train loss: 2.9178152084350586 Valid loss: 3.432077407836914\n",
      "Epoch: 2439: Train loss: 2.9176313877105713 Valid loss: 3.4329867362976074\n",
      "Epoch: 2440: Train loss: 2.9174466133117676 Valid loss: 3.4338974952697754\n",
      "Epoch: 2441: Train loss: 2.9172630310058594 Valid loss: 3.4348037242889404\n",
      "Epoch: 2442: Train loss: 2.917078971862793 Valid loss: 3.435710906982422\n",
      "Epoch: 2443: Train loss: 2.9168944358825684 Valid loss: 3.4366226196289062\n",
      "Epoch: 2444: Train loss: 2.9167113304138184 Valid loss: 3.43751859664917\n",
      "Epoch: 2445: Train loss: 2.916532039642334 Valid loss: 3.4384336471557617\n",
      "Epoch: 2446: Train loss: 2.916348457336426 Valid loss: 3.4393441677093506\n",
      "Epoch: 2447: Train loss: 2.916166067123413 Valid loss: 3.44024658203125\n",
      "Epoch: 2448: Train loss: 2.9159865379333496 Valid loss: 3.4411611557006836\n",
      "Epoch: 2449: Train loss: 2.915804862976074 Valid loss: 3.44206166267395\n",
      "Epoch: 2450: Train loss: 2.9156241416931152 Valid loss: 3.442972183227539\n",
      "Epoch: 2451: Train loss: 2.91544508934021 Valid loss: 3.443876028060913\n",
      "Epoch: 2452: Train loss: 2.915264129638672 Valid loss: 3.4447827339172363\n",
      "Epoch: 2453: Train loss: 2.9150850772857666 Valid loss: 3.445683479309082\n",
      "Epoch: 2454: Train loss: 2.9149081707000732 Valid loss: 3.4465866088867188\n",
      "Epoch: 2455: Train loss: 2.9147286415100098 Valid loss: 3.4474921226501465\n",
      "Epoch: 2456: Train loss: 2.91455078125 Valid loss: 3.4483985900878906\n",
      "Epoch: 2457: Train loss: 2.9143733978271484 Valid loss: 3.449300765991211\n",
      "Epoch: 2458: Train loss: 2.9141972064971924 Valid loss: 3.4502053260803223\n",
      "Epoch: 2459: Train loss: 2.9140193462371826 Valid loss: 3.451103687286377\n",
      "Epoch: 2460: Train loss: 2.913844108581543 Valid loss: 3.452004909515381\n",
      "Epoch: 2461: Train loss: 2.913668632507324 Valid loss: 3.452908515930176\n",
      "Epoch: 2462: Train loss: 2.913494110107422 Valid loss: 3.453812837600708\n",
      "Epoch: 2463: Train loss: 2.9133169651031494 Valid loss: 3.4547128677368164\n",
      "Epoch: 2464: Train loss: 2.9131436347961426 Valid loss: 3.4556150436401367\n",
      "Epoch: 2465: Train loss: 2.912968635559082 Valid loss: 3.456509590148926\n",
      "Epoch: 2466: Train loss: 2.91279673576355 Valid loss: 3.4574155807495117\n",
      "Epoch: 2467: Train loss: 2.91262149810791 Valid loss: 3.458315134048462\n",
      "Epoch: 2468: Train loss: 2.9124491214752197 Valid loss: 3.4592084884643555\n",
      "Epoch: 2469: Train loss: 2.9122772216796875 Valid loss: 3.460113286972046\n",
      "Epoch: 2470: Train loss: 2.9121038913726807 Valid loss: 3.4610116481781006\n",
      "Epoch: 2471: Train loss: 2.911932945251465 Valid loss: 3.4619197845458984\n",
      "Epoch: 2472: Train loss: 2.91176176071167 Valid loss: 3.4628143310546875\n",
      "Epoch: 2473: Train loss: 2.911590337753296 Valid loss: 3.463711738586426\n",
      "Epoch: 2474: Train loss: 2.9114198684692383 Valid loss: 3.464602470397949\n",
      "Epoch: 2475: Train loss: 2.911250114440918 Valid loss: 3.465503215789795\n",
      "Epoch: 2476: Train loss: 2.911081314086914 Valid loss: 3.4663989543914795\n",
      "Epoch: 2477: Train loss: 2.91090989112854 Valid loss: 3.4672958850860596\n",
      "Epoch: 2478: Train loss: 2.9107425212860107 Valid loss: 3.4681954383850098\n",
      "Epoch: 2479: Train loss: 2.910573959350586 Valid loss: 3.469088554382324\n",
      "Epoch: 2480: Train loss: 2.910405158996582 Valid loss: 3.4699857234954834\n",
      "Epoch: 2481: Train loss: 2.9102370738983154 Valid loss: 3.4708824157714844\n",
      "Epoch: 2482: Train loss: 2.9100708961486816 Valid loss: 3.471775770187378\n",
      "Epoch: 2483: Train loss: 2.909902811050415 Valid loss: 3.472668409347534\n",
      "Epoch: 2484: Train loss: 2.9097368717193604 Valid loss: 3.473565101623535\n",
      "Epoch: 2485: Train loss: 2.9095704555511475 Valid loss: 3.4744646549224854\n",
      "Epoch: 2486: Train loss: 2.9094042778015137 Valid loss: 3.475356101989746\n",
      "Epoch: 2487: Train loss: 2.909238576889038 Valid loss: 3.4762425422668457\n",
      "Epoch: 2488: Train loss: 2.909074544906616 Valid loss: 3.4771318435668945\n",
      "Epoch: 2489: Train loss: 2.9089109897613525 Valid loss: 3.4780306816101074\n",
      "Epoch: 2490: Train loss: 2.9087462425231934 Valid loss: 3.4789233207702637\n",
      "Epoch: 2491: Train loss: 2.9085824489593506 Valid loss: 3.479818344116211\n",
      "Epoch: 2492: Train loss: 2.9084157943725586 Valid loss: 3.4807071685791016\n",
      "Epoch: 2493: Train loss: 2.9082536697387695 Valid loss: 3.481598138809204\n",
      "Epoch: 2494: Train loss: 2.908092975616455 Valid loss: 3.482490062713623\n",
      "Epoch: 2495: Train loss: 2.9079291820526123 Valid loss: 3.483377456665039\n",
      "Epoch: 2496: Train loss: 2.907766580581665 Valid loss: 3.484267234802246\n",
      "Epoch: 2497: Train loss: 2.907607316970825 Valid loss: 3.4851579666137695\n",
      "Epoch: 2498: Train loss: 2.907444715499878 Valid loss: 3.486052989959717\n",
      "Epoch: 2499: Train loss: 2.9072842597961426 Valid loss: 3.4869394302368164\n",
      "Epoch: 2500: Train loss: 2.9071238040924072 Valid loss: 3.4878196716308594\n",
      "Epoch: 2501: Train loss: 2.9069628715515137 Valid loss: 3.488704204559326\n",
      "Epoch: 2502: Train loss: 2.9068033695220947 Valid loss: 3.4895968437194824\n",
      "Epoch: 2503: Train loss: 2.906644582748413 Valid loss: 3.4904847145080566\n",
      "Epoch: 2504: Train loss: 2.906484842300415 Valid loss: 3.491365909576416\n",
      "Epoch: 2505: Train loss: 2.9063260555267334 Valid loss: 3.4922497272491455\n",
      "Epoch: 2506: Train loss: 2.90617036819458 Valid loss: 3.493143320083618\n",
      "Epoch: 2507: Train loss: 2.9060118198394775 Valid loss: 3.494030475616455\n",
      "Epoch: 2508: Train loss: 2.9058544635772705 Valid loss: 3.49491286277771\n",
      "Epoch: 2509: Train loss: 2.905697822570801 Valid loss: 3.495795965194702\n",
      "Epoch: 2510: Train loss: 2.9055397510528564 Valid loss: 3.4966816902160645\n",
      "Epoch: 2511: Train loss: 2.9053828716278076 Valid loss: 3.4975695610046387\n",
      "Epoch: 2512: Train loss: 2.9052257537841797 Valid loss: 3.4984512329101562\n",
      "Epoch: 2513: Train loss: 2.9050724506378174 Valid loss: 3.4993338584899902\n",
      "Epoch: 2514: Train loss: 2.904916524887085 Valid loss: 3.500211238861084\n",
      "Epoch: 2515: Train loss: 2.9047622680664062 Valid loss: 3.501098871231079\n",
      "Epoch: 2516: Train loss: 2.9046077728271484 Valid loss: 3.5019798278808594\n",
      "Epoch: 2517: Train loss: 2.904452323913574 Valid loss: 3.5028631687164307\n",
      "Epoch: 2518: Train loss: 2.9042999744415283 Valid loss: 3.503748893737793\n",
      "Epoch: 2519: Train loss: 2.9041459560394287 Valid loss: 3.5046281814575195\n",
      "Epoch: 2520: Train loss: 2.9039924144744873 Valid loss: 3.5055084228515625\n",
      "Epoch: 2521: Train loss: 2.903841495513916 Valid loss: 3.506385564804077\n",
      "Epoch: 2522: Train loss: 2.9036865234375 Valid loss: 3.5072615146636963\n",
      "Epoch: 2523: Train loss: 2.9035353660583496 Valid loss: 3.5081491470336914\n",
      "Epoch: 2524: Train loss: 2.903383255004883 Valid loss: 3.509021282196045\n",
      "Epoch: 2525: Train loss: 2.9032323360443115 Valid loss: 3.509897232055664\n",
      "Epoch: 2526: Train loss: 2.903080940246582 Valid loss: 3.5107815265655518\n",
      "Epoch: 2527: Train loss: 2.902930974960327 Valid loss: 3.5116610527038574\n",
      "Epoch: 2528: Train loss: 2.9027795791625977 Valid loss: 3.5125339031219482\n",
      "Epoch: 2529: Train loss: 2.902630090713501 Valid loss: 3.5134077072143555\n",
      "Epoch: 2530: Train loss: 2.9024815559387207 Valid loss: 3.5142853260040283\n",
      "Epoch: 2531: Train loss: 2.9023306369781494 Valid loss: 3.5151638984680176\n",
      "Epoch: 2532: Train loss: 2.9021806716918945 Valid loss: 3.516035795211792\n",
      "Epoch: 2533: Train loss: 2.902033805847168 Valid loss: 3.5169029235839844\n",
      "Epoch: 2534: Train loss: 2.9018847942352295 Valid loss: 3.517786979675293\n",
      "Epoch: 2535: Train loss: 2.901740074157715 Valid loss: 3.5186574459075928\n",
      "Epoch: 2536: Train loss: 2.901589870452881 Valid loss: 3.5195302963256836\n",
      "Epoch: 2537: Train loss: 2.9014437198638916 Valid loss: 3.5203964710235596\n",
      "Epoch: 2538: Train loss: 2.9012956619262695 Valid loss: 3.521279811859131\n",
      "Epoch: 2539: Train loss: 2.901150941848755 Valid loss: 3.522150754928589\n",
      "Epoch: 2540: Train loss: 2.9010043144226074 Valid loss: 3.5230226516723633\n",
      "Epoch: 2541: Train loss: 2.9008584022521973 Valid loss: 3.5238966941833496\n",
      "Epoch: 2542: Train loss: 2.900712490081787 Valid loss: 3.524773597717285\n",
      "Epoch: 2543: Train loss: 2.900567054748535 Valid loss: 3.525641918182373\n",
      "Epoch: 2544: Train loss: 2.9004228115081787 Valid loss: 3.5265145301818848\n",
      "Epoch: 2545: Train loss: 2.9002771377563477 Valid loss: 3.527378559112549\n",
      "Epoch: 2546: Train loss: 2.900134801864624 Valid loss: 3.528254508972168\n",
      "Epoch: 2547: Train loss: 2.899991035461426 Valid loss: 3.529114246368408\n",
      "Epoch: 2548: Train loss: 2.8998475074768066 Valid loss: 3.5299859046936035\n",
      "Epoch: 2549: Train loss: 2.899705648422241 Valid loss: 3.53085994720459\n",
      "Epoch: 2550: Train loss: 2.8995604515075684 Valid loss: 3.5317273139953613\n",
      "Epoch: 2551: Train loss: 2.899419069290161 Valid loss: 3.532586097717285\n",
      "Epoch: 2552: Train loss: 2.899278163909912 Valid loss: 3.533456563949585\n",
      "Epoch: 2553: Train loss: 2.899135112762451 Valid loss: 3.53432035446167\n",
      "Epoch: 2554: Train loss: 2.8989949226379395 Valid loss: 3.535186529159546\n",
      "Epoch: 2555: Train loss: 2.8988535404205322 Valid loss: 3.536046028137207\n",
      "Epoch: 2556: Train loss: 2.8987114429473877 Valid loss: 3.5369153022766113\n",
      "Epoch: 2557: Train loss: 2.8985700607299805 Valid loss: 3.5377871990203857\n",
      "Epoch: 2558: Train loss: 2.898432731628418 Valid loss: 3.538645029067993\n",
      "Epoch: 2559: Train loss: 2.89829158782959 Valid loss: 3.539510726928711\n",
      "Epoch: 2560: Train loss: 2.898151159286499 Valid loss: 3.540379047393799\n",
      "Epoch: 2561: Train loss: 2.898013114929199 Valid loss: 3.5412349700927734\n",
      "Epoch: 2562: Train loss: 2.897873640060425 Valid loss: 3.5420913696289062\n",
      "Epoch: 2563: Train loss: 2.897735118865967 Valid loss: 3.5429651737213135\n",
      "Epoch: 2564: Train loss: 2.8975956439971924 Valid loss: 3.5438265800476074\n",
      "Epoch: 2565: Train loss: 2.897458791732788 Valid loss: 3.5446887016296387\n",
      "Epoch: 2566: Train loss: 2.8973207473754883 Valid loss: 3.545553207397461\n",
      "Epoch: 2567: Train loss: 2.8971848487854004 Valid loss: 3.5464091300964355\n",
      "Epoch: 2568: Train loss: 2.8970487117767334 Valid loss: 3.547269344329834\n",
      "Epoch: 2569: Train loss: 2.896911144256592 Valid loss: 3.5481302738189697\n",
      "Epoch: 2570: Train loss: 2.8967771530151367 Valid loss: 3.5489861965179443\n",
      "Epoch: 2571: Train loss: 2.896638870239258 Valid loss: 3.5498428344726562\n",
      "Epoch: 2572: Train loss: 2.896503210067749 Valid loss: 3.5507020950317383\n",
      "Epoch: 2573: Train loss: 2.896368980407715 Valid loss: 3.5515613555908203\n",
      "Epoch: 2574: Train loss: 2.896232843399048 Valid loss: 3.5524234771728516\n",
      "Epoch: 2575: Train loss: 2.896099328994751 Valid loss: 3.553271532058716\n",
      "Epoch: 2576: Train loss: 2.895963430404663 Valid loss: 3.5541365146636963\n",
      "Epoch: 2577: Train loss: 2.895831346511841 Valid loss: 3.5549874305725098\n",
      "Epoch: 2578: Train loss: 2.8956971168518066 Valid loss: 3.5558409690856934\n",
      "Epoch: 2579: Train loss: 2.895564556121826 Valid loss: 3.556687831878662\n",
      "Epoch: 2580: Train loss: 2.8954315185546875 Valid loss: 3.557543992996216\n",
      "Epoch: 2581: Train loss: 2.8952982425689697 Valid loss: 3.5584027767181396\n",
      "Epoch: 2582: Train loss: 2.895165205001831 Valid loss: 3.5592548847198486\n",
      "Epoch: 2583: Train loss: 2.895033121109009 Valid loss: 3.5601091384887695\n",
      "Epoch: 2584: Train loss: 2.89490008354187 Valid loss: 3.560964345932007\n",
      "Epoch: 2585: Train loss: 2.8947701454162598 Valid loss: 3.561814308166504\n",
      "Epoch: 2586: Train loss: 2.8946385383605957 Valid loss: 3.562674045562744\n",
      "Epoch: 2587: Train loss: 2.8945069313049316 Valid loss: 3.5635251998901367\n",
      "Epoch: 2588: Train loss: 2.894376516342163 Valid loss: 3.56437349319458\n",
      "Epoch: 2589: Train loss: 2.894246816635132 Valid loss: 3.565227746963501\n",
      "Epoch: 2590: Train loss: 2.894117593765259 Valid loss: 3.5660789012908936\n",
      "Epoch: 2591: Train loss: 2.893986940383911 Valid loss: 3.5669214725494385\n",
      "Epoch: 2592: Train loss: 2.893857002258301 Valid loss: 3.5677683353424072\n",
      "Epoch: 2593: Train loss: 2.8937292098999023 Valid loss: 3.5686209201812744\n",
      "Epoch: 2594: Train loss: 2.89359974861145 Valid loss: 3.5694706439971924\n",
      "Epoch: 2595: Train loss: 2.8934707641601562 Valid loss: 3.5703136920928955\n",
      "Epoch: 2596: Train loss: 2.893343210220337 Valid loss: 3.5711662769317627\n",
      "Epoch: 2597: Train loss: 2.893214225769043 Valid loss: 3.572004795074463\n",
      "Epoch: 2598: Train loss: 2.8930869102478027 Valid loss: 3.5728530883789062\n",
      "Epoch: 2599: Train loss: 2.8929615020751953 Valid loss: 3.573699951171875\n",
      "Epoch: 2600: Train loss: 2.8928329944610596 Valid loss: 3.5745439529418945\n",
      "Epoch: 2601: Train loss: 2.892706871032715 Valid loss: 3.575388193130493\n",
      "Epoch: 2602: Train loss: 2.8925812244415283 Valid loss: 3.5762438774108887\n",
      "Epoch: 2603: Train loss: 2.89245343208313 Valid loss: 3.5770840644836426\n",
      "Epoch: 2604: Train loss: 2.8923277854919434 Valid loss: 3.5779170989990234\n",
      "Epoch: 2605: Train loss: 2.8922033309936523 Valid loss: 3.5787672996520996\n",
      "Epoch: 2606: Train loss: 2.8920774459838867 Valid loss: 3.5796124935150146\n",
      "Epoch: 2607: Train loss: 2.8919522762298584 Valid loss: 3.580451011657715\n",
      "Epoch: 2608: Train loss: 2.891828775405884 Valid loss: 3.581291675567627\n",
      "Epoch: 2609: Train loss: 2.891704559326172 Valid loss: 3.5821332931518555\n",
      "Epoch: 2610: Train loss: 2.8915791511535645 Valid loss: 3.5829696655273438\n",
      "Epoch: 2611: Train loss: 2.891456127166748 Valid loss: 3.5838136672973633\n",
      "Epoch: 2612: Train loss: 2.891331672668457 Valid loss: 3.584650993347168\n",
      "Epoch: 2613: Train loss: 2.891206979751587 Valid loss: 3.5854926109313965\n",
      "Epoch: 2614: Train loss: 2.891085624694824 Valid loss: 3.586336612701416\n",
      "Epoch: 2615: Train loss: 2.890962600708008 Valid loss: 3.587170124053955\n",
      "Epoch: 2616: Train loss: 2.890841007232666 Valid loss: 3.5880074501037598\n",
      "Epoch: 2617: Train loss: 2.890718698501587 Valid loss: 3.5888476371765137\n",
      "Epoch: 2618: Train loss: 2.890597105026245 Valid loss: 3.589679002761841\n",
      "Epoch: 2619: Train loss: 2.8904757499694824 Valid loss: 3.590521812438965\n",
      "Epoch: 2620: Train loss: 2.890352249145508 Valid loss: 3.591357707977295\n",
      "Epoch: 2621: Train loss: 2.890233039855957 Valid loss: 3.59218692779541\n",
      "Epoch: 2622: Train loss: 2.8901119232177734 Valid loss: 3.5930185317993164\n",
      "Epoch: 2623: Train loss: 2.8899922370910645 Valid loss: 3.5938596725463867\n",
      "Epoch: 2624: Train loss: 2.889871597290039 Valid loss: 3.594694137573242\n",
      "Epoch: 2625: Train loss: 2.889752149581909 Valid loss: 3.595521926879883\n",
      "Epoch: 2626: Train loss: 2.889631748199463 Valid loss: 3.5963587760925293\n",
      "Epoch: 2627: Train loss: 2.889512062072754 Valid loss: 3.597198247909546\n",
      "Epoch: 2628: Train loss: 2.889394521713257 Valid loss: 3.598021984100342\n",
      "Epoch: 2629: Train loss: 2.889275312423706 Valid loss: 3.5988569259643555\n",
      "Epoch: 2630: Train loss: 2.889157772064209 Valid loss: 3.5996832847595215\n",
      "Epoch: 2631: Train loss: 2.8890392780303955 Valid loss: 3.6005139350891113\n",
      "Epoch: 2632: Train loss: 2.888922691345215 Valid loss: 3.6013448238372803\n",
      "Epoch: 2633: Train loss: 2.888803720474243 Valid loss: 3.6021761894226074\n",
      "Epoch: 2634: Train loss: 2.8886866569519043 Valid loss: 3.6030044555664062\n",
      "Epoch: 2635: Train loss: 2.888570785522461 Valid loss: 3.6038317680358887\n",
      "Epoch: 2636: Train loss: 2.8884520530700684 Valid loss: 3.6046626567840576\n",
      "Epoch: 2637: Train loss: 2.888336658477783 Valid loss: 3.605484962463379\n",
      "Epoch: 2638: Train loss: 2.8882219791412354 Valid loss: 3.606318950653076\n",
      "Epoch: 2639: Train loss: 2.8881025314331055 Valid loss: 3.6071457862854004\n",
      "Epoch: 2640: Train loss: 2.887988805770874 Valid loss: 3.607964515686035\n",
      "Epoch: 2641: Train loss: 2.8878724575042725 Valid loss: 3.6087942123413086\n",
      "Epoch: 2642: Train loss: 2.887756824493408 Valid loss: 3.609617233276367\n",
      "Epoch: 2643: Train loss: 2.887643337249756 Valid loss: 3.6104495525360107\n",
      "Epoch: 2644: Train loss: 2.8875298500061035 Valid loss: 3.6112661361694336\n",
      "Epoch: 2645: Train loss: 2.88741397857666 Valid loss: 3.6120944023132324\n",
      "Epoch: 2646: Train loss: 2.8872976303100586 Valid loss: 3.6129226684570312\n",
      "Epoch: 2647: Train loss: 2.8871865272521973 Valid loss: 3.613737106323242\n",
      "Epoch: 2648: Train loss: 2.8870739936828613 Valid loss: 3.614553689956665\n",
      "Epoch: 2649: Train loss: 2.886958122253418 Valid loss: 3.615386962890625\n",
      "Epoch: 2650: Train loss: 2.886845827102661 Valid loss: 3.616206645965576\n",
      "Epoch: 2651: Train loss: 2.886733055114746 Valid loss: 3.6170191764831543\n",
      "Epoch: 2652: Train loss: 2.886622190475464 Valid loss: 3.6178412437438965\n",
      "Epoch: 2653: Train loss: 2.8865108489990234 Valid loss: 3.6186583042144775\n",
      "Epoch: 2654: Train loss: 2.8863961696624756 Valid loss: 3.619475841522217\n",
      "Epoch: 2655: Train loss: 2.886284828186035 Valid loss: 3.620302677154541\n",
      "Epoch: 2656: Train loss: 2.8861749172210693 Valid loss: 3.621124744415283\n",
      "Epoch: 2657: Train loss: 2.8860623836517334 Valid loss: 3.6219382286071777\n",
      "Epoch: 2658: Train loss: 2.885951042175293 Valid loss: 3.6227610111236572\n",
      "Epoch: 2659: Train loss: 2.8858416080474854 Valid loss: 3.6235718727111816\n",
      "Epoch: 2660: Train loss: 2.8857314586639404 Valid loss: 3.6243882179260254\n",
      "Epoch: 2661: Train loss: 2.8856213092803955 Valid loss: 3.625201463699341\n",
      "Epoch: 2662: Train loss: 2.8855090141296387 Valid loss: 3.6260132789611816\n",
      "Epoch: 2663: Train loss: 2.885401964187622 Valid loss: 3.6268293857574463\n",
      "Epoch: 2664: Train loss: 2.885291337966919 Valid loss: 3.62764573097229\n",
      "Epoch: 2665: Train loss: 2.8851828575134277 Valid loss: 3.628455400466919\n",
      "Epoch: 2666: Train loss: 2.8850746154785156 Valid loss: 3.6292762756347656\n",
      "Epoch: 2667: Train loss: 2.884965181350708 Valid loss: 3.6300811767578125\n",
      "Epoch: 2668: Train loss: 2.8848562240600586 Valid loss: 3.6309027671813965\n",
      "Epoch: 2669: Train loss: 2.884748697280884 Valid loss: 3.6317105293273926\n",
      "Epoch: 2670: Train loss: 2.8846418857574463 Valid loss: 3.6325204372406006\n",
      "Epoch: 2671: Train loss: 2.884533405303955 Valid loss: 3.633330821990967\n",
      "Epoch: 2672: Train loss: 2.8844263553619385 Valid loss: 3.634143352508545\n",
      "Epoch: 2673: Train loss: 2.8843183517456055 Valid loss: 3.634948968887329\n",
      "Epoch: 2674: Train loss: 2.884211778640747 Valid loss: 3.6357641220092773\n",
      "Epoch: 2675: Train loss: 2.884106397628784 Valid loss: 3.6365652084350586\n",
      "Epoch: 2676: Train loss: 2.8839986324310303 Valid loss: 3.6373846530914307\n",
      "Epoch: 2677: Train loss: 2.8838913440704346 Valid loss: 3.638188362121582\n",
      "Epoch: 2678: Train loss: 2.8837857246398926 Valid loss: 3.6389923095703125\n",
      "Epoch: 2679: Train loss: 2.8836817741394043 Valid loss: 3.639798879623413\n",
      "Epoch: 2680: Train loss: 2.883578062057495 Valid loss: 3.6406054496765137\n",
      "Epoch: 2681: Train loss: 2.883471965789795 Valid loss: 3.641408920288086\n",
      "Epoch: 2682: Train loss: 2.8833658695220947 Valid loss: 3.6422109603881836\n",
      "Epoch: 2683: Train loss: 2.8832616806030273 Valid loss: 3.6430153846740723\n",
      "Epoch: 2684: Train loss: 2.8831567764282227 Valid loss: 3.6438217163085938\n",
      "Epoch: 2685: Train loss: 2.8830533027648926 Valid loss: 3.6446285247802734\n",
      "Epoch: 2686: Train loss: 2.8829505443573 Valid loss: 3.645430564880371\n",
      "Epoch: 2687: Train loss: 2.8828461170196533 Valid loss: 3.646230697631836\n",
      "Epoch: 2688: Train loss: 2.8827426433563232 Valid loss: 3.6470351219177246\n",
      "Epoch: 2689: Train loss: 2.8826394081115723 Valid loss: 3.6478400230407715\n",
      "Epoch: 2690: Train loss: 2.8825368881225586 Valid loss: 3.6486382484436035\n",
      "Epoch: 2691: Train loss: 2.882434129714966 Valid loss: 3.6494383811950684\n",
      "Epoch: 2692: Train loss: 2.882331132888794 Valid loss: 3.650238513946533\n",
      "Epoch: 2693: Train loss: 2.882229804992676 Valid loss: 3.651034355163574\n",
      "Epoch: 2694: Train loss: 2.8821280002593994 Valid loss: 3.6518301963806152\n",
      "Epoch: 2695: Train loss: 2.8820247650146484 Valid loss: 3.652635335922241\n",
      "Epoch: 2696: Train loss: 2.881924867630005 Valid loss: 3.6534266471862793\n",
      "Epoch: 2697: Train loss: 2.8818230628967285 Valid loss: 3.6542272567749023\n",
      "Epoch: 2698: Train loss: 2.8817214965820312 Valid loss: 3.6550281047821045\n",
      "Epoch: 2699: Train loss: 2.881621837615967 Valid loss: 3.6558239459991455\n",
      "Epoch: 2700: Train loss: 2.8815200328826904 Valid loss: 3.656620502471924\n",
      "Epoch: 2701: Train loss: 2.881420612335205 Valid loss: 3.65740966796875\n",
      "Epoch: 2702: Train loss: 2.881319522857666 Valid loss: 3.6582086086273193\n",
      "Epoch: 2703: Train loss: 2.881220817565918 Valid loss: 3.6590023040771484\n",
      "Epoch: 2704: Train loss: 2.881120443344116 Valid loss: 3.659789562225342\n",
      "Epoch: 2705: Train loss: 2.88102126121521 Valid loss: 3.660583972930908\n",
      "Epoch: 2706: Train loss: 2.880922794342041 Valid loss: 3.6613805294036865\n",
      "Epoch: 2707: Train loss: 2.8808236122131348 Valid loss: 3.662179470062256\n",
      "Epoch: 2708: Train loss: 2.880725860595703 Valid loss: 3.6629624366760254\n",
      "Epoch: 2709: Train loss: 2.8806264400482178 Valid loss: 3.6637566089630127\n",
      "Epoch: 2710: Train loss: 2.880528450012207 Valid loss: 3.6645421981811523\n",
      "Epoch: 2711: Train loss: 2.8804309368133545 Valid loss: 3.665329694747925\n",
      "Epoch: 2712: Train loss: 2.8803303241729736 Valid loss: 3.6661267280578613\n",
      "Epoch: 2713: Train loss: 2.880234479904175 Valid loss: 3.666907787322998\n",
      "Epoch: 2714: Train loss: 2.8801374435424805 Valid loss: 3.6676928997039795\n",
      "Epoch: 2715: Train loss: 2.8800408840179443 Valid loss: 3.668487548828125\n",
      "Epoch: 2716: Train loss: 2.8799445629119873 Valid loss: 3.6692731380462646\n",
      "Epoch: 2717: Train loss: 2.879847288131714 Valid loss: 3.6700682640075684\n",
      "Epoch: 2718: Train loss: 2.8797502517700195 Valid loss: 3.670849323272705\n",
      "Epoch: 2719: Train loss: 2.879655122756958 Valid loss: 3.6716253757476807\n",
      "Epoch: 2720: Train loss: 2.8795578479766846 Valid loss: 3.6724162101745605\n",
      "Epoch: 2721: Train loss: 2.879462957382202 Valid loss: 3.673201560974121\n",
      "Epoch: 2722: Train loss: 2.879366636276245 Valid loss: 3.673987865447998\n",
      "Epoch: 2723: Train loss: 2.8792734146118164 Valid loss: 3.6747689247131348\n",
      "Epoch: 2724: Train loss: 2.879176139831543 Valid loss: 3.6755499839782715\n",
      "Epoch: 2725: Train loss: 2.8790810108184814 Valid loss: 3.6763405799865723\n",
      "Epoch: 2726: Train loss: 2.8789865970611572 Valid loss: 3.677117347717285\n",
      "Epoch: 2727: Train loss: 2.8788928985595703 Valid loss: 3.677903175354004\n",
      "Epoch: 2728: Train loss: 2.8787992000579834 Valid loss: 3.678680419921875\n",
      "Epoch: 2729: Train loss: 2.8787038326263428 Valid loss: 3.6794614791870117\n",
      "Epoch: 2730: Train loss: 2.878610849380493 Valid loss: 3.6802430152893066\n",
      "Epoch: 2731: Train loss: 2.8785176277160645 Valid loss: 3.6810176372528076\n",
      "Epoch: 2732: Train loss: 2.878424644470215 Valid loss: 3.6818087100982666\n",
      "Epoch: 2733: Train loss: 2.8783318996429443 Valid loss: 3.6825785636901855\n",
      "Epoch: 2734: Train loss: 2.87823748588562 Valid loss: 3.6833558082580566\n",
      "Epoch: 2735: Train loss: 2.878145694732666 Valid loss: 3.6841354370117188\n",
      "Epoch: 2736: Train loss: 2.878053903579712 Valid loss: 3.6849098205566406\n",
      "Epoch: 2737: Train loss: 2.8779590129852295 Valid loss: 3.6856844425201416\n",
      "Epoch: 2738: Train loss: 2.8778700828552246 Valid loss: 3.6864614486694336\n",
      "Epoch: 2739: Train loss: 2.877777576446533 Valid loss: 3.687236785888672\n",
      "Epoch: 2740: Train loss: 2.8776845932006836 Valid loss: 3.6880087852478027\n",
      "Epoch: 2741: Train loss: 2.877594470977783 Valid loss: 3.688786506652832\n",
      "Epoch: 2742: Train loss: 2.877504348754883 Valid loss: 3.689561367034912\n",
      "Epoch: 2743: Train loss: 2.8774120807647705 Valid loss: 3.690335750579834\n",
      "Epoch: 2744: Train loss: 2.877321481704712 Valid loss: 3.6911020278930664\n",
      "Epoch: 2745: Train loss: 2.8772313594818115 Valid loss: 3.6918792724609375\n",
      "Epoch: 2746: Train loss: 2.8771414756774902 Valid loss: 3.6926496028900146\n",
      "Epoch: 2747: Train loss: 2.877052068710327 Valid loss: 3.6934220790863037\n",
      "Epoch: 2748: Train loss: 2.8769595623016357 Valid loss: 3.694194793701172\n",
      "Epoch: 2749: Train loss: 2.8768718242645264 Valid loss: 3.694967746734619\n",
      "Epoch: 2750: Train loss: 2.876781940460205 Valid loss: 3.6957337856292725\n",
      "Epoch: 2751: Train loss: 2.8766937255859375 Valid loss: 3.696502208709717\n",
      "Epoch: 2752: Train loss: 2.8766043186187744 Valid loss: 3.697265148162842\n",
      "Epoch: 2753: Train loss: 2.876513957977295 Valid loss: 3.69803786277771\n",
      "Epoch: 2754: Train loss: 2.876427412033081 Valid loss: 3.6988015174865723\n",
      "Epoch: 2755: Train loss: 2.876338005065918 Valid loss: 3.6995744705200195\n",
      "Epoch: 2756: Train loss: 2.8762500286102295 Valid loss: 3.700340747833252\n",
      "Epoch: 2757: Train loss: 2.8761610984802246 Valid loss: 3.701101779937744\n",
      "Epoch: 2758: Train loss: 2.8760740756988525 Valid loss: 3.7018702030181885\n",
      "Epoch: 2759: Train loss: 2.8759870529174805 Valid loss: 3.702624797821045\n",
      "Epoch: 2760: Train loss: 2.8759005069732666 Valid loss: 3.703397274017334\n",
      "Epoch: 2761: Train loss: 2.875810146331787 Valid loss: 3.7041540145874023\n",
      "Epoch: 2762: Train loss: 2.8757259845733643 Valid loss: 3.7049293518066406\n",
      "Epoch: 2763: Train loss: 2.8756375312805176 Valid loss: 3.7056884765625\n",
      "Epoch: 2764: Train loss: 2.875551462173462 Valid loss: 3.7064404487609863\n",
      "Epoch: 2765: Train loss: 2.875464916229248 Valid loss: 3.7072041034698486\n",
      "Epoch: 2766: Train loss: 2.8753795623779297 Valid loss: 3.707965850830078\n",
      "Epoch: 2767: Train loss: 2.8752927780151367 Valid loss: 3.7087316513061523\n",
      "Epoch: 2768: Train loss: 2.8752081394195557 Valid loss: 3.7094883918762207\n",
      "Epoch: 2769: Train loss: 2.8751211166381836 Valid loss: 3.71024751663208\n",
      "Epoch: 2770: Train loss: 2.8750360012054443 Valid loss: 3.711001396179199\n",
      "Epoch: 2771: Train loss: 2.8749513626098633 Valid loss: 3.711760997772217\n",
      "Epoch: 2772: Train loss: 2.874865770339966 Valid loss: 3.7125244140625\n",
      "Epoch: 2773: Train loss: 2.874781608581543 Valid loss: 3.7132811546325684\n",
      "Epoch: 2774: Train loss: 2.8746981620788574 Valid loss: 3.7140307426452637\n",
      "Epoch: 2775: Train loss: 2.874612808227539 Valid loss: 3.7147879600524902\n",
      "Epoch: 2776: Train loss: 2.8745296001434326 Valid loss: 3.7155470848083496\n",
      "Epoch: 2777: Train loss: 2.8744447231292725 Valid loss: 3.7163009643554688\n",
      "Epoch: 2778: Train loss: 2.8743603229522705 Valid loss: 3.717055559158325\n",
      "Epoch: 2779: Train loss: 2.8742761611938477 Valid loss: 3.7178101539611816\n",
      "Epoch: 2780: Train loss: 2.874195098876953 Valid loss: 3.71856689453125\n",
      "Epoch: 2781: Train loss: 2.874110460281372 Valid loss: 3.7193164825439453\n",
      "Epoch: 2782: Train loss: 2.874027967453003 Valid loss: 3.7200663089752197\n",
      "Epoch: 2783: Train loss: 2.873945951461792 Valid loss: 3.720818519592285\n",
      "Epoch: 2784: Train loss: 2.8738625049591064 Valid loss: 3.7215633392333984\n",
      "Epoch: 2785: Train loss: 2.8737802505493164 Valid loss: 3.7223198413848877\n",
      "Epoch: 2786: Train loss: 2.8736953735351562 Valid loss: 3.723067283630371\n",
      "Epoch: 2787: Train loss: 2.873615026473999 Valid loss: 3.7238149642944336\n",
      "Epoch: 2788: Train loss: 2.8735342025756836 Valid loss: 3.724564552307129\n",
      "Epoch: 2789: Train loss: 2.873452663421631 Valid loss: 3.7253165245056152\n",
      "Epoch: 2790: Train loss: 2.8733716011047363 Valid loss: 3.7260613441467285\n",
      "Epoch: 2791: Train loss: 2.873288631439209 Valid loss: 3.726815700531006\n",
      "Epoch: 2792: Train loss: 2.8732082843780518 Valid loss: 3.7275607585906982\n",
      "Epoch: 2793: Train loss: 2.8731260299682617 Valid loss: 3.7283010482788086\n",
      "Epoch: 2794: Train loss: 2.8730475902557373 Valid loss: 3.729050397872925\n",
      "Epoch: 2795: Train loss: 2.8729662895202637 Valid loss: 3.7297911643981934\n",
      "Epoch: 2796: Train loss: 2.8728866577148438 Valid loss: 3.7305357456207275\n",
      "Epoch: 2797: Train loss: 2.8728067874908447 Valid loss: 3.7312824726104736\n",
      "Epoch: 2798: Train loss: 2.8727259635925293 Valid loss: 3.732027530670166\n",
      "Epoch: 2799: Train loss: 2.872645378112793 Valid loss: 3.7327747344970703\n",
      "Epoch: 2800: Train loss: 2.8725666999816895 Valid loss: 3.7335147857666016\n",
      "Epoch: 2801: Train loss: 2.8724868297576904 Valid loss: 3.734255313873291\n",
      "Epoch: 2802: Train loss: 2.872408628463745 Valid loss: 3.7349977493286133\n",
      "Epoch: 2803: Train loss: 2.8723278045654297 Valid loss: 3.7357335090637207\n",
      "Epoch: 2804: Train loss: 2.872250556945801 Valid loss: 3.736478090286255\n",
      "Epoch: 2805: Train loss: 2.8721723556518555 Valid loss: 3.7372140884399414\n",
      "Epoch: 2806: Train loss: 2.872093439102173 Valid loss: 3.7379539012908936\n",
      "Epoch: 2807: Train loss: 2.872014045715332 Valid loss: 3.7386937141418457\n",
      "Epoch: 2808: Train loss: 2.871936798095703 Valid loss: 3.739427089691162\n",
      "Epoch: 2809: Train loss: 2.8718581199645996 Valid loss: 3.740169048309326\n",
      "Epoch: 2810: Train loss: 2.8717808723449707 Valid loss: 3.7408971786499023\n",
      "Epoch: 2811: Train loss: 2.8717033863067627 Valid loss: 3.7416419982910156\n",
      "Epoch: 2812: Train loss: 2.8716254234313965 Valid loss: 3.7423744201660156\n",
      "Epoch: 2813: Train loss: 2.8715474605560303 Valid loss: 3.743112087249756\n",
      "Epoch: 2814: Train loss: 2.871471881866455 Valid loss: 3.743844985961914\n",
      "Epoch: 2815: Train loss: 2.871396064758301 Valid loss: 3.7445850372314453\n",
      "Epoch: 2816: Train loss: 2.8713178634643555 Valid loss: 3.7453107833862305\n",
      "Epoch: 2817: Train loss: 2.871241331100464 Valid loss: 3.7460482120513916\n",
      "Epoch: 2818: Train loss: 2.8711650371551514 Valid loss: 3.746783494949341\n",
      "Epoch: 2819: Train loss: 2.871088743209839 Valid loss: 3.747520923614502\n",
      "Epoch: 2820: Train loss: 2.8710129261016846 Valid loss: 3.748253345489502\n",
      "Epoch: 2821: Train loss: 2.870938301086426 Valid loss: 3.748983860015869\n",
      "Epoch: 2822: Train loss: 2.870861291885376 Valid loss: 3.7497076988220215\n",
      "Epoch: 2823: Train loss: 2.8707873821258545 Valid loss: 3.7504348754882812\n",
      "Epoch: 2824: Train loss: 2.870710611343384 Valid loss: 3.7511699199676514\n",
      "Epoch: 2825: Train loss: 2.8706376552581787 Valid loss: 3.7518997192382812\n",
      "Epoch: 2826: Train loss: 2.870560646057129 Valid loss: 3.7526209354400635\n",
      "Epoch: 2827: Train loss: 2.870487928390503 Valid loss: 3.7533438205718994\n",
      "Epoch: 2828: Train loss: 2.8704121112823486 Valid loss: 3.7540760040283203\n",
      "Epoch: 2829: Train loss: 2.8703393936157227 Valid loss: 3.7548012733459473\n",
      "Epoch: 2830: Train loss: 2.8702642917633057 Valid loss: 3.7555267810821533\n",
      "Epoch: 2831: Train loss: 2.8701906204223633 Valid loss: 3.756254196166992\n",
      "Epoch: 2832: Train loss: 2.870114326477051 Valid loss: 3.756983757019043\n",
      "Epoch: 2833: Train loss: 2.870042324066162 Valid loss: 3.757704257965088\n",
      "Epoch: 2834: Train loss: 2.8699681758880615 Valid loss: 3.758434295654297\n",
      "Epoch: 2835: Train loss: 2.869896650314331 Valid loss: 3.7591500282287598\n",
      "Epoch: 2836: Train loss: 2.8698229789733887 Valid loss: 3.7598750591278076\n",
      "Epoch: 2837: Train loss: 2.8697497844696045 Valid loss: 3.7606000900268555\n",
      "Epoch: 2838: Train loss: 2.869676113128662 Valid loss: 3.7613272666931152\n",
      "Epoch: 2839: Train loss: 2.8696036338806152 Valid loss: 3.762047290802002\n",
      "Epoch: 2840: Train loss: 2.8695313930511475 Valid loss: 3.762768030166626\n",
      "Epoch: 2841: Train loss: 2.869459390640259 Valid loss: 3.763481378555298\n",
      "Epoch: 2842: Train loss: 2.8693859577178955 Valid loss: 3.7642040252685547\n",
      "Epoch: 2843: Train loss: 2.869314432144165 Valid loss: 3.7649192810058594\n",
      "Epoch: 2844: Train loss: 2.8692426681518555 Valid loss: 3.7656443119049072\n",
      "Epoch: 2845: Train loss: 2.8691704273223877 Valid loss: 3.7663638591766357\n",
      "Epoch: 2846: Train loss: 2.869098663330078 Valid loss: 3.7670750617980957\n",
      "Epoch: 2847: Train loss: 2.8690285682678223 Valid loss: 3.767794609069824\n",
      "Epoch: 2848: Train loss: 2.868955612182617 Valid loss: 3.768515110015869\n",
      "Epoch: 2849: Train loss: 2.8688859939575195 Valid loss: 3.7692227363586426\n",
      "Epoch: 2850: Train loss: 2.8688158988952637 Valid loss: 3.7699451446533203\n",
      "Epoch: 2851: Train loss: 2.8687431812286377 Valid loss: 3.770653486251831\n",
      "Epoch: 2852: Train loss: 2.8686749935150146 Valid loss: 3.7713799476623535\n",
      "Epoch: 2853: Train loss: 2.8686041831970215 Valid loss: 3.772090435028076\n",
      "Epoch: 2854: Train loss: 2.8685340881347656 Valid loss: 3.7728099822998047\n",
      "Epoch: 2855: Train loss: 2.868464708328247 Valid loss: 3.7735133171081543\n",
      "Epoch: 2856: Train loss: 2.8683929443359375 Valid loss: 3.774228096008301\n",
      "Epoch: 2857: Train loss: 2.8683245182037354 Valid loss: 3.7749338150024414\n",
      "Epoch: 2858: Train loss: 2.8682548999786377 Valid loss: 3.7756507396698\n",
      "Epoch: 2859: Train loss: 2.8681840896606445 Valid loss: 3.776360511779785\n",
      "Epoch: 2860: Train loss: 2.868116617202759 Valid loss: 3.7770683765411377\n",
      "Epoch: 2861: Train loss: 2.868046283721924 Valid loss: 3.777780055999756\n",
      "Epoch: 2862: Train loss: 2.867978572845459 Valid loss: 3.7784852981567383\n",
      "Epoch: 2863: Train loss: 2.867908477783203 Valid loss: 3.779188632965088\n",
      "Epoch: 2864: Train loss: 2.867839813232422 Valid loss: 3.779902935028076\n",
      "Epoch: 2865: Train loss: 2.867772340774536 Valid loss: 3.7806100845336914\n",
      "Epoch: 2866: Train loss: 2.8677027225494385 Valid loss: 3.7813262939453125\n",
      "Epoch: 2867: Train loss: 2.8676352500915527 Valid loss: 3.78201961517334\n",
      "Epoch: 2868: Train loss: 2.8675689697265625 Valid loss: 3.7827296257019043\n",
      "Epoch: 2869: Train loss: 2.8675007820129395 Valid loss: 3.7834410667419434\n",
      "Epoch: 2870: Train loss: 2.867433786392212 Valid loss: 3.7841455936431885\n",
      "Epoch: 2871: Train loss: 2.8673651218414307 Valid loss: 3.7848501205444336\n",
      "Epoch: 2872: Train loss: 2.867298126220703 Valid loss: 3.785548210144043\n",
      "Epoch: 2873: Train loss: 2.8672304153442383 Valid loss: 3.786247730255127\n",
      "Epoch: 2874: Train loss: 2.867164134979248 Valid loss: 3.786954879760742\n",
      "Epoch: 2875: Train loss: 2.8670971393585205 Valid loss: 3.787656784057617\n",
      "Epoch: 2876: Train loss: 2.867030143737793 Valid loss: 3.788358688354492\n",
      "Epoch: 2877: Train loss: 2.86696457862854 Valid loss: 3.7890539169311523\n",
      "Epoch: 2878: Train loss: 2.8668980598449707 Valid loss: 3.789760112762451\n",
      "Epoch: 2879: Train loss: 2.866831064224243 Valid loss: 3.790463924407959\n",
      "Epoch: 2880: Train loss: 2.8667666912078857 Valid loss: 3.791161298751831\n",
      "Epoch: 2881: Train loss: 2.8667008876800537 Valid loss: 3.791853427886963\n",
      "Epoch: 2882: Train loss: 2.8666341304779053 Valid loss: 3.792552947998047\n",
      "Epoch: 2883: Train loss: 2.8665685653686523 Valid loss: 3.7932381629943848\n",
      "Epoch: 2884: Train loss: 2.8665032386779785 Valid loss: 3.7939417362213135\n",
      "Epoch: 2885: Train loss: 2.8664379119873047 Valid loss: 3.7946360111236572\n",
      "Epoch: 2886: Train loss: 2.8663737773895264 Valid loss: 3.7953414916992188\n",
      "Epoch: 2887: Train loss: 2.866307258605957 Valid loss: 3.7960309982299805\n",
      "Epoch: 2888: Train loss: 2.8662424087524414 Valid loss: 3.796729564666748\n",
      "Epoch: 2889: Train loss: 2.8661789894104004 Valid loss: 3.797419309616089\n",
      "Epoch: 2890: Train loss: 2.866112232208252 Valid loss: 3.7981133460998535\n",
      "Epoch: 2891: Train loss: 2.8660500049591064 Valid loss: 3.798806667327881\n",
      "Epoch: 2892: Train loss: 2.865985155105591 Valid loss: 3.7995007038116455\n",
      "Epoch: 2893: Train loss: 2.865922212600708 Valid loss: 3.8001890182495117\n",
      "Epoch: 2894: Train loss: 2.8658576011657715 Valid loss: 3.8008852005004883\n",
      "Epoch: 2895: Train loss: 2.865793466567993 Valid loss: 3.8015761375427246\n",
      "Epoch: 2896: Train loss: 2.865729331970215 Valid loss: 3.802265167236328\n",
      "Epoch: 2897: Train loss: 2.8656654357910156 Valid loss: 3.8029489517211914\n",
      "Epoch: 2898: Train loss: 2.86560320854187 Valid loss: 3.8036417961120605\n",
      "Epoch: 2899: Train loss: 2.865539073944092 Valid loss: 3.804327964782715\n",
      "Epoch: 2900: Train loss: 2.8654770851135254 Valid loss: 3.805014133453369\n",
      "Epoch: 2901: Train loss: 2.865415096282959 Valid loss: 3.805701971054077\n",
      "Epoch: 2902: Train loss: 2.8653504848480225 Valid loss: 3.8063900470733643\n",
      "Epoch: 2903: Train loss: 2.865290403366089 Valid loss: 3.8070712089538574\n",
      "Epoch: 2904: Train loss: 2.865226984024048 Valid loss: 3.8077616691589355\n",
      "Epoch: 2905: Train loss: 2.8651628494262695 Valid loss: 3.8084521293640137\n",
      "Epoch: 2906: Train loss: 2.865100860595703 Valid loss: 3.8091282844543457\n",
      "Epoch: 2907: Train loss: 2.8650410175323486 Valid loss: 3.809821128845215\n",
      "Epoch: 2908: Train loss: 2.864978313446045 Valid loss: 3.8105063438415527\n",
      "Epoch: 2909: Train loss: 2.8649168014526367 Valid loss: 3.811187267303467\n",
      "Epoch: 2910: Train loss: 2.8648550510406494 Valid loss: 3.8118653297424316\n",
      "Epoch: 2911: Train loss: 2.8647942543029785 Valid loss: 3.8125462532043457\n",
      "Epoch: 2912: Train loss: 2.8647336959838867 Valid loss: 3.8132357597351074\n",
      "Epoch: 2913: Train loss: 2.8646726608276367 Valid loss: 3.8139114379882812\n",
      "Epoch: 2914: Train loss: 2.8646111488342285 Valid loss: 3.814587116241455\n",
      "Epoch: 2915: Train loss: 2.864549160003662 Valid loss: 3.815269947052002\n",
      "Epoch: 2916: Train loss: 2.8644871711730957 Valid loss: 3.8159494400024414\n",
      "Epoch: 2917: Train loss: 2.8644285202026367 Valid loss: 3.8166275024414062\n",
      "Epoch: 2918: Train loss: 2.864367723464966 Valid loss: 3.8172998428344727\n",
      "Epoch: 2919: Train loss: 2.864307165145874 Valid loss: 3.8179798126220703\n",
      "Epoch: 2920: Train loss: 2.8642477989196777 Valid loss: 3.8186545372009277\n",
      "Epoch: 2921: Train loss: 2.8641881942749023 Valid loss: 3.819336175918579\n",
      "Epoch: 2922: Train loss: 2.8641257286071777 Valid loss: 3.8200130462646484\n",
      "Epoch: 2923: Train loss: 2.864067316055298 Valid loss: 3.8206899166107178\n",
      "Epoch: 2924: Train loss: 2.864008903503418 Valid loss: 3.8213648796081543\n",
      "Epoch: 2925: Train loss: 2.8639473915100098 Valid loss: 3.8220348358154297\n",
      "Epoch: 2926: Train loss: 2.863889217376709 Valid loss: 3.8227052688598633\n",
      "Epoch: 2927: Train loss: 2.8638315200805664 Valid loss: 3.8233771324157715\n",
      "Epoch: 2928: Train loss: 2.863772392272949 Valid loss: 3.8240580558776855\n",
      "Epoch: 2929: Train loss: 2.863711357116699 Valid loss: 3.8247251510620117\n",
      "Epoch: 2930: Train loss: 2.8636553287506104 Valid loss: 3.825399398803711\n",
      "Epoch: 2931: Train loss: 2.8635952472686768 Valid loss: 3.826066493988037\n",
      "Epoch: 2932: Train loss: 2.8635377883911133 Valid loss: 3.826742649078369\n",
      "Epoch: 2933: Train loss: 2.8634791374206543 Valid loss: 3.827404737472534\n",
      "Epoch: 2934: Train loss: 2.8634212017059326 Valid loss: 3.828083038330078\n",
      "Epoch: 2935: Train loss: 2.8633625507354736 Valid loss: 3.828746795654297\n",
      "Epoch: 2936: Train loss: 2.8633034229278564 Valid loss: 3.829411029815674\n",
      "Epoch: 2937: Train loss: 2.8632471561431885 Valid loss: 3.8300845623016357\n",
      "Epoch: 2938: Train loss: 2.863189697265625 Valid loss: 3.8307580947875977\n",
      "Epoch: 2939: Train loss: 2.8631324768066406 Valid loss: 3.8314244747161865\n",
      "Epoch: 2940: Train loss: 2.8630752563476562 Valid loss: 3.8320837020874023\n",
      "Epoch: 2941: Train loss: 2.8630168437957764 Valid loss: 3.832754135131836\n",
      "Epoch: 2942: Train loss: 2.8629605770111084 Valid loss: 3.8334152698516846\n",
      "Epoch: 2943: Train loss: 2.862903118133545 Valid loss: 3.834085702896118\n",
      "Epoch: 2944: Train loss: 2.862847328186035 Valid loss: 3.834749221801758\n",
      "Epoch: 2945: Train loss: 2.8627889156341553 Valid loss: 3.835414409637451\n",
      "Epoch: 2946: Train loss: 2.8627331256866455 Valid loss: 3.836063861846924\n",
      "Epoch: 2947: Train loss: 2.862677574157715 Valid loss: 3.836729049682617\n",
      "Epoch: 2948: Train loss: 2.8626201152801514 Valid loss: 3.8373947143554688\n",
      "Epoch: 2949: Train loss: 2.8625643253326416 Valid loss: 3.83805513381958\n",
      "Epoch: 2950: Train loss: 2.86250901222229 Valid loss: 3.8387157917022705\n",
      "Epoch: 2951: Train loss: 2.8624515533447266 Valid loss: 3.8393781185150146\n",
      "Epoch: 2952: Train loss: 2.8623974323272705 Valid loss: 3.8400321006774902\n",
      "Epoch: 2953: Train loss: 2.8623406887054443 Valid loss: 3.840696096420288\n",
      "Epoch: 2954: Train loss: 2.8622848987579346 Valid loss: 3.8413519859313965\n",
      "Epoch: 2955: Train loss: 2.862229347229004 Valid loss: 3.8420090675354004\n",
      "Epoch: 2956: Train loss: 2.862175226211548 Valid loss: 3.8426668643951416\n",
      "Epoch: 2957: Train loss: 2.8621184825897217 Valid loss: 3.8433189392089844\n",
      "Epoch: 2958: Train loss: 2.8620638847351074 Valid loss: 3.8439807891845703\n",
      "Epoch: 2959: Train loss: 2.8620100021362305 Valid loss: 3.8446383476257324\n",
      "Epoch: 2960: Train loss: 2.861955404281616 Valid loss: 3.8452930450439453\n",
      "Epoch: 2961: Train loss: 2.8619017601013184 Valid loss: 3.845947504043579\n",
      "Epoch: 2962: Train loss: 2.8618454933166504 Valid loss: 3.846601963043213\n",
      "Epoch: 2963: Train loss: 2.8617918491363525 Valid loss: 3.8472588062286377\n",
      "Epoch: 2964: Train loss: 2.8617355823516846 Valid loss: 3.8479080200195312\n",
      "Epoch: 2965: Train loss: 2.8616836071014404 Valid loss: 3.8485665321350098\n",
      "Epoch: 2966: Train loss: 2.861628532409668 Valid loss: 3.8492090702056885\n",
      "Epoch: 2967: Train loss: 2.8615753650665283 Valid loss: 3.849860668182373\n",
      "Epoch: 2968: Train loss: 2.861520767211914 Valid loss: 3.8505196571350098\n",
      "Epoch: 2969: Train loss: 2.8614678382873535 Valid loss: 3.851166248321533\n",
      "Epoch: 2970: Train loss: 2.8614132404327393 Valid loss: 3.8518199920654297\n",
      "Epoch: 2971: Train loss: 2.8613622188568115 Valid loss: 3.852466344833374\n",
      "Epoch: 2972: Train loss: 2.8613076210021973 Valid loss: 3.8531150817871094\n",
      "Epoch: 2973: Train loss: 2.8612546920776367 Valid loss: 3.8537654876708984\n",
      "Epoch: 2974: Train loss: 2.861201047897339 Valid loss: 3.85440731048584\n",
      "Epoch: 2975: Train loss: 2.8611483573913574 Valid loss: 3.855057716369629\n",
      "Epoch: 2976: Train loss: 2.861095428466797 Valid loss: 3.855700969696045\n",
      "Epoch: 2977: Train loss: 2.8610446453094482 Valid loss: 3.856351852416992\n",
      "Epoch: 2978: Train loss: 2.8609914779663086 Valid loss: 3.8569884300231934\n",
      "Epoch: 2979: Train loss: 2.8609375953674316 Valid loss: 3.8576340675354004\n",
      "Epoch: 2980: Train loss: 2.8608856201171875 Valid loss: 3.8582816123962402\n",
      "Epoch: 2981: Train loss: 2.8608345985412598 Valid loss: 3.85892915725708\n",
      "Epoch: 2982: Train loss: 2.860781192779541 Valid loss: 3.859569549560547\n",
      "Epoch: 2983: Train loss: 2.8607306480407715 Valid loss: 3.8602123260498047\n",
      "Epoch: 2984: Train loss: 2.8606786727905273 Valid loss: 3.8608531951904297\n",
      "Epoch: 2985: Train loss: 2.860625982284546 Valid loss: 3.8614954948425293\n",
      "Epoch: 2986: Train loss: 2.8605756759643555 Valid loss: 3.86214017868042\n",
      "Epoch: 2987: Train loss: 2.8605246543884277 Valid loss: 3.8627758026123047\n",
      "Epoch: 2988: Train loss: 2.860471248626709 Valid loss: 3.863422393798828\n",
      "Epoch: 2989: Train loss: 2.8604207038879395 Valid loss: 3.8640599250793457\n",
      "Epoch: 2990: Train loss: 2.8603689670562744 Valid loss: 3.864699363708496\n",
      "Epoch: 2991: Train loss: 2.860318660736084 Valid loss: 3.8653392791748047\n",
      "Epoch: 2992: Train loss: 2.8602685928344727 Valid loss: 3.865971565246582\n",
      "Epoch: 2993: Train loss: 2.860217571258545 Valid loss: 3.8666114807128906\n",
      "Epoch: 2994: Train loss: 2.8601667881011963 Valid loss: 3.867245674133301\n",
      "Epoch: 2995: Train loss: 2.8601179122924805 Valid loss: 3.867882251739502\n",
      "Epoch: 2996: Train loss: 2.860065460205078 Valid loss: 3.868516445159912\n",
      "Epoch: 2997: Train loss: 2.860016107559204 Valid loss: 3.869154930114746\n",
      "Epoch: 2998: Train loss: 2.8599674701690674 Valid loss: 3.869800567626953\n",
      "Epoch: 2999: Train loss: 2.8599154949188232 Valid loss: 3.8704233169555664\n",
      "Epoch: 3000: Train loss: 2.8598663806915283 Valid loss: 3.8710596561431885\n"
     ]
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs=3000,\n",
    "    optimizer=optimizer,\n",
    "    model=linear_model,\n",
    "    loss_fn=nn.MSELoss(),\n",
    "    t_u_train=t_un_train,\n",
    "    t_u_val=t_un_val,\n",
    "    t_c_train=t_c_train,\n",
    "    t_c_val=t_c_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[5.4243]], requires_grad=True), Parameter containing:\n",
      "tensor([-17.9264], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(linear_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=5, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = nn.Sequential(\n",
    "    nn.Linear(1, 5),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(5, 1),\n",
    ")\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([5, 1]), torch.Size([5]), torch.Size([1, 5]), torch.Size([1])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.shape for param in seq_model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight torch.Size([5, 1])\n",
      "0.bias torch.Size([5])\n",
      "2.weight torch.Size([1, 5])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (Linear_Block): Linear(in_features=1, out_features=8, bias=True)\n",
       "  (hidden_activation): Tanh()\n",
       "  (output_linear): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "seq_model = nn.Sequential(OrderedDict([\n",
    "    ('Linear_Block', nn.Linear(1, 8)),\n",
    "    ('hidden_activation', nn.Tanh()),\n",
    "    ('output_linear', nn.Linear(8, 1)),  \n",
    "]))\n",
    "seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1: Train loss: 177.65252685546875 Valid loss: 227.84193420410156\n",
      "Epoch: 2: Train loss: 173.54808044433594 Valid loss: 224.47373962402344\n",
      "Epoch: 3: Train loss: 169.575927734375 Valid loss: 221.2199249267578\n",
      "Epoch: 4: Train loss: 165.73329162597656 Valid loss: 218.07791137695312\n",
      "Epoch: 5: Train loss: 162.01712036132812 Valid loss: 215.04483032226562\n",
      "Epoch: 6: Train loss: 158.42396545410156 Valid loss: 212.11766052246094\n",
      "Epoch: 7: Train loss: 154.9503936767578 Valid loss: 209.29319763183594\n",
      "Epoch: 8: Train loss: 151.59278869628906 Valid loss: 206.56825256347656\n",
      "Epoch: 9: Train loss: 148.34751892089844 Valid loss: 203.9395751953125\n",
      "Epoch: 10: Train loss: 145.2110137939453 Valid loss: 201.40408325195312\n",
      "Epoch: 11: Train loss: 142.17974853515625 Valid loss: 198.9585418701172\n",
      "Epoch: 12: Train loss: 139.25018310546875 Valid loss: 196.5999755859375\n",
      "Epoch: 13: Train loss: 136.41897583007812 Valid loss: 194.32534790039062\n",
      "Epoch: 14: Train loss: 133.6827850341797 Valid loss: 192.1317596435547\n",
      "Epoch: 15: Train loss: 131.03839111328125 Valid loss: 190.0164794921875\n",
      "Epoch: 16: Train loss: 128.4827423095703 Valid loss: 187.9766845703125\n",
      "Epoch: 17: Train loss: 126.01283264160156 Valid loss: 186.00982666015625\n",
      "Epoch: 18: Train loss: 123.625732421875 Valid loss: 184.1133270263672\n",
      "Epoch: 19: Train loss: 121.31869506835938 Valid loss: 182.2847442626953\n",
      "Epoch: 20: Train loss: 119.08905792236328 Valid loss: 180.52171325683594\n",
      "Epoch: 21: Train loss: 116.9342041015625 Valid loss: 178.82200622558594\n",
      "Epoch: 22: Train loss: 114.85166931152344 Valid loss: 177.183349609375\n",
      "Epoch: 23: Train loss: 112.83909606933594 Valid loss: 175.60366821289062\n",
      "Epoch: 24: Train loss: 110.89414978027344 Valid loss: 174.08090209960938\n",
      "Epoch: 25: Train loss: 109.0146255493164 Valid loss: 172.6130828857422\n",
      "Epoch: 26: Train loss: 107.19837951660156 Valid loss: 171.19833374023438\n",
      "Epoch: 27: Train loss: 105.44335174560547 Valid loss: 169.8347625732422\n",
      "Epoch: 28: Train loss: 103.74752807617188 Valid loss: 168.52053833007812\n",
      "Epoch: 29: Train loss: 102.10899353027344 Valid loss: 167.25396728515625\n",
      "Epoch: 30: Train loss: 100.52588653564453 Valid loss: 166.0333709716797\n",
      "Epoch: 31: Train loss: 98.99639129638672 Valid loss: 164.8571319580078\n",
      "Epoch: 32: Train loss: 97.51873779296875 Valid loss: 163.72360229492188\n",
      "Epoch: 33: Train loss: 96.09123992919922 Valid loss: 162.63131713867188\n",
      "Epoch: 34: Train loss: 94.7122573852539 Valid loss: 161.57872009277344\n",
      "Epoch: 35: Train loss: 93.38016510009766 Valid loss: 160.5643768310547\n",
      "Epoch: 36: Train loss: 92.09344482421875 Valid loss: 159.58688354492188\n",
      "Epoch: 37: Train loss: 90.85057067871094 Valid loss: 158.6448211669922\n",
      "Epoch: 38: Train loss: 89.65007781982422 Valid loss: 157.73695373535156\n",
      "Epoch: 39: Train loss: 88.49056243896484 Valid loss: 156.86187744140625\n",
      "Epoch: 40: Train loss: 87.37065124511719 Valid loss: 156.0184326171875\n",
      "Epoch: 41: Train loss: 86.28900909423828 Valid loss: 155.20535278320312\n",
      "Epoch: 42: Train loss: 85.24433135986328 Valid loss: 154.42144775390625\n",
      "Epoch: 43: Train loss: 84.23538208007812 Valid loss: 153.66558837890625\n",
      "Epoch: 44: Train loss: 83.26091766357422 Valid loss: 152.93666076660156\n",
      "Epoch: 45: Train loss: 82.31979370117188 Valid loss: 152.23358154296875\n",
      "Epoch: 46: Train loss: 81.41087341308594 Valid loss: 151.55531311035156\n",
      "Epoch: 47: Train loss: 80.53299713134766 Valid loss: 150.9008026123047\n",
      "Epoch: 48: Train loss: 79.68515014648438 Valid loss: 150.2690887451172\n",
      "Epoch: 49: Train loss: 78.86624908447266 Valid loss: 149.65921020507812\n",
      "Epoch: 50: Train loss: 78.0753173828125 Valid loss: 149.07025146484375\n",
      "Epoch: 51: Train loss: 77.31137084960938 Valid loss: 148.50128173828125\n",
      "Epoch: 52: Train loss: 76.5734634399414 Valid loss: 147.95147705078125\n",
      "Epoch: 53: Train loss: 75.86067199707031 Valid loss: 147.419921875\n",
      "Epoch: 54: Train loss: 75.17212677001953 Valid loss: 146.9058380126953\n",
      "Epoch: 55: Train loss: 74.50697326660156 Valid loss: 146.40841674804688\n",
      "Epoch: 56: Train loss: 73.86436462402344 Valid loss: 145.9268798828125\n",
      "Epoch: 57: Train loss: 73.2435073852539 Valid loss: 145.4604949951172\n",
      "Epoch: 58: Train loss: 72.64363098144531 Valid loss: 145.00851440429688\n",
      "Epoch: 59: Train loss: 72.06396484375 Valid loss: 144.57025146484375\n",
      "Epoch: 60: Train loss: 71.50379180908203 Valid loss: 144.14498901367188\n",
      "Epoch: 61: Train loss: 70.96240997314453 Valid loss: 143.73208618164062\n",
      "Epoch: 62: Train loss: 70.43912506103516 Valid loss: 143.33084106445312\n",
      "Epoch: 63: Train loss: 69.93328094482422 Valid loss: 142.94068908691406\n",
      "Epoch: 64: Train loss: 69.4442367553711 Valid loss: 142.5609893798828\n",
      "Epoch: 65: Train loss: 68.97136688232422 Valid loss: 142.1911163330078\n",
      "Epoch: 66: Train loss: 68.51409149169922 Valid loss: 141.8305206298828\n",
      "Epoch: 67: Train loss: 68.0718002319336 Valid loss: 141.47860717773438\n",
      "Epoch: 68: Train loss: 67.6439437866211 Valid loss: 141.1348419189453\n",
      "Epoch: 69: Train loss: 67.22997283935547 Valid loss: 140.79867553710938\n",
      "Epoch: 70: Train loss: 66.82936096191406 Valid loss: 140.4695587158203\n",
      "Epoch: 71: Train loss: 66.44156646728516 Valid loss: 140.14700317382812\n",
      "Epoch: 72: Train loss: 66.06614685058594 Valid loss: 139.83045959472656\n",
      "Epoch: 73: Train loss: 65.70256042480469 Valid loss: 139.51947021484375\n",
      "Epoch: 74: Train loss: 65.35037231445312 Valid loss: 139.2135467529297\n",
      "Epoch: 75: Train loss: 65.00911712646484 Valid loss: 138.91220092773438\n",
      "Epoch: 76: Train loss: 64.67832946777344 Valid loss: 138.61492919921875\n",
      "Epoch: 77: Train loss: 64.35759735107422 Valid loss: 138.3212890625\n",
      "Epoch: 78: Train loss: 64.0465087890625 Valid loss: 138.03085327148438\n",
      "Epoch: 79: Train loss: 63.74461364746094 Valid loss: 137.74310302734375\n",
      "Epoch: 80: Train loss: 63.45155334472656 Valid loss: 137.45767211914062\n",
      "Epoch: 81: Train loss: 63.16691207885742 Valid loss: 137.17405700683594\n",
      "Epoch: 82: Train loss: 62.89031219482422 Valid loss: 136.891845703125\n",
      "Epoch: 83: Train loss: 62.62139129638672 Valid loss: 136.610595703125\n",
      "Epoch: 84: Train loss: 62.35977554321289 Valid loss: 136.32986450195312\n",
      "Epoch: 85: Train loss: 62.1051025390625 Valid loss: 136.0492706298828\n",
      "Epoch: 86: Train loss: 61.85701370239258 Valid loss: 135.76834106445312\n",
      "Epoch: 87: Train loss: 61.61515426635742 Valid loss: 135.48666381835938\n",
      "Epoch: 88: Train loss: 61.37921142578125 Valid loss: 135.20384216308594\n",
      "Epoch: 89: Train loss: 61.14881896972656 Valid loss: 134.91943359375\n",
      "Epoch: 90: Train loss: 60.92366027832031 Valid loss: 134.63304138183594\n",
      "Epoch: 91: Train loss: 60.70338439941406 Valid loss: 134.34425354003906\n",
      "Epoch: 92: Train loss: 60.48767852783203 Valid loss: 134.05267333984375\n",
      "Epoch: 93: Train loss: 60.27621841430664 Valid loss: 133.75790405273438\n",
      "Epoch: 94: Train loss: 60.06867980957031 Valid loss: 133.45953369140625\n",
      "Epoch: 95: Train loss: 59.86473083496094 Valid loss: 133.1572265625\n",
      "Epoch: 96: Train loss: 59.66405487060547 Valid loss: 132.8505859375\n",
      "Epoch: 97: Train loss: 59.46636199951172 Valid loss: 132.539306640625\n",
      "Epoch: 98: Train loss: 59.27129364013672 Valid loss: 132.22305297851562\n",
      "Epoch: 99: Train loss: 59.07857131958008 Valid loss: 131.90150451660156\n",
      "Epoch: 100: Train loss: 58.88789367675781 Valid loss: 131.574462890625\n",
      "Epoch: 101: Train loss: 58.69893264770508 Valid loss: 131.24171447753906\n",
      "Epoch: 102: Train loss: 58.51142883300781 Valid loss: 130.903076171875\n",
      "Epoch: 103: Train loss: 58.32508087158203 Valid loss: 130.5584716796875\n",
      "Epoch: 104: Train loss: 58.1396484375 Valid loss: 130.20797729492188\n",
      "Epoch: 105: Train loss: 57.95490264892578 Valid loss: 129.85162353515625\n",
      "Epoch: 106: Train loss: 57.77061080932617 Valid loss: 129.48968505859375\n",
      "Epoch: 107: Train loss: 57.58661651611328 Valid loss: 129.12246704101562\n",
      "Epoch: 108: Train loss: 57.40276336669922 Valid loss: 128.75051879882812\n",
      "Epoch: 109: Train loss: 57.21901321411133 Valid loss: 128.37451171875\n",
      "Epoch: 110: Train loss: 57.03533935546875 Valid loss: 127.99532318115234\n",
      "Epoch: 111: Train loss: 56.85179901123047 Valid loss: 127.61400604248047\n",
      "Epoch: 112: Train loss: 56.66853713989258 Valid loss: 127.23181915283203\n",
      "Epoch: 113: Train loss: 56.48579406738281 Valid loss: 126.85023498535156\n",
      "Epoch: 114: Train loss: 56.303871154785156 Valid loss: 126.47086334228516\n",
      "Epoch: 115: Train loss: 56.1231803894043 Valid loss: 126.0954360961914\n",
      "Epoch: 116: Train loss: 55.944175720214844 Valid loss: 125.72579956054688\n",
      "Epoch: 117: Train loss: 55.76736831665039 Valid loss: 125.36373901367188\n",
      "Epoch: 118: Train loss: 55.59330749511719 Valid loss: 125.01093292236328\n",
      "Epoch: 119: Train loss: 55.42250061035156 Valid loss: 124.66889953613281\n",
      "Epoch: 120: Train loss: 55.25539779663086 Valid loss: 124.3388671875\n",
      "Epoch: 121: Train loss: 55.092403411865234 Valid loss: 124.02164459228516\n",
      "Epoch: 122: Train loss: 54.93373107910156 Valid loss: 123.7176284790039\n",
      "Epoch: 123: Train loss: 54.77952575683594 Valid loss: 123.42684936523438\n",
      "Epoch: 124: Train loss: 54.62976837158203 Valid loss: 123.14881896972656\n",
      "Epoch: 125: Train loss: 54.48431396484375 Valid loss: 122.88279724121094\n",
      "Epoch: 126: Train loss: 54.34296417236328 Valid loss: 122.62772369384766\n",
      "Epoch: 127: Train loss: 54.20539474487305 Valid loss: 122.38238525390625\n",
      "Epoch: 128: Train loss: 54.071292877197266 Valid loss: 122.1455078125\n",
      "Epoch: 129: Train loss: 53.94032287597656 Valid loss: 121.91585540771484\n",
      "Epoch: 130: Train loss: 53.81217575073242 Valid loss: 121.69218444824219\n",
      "Epoch: 131: Train loss: 53.68656539916992 Valid loss: 121.47348022460938\n",
      "Epoch: 132: Train loss: 53.563236236572266 Valid loss: 121.2587890625\n",
      "Epoch: 133: Train loss: 53.441986083984375 Valid loss: 121.04731750488281\n",
      "Epoch: 134: Train loss: 53.32265090942383 Valid loss: 120.83837890625\n",
      "Epoch: 135: Train loss: 53.2050666809082 Valid loss: 120.63148498535156\n",
      "Epoch: 136: Train loss: 53.08911895751953 Valid loss: 120.42619323730469\n",
      "Epoch: 137: Train loss: 52.97472381591797 Valid loss: 120.22213745117188\n",
      "Epoch: 138: Train loss: 52.86176681518555 Valid loss: 120.01904296875\n",
      "Epoch: 139: Train loss: 52.75020217895508 Valid loss: 119.81671142578125\n",
      "Epoch: 140: Train loss: 52.63997268676758 Valid loss: 119.6149673461914\n",
      "Epoch: 141: Train loss: 52.531005859375 Valid loss: 119.41370391845703\n",
      "Epoch: 142: Train loss: 52.42324447631836 Valid loss: 119.21277618408203\n",
      "Epoch: 143: Train loss: 52.3166618347168 Valid loss: 119.01213073730469\n",
      "Epoch: 144: Train loss: 52.21120834350586 Valid loss: 118.81167602539062\n",
      "Epoch: 145: Train loss: 52.10683822631836 Valid loss: 118.61135864257812\n",
      "Epoch: 146: Train loss: 52.00351333618164 Valid loss: 118.41114807128906\n",
      "Epoch: 147: Train loss: 51.90119171142578 Valid loss: 118.21096801757812\n",
      "Epoch: 148: Train loss: 51.79983139038086 Valid loss: 118.01081848144531\n",
      "Epoch: 149: Train loss: 51.69939422607422 Valid loss: 117.81063079833984\n",
      "Epoch: 150: Train loss: 51.59986114501953 Valid loss: 117.6103515625\n",
      "Epoch: 151: Train loss: 51.50117874145508 Valid loss: 117.40999603271484\n",
      "Epoch: 152: Train loss: 51.4033203125 Valid loss: 117.20948791503906\n",
      "Epoch: 153: Train loss: 51.30623245239258 Valid loss: 117.0087890625\n",
      "Epoch: 154: Train loss: 51.20989227294922 Valid loss: 116.807861328125\n",
      "Epoch: 155: Train loss: 51.11426544189453 Valid loss: 116.60664367675781\n",
      "Epoch: 156: Train loss: 51.01930618286133 Valid loss: 116.40510559082031\n",
      "Epoch: 157: Train loss: 50.92498779296875 Valid loss: 116.20320129394531\n",
      "Epoch: 158: Train loss: 50.83126449584961 Valid loss: 116.00086975097656\n",
      "Epoch: 159: Train loss: 50.73810577392578 Valid loss: 115.79808044433594\n",
      "Epoch: 160: Train loss: 50.64546585083008 Valid loss: 115.59474182128906\n",
      "Epoch: 161: Train loss: 50.55331802368164 Valid loss: 115.39079284667969\n",
      "Epoch: 162: Train loss: 50.46162414550781 Valid loss: 115.18624877929688\n",
      "Epoch: 163: Train loss: 50.370330810546875 Valid loss: 114.98097229003906\n",
      "Epoch: 164: Train loss: 50.27943420410156 Valid loss: 114.77496337890625\n",
      "Epoch: 165: Train loss: 50.18887710571289 Valid loss: 114.568115234375\n",
      "Epoch: 166: Train loss: 50.09861373901367 Valid loss: 114.36042785644531\n",
      "Epoch: 167: Train loss: 50.00862121582031 Valid loss: 114.15179443359375\n",
      "Epoch: 168: Train loss: 49.91887664794922 Valid loss: 113.94219970703125\n",
      "Epoch: 169: Train loss: 49.82931137084961 Valid loss: 113.7315673828125\n",
      "Epoch: 170: Train loss: 49.73991775512695 Valid loss: 113.51985931396484\n",
      "Epoch: 171: Train loss: 49.650657653808594 Valid loss: 113.30702209472656\n",
      "Epoch: 172: Train loss: 49.56149673461914 Valid loss: 113.09304809570312\n",
      "Epoch: 173: Train loss: 49.47239685058594 Valid loss: 112.87788391113281\n",
      "Epoch: 174: Train loss: 49.38334274291992 Valid loss: 112.66149139404297\n",
      "Epoch: 175: Train loss: 49.29429244995117 Valid loss: 112.4438705444336\n",
      "Epoch: 176: Train loss: 49.2052116394043 Valid loss: 112.22498321533203\n",
      "Epoch: 177: Train loss: 49.11610412597656 Valid loss: 112.00486755371094\n",
      "Epoch: 178: Train loss: 49.02692794799805 Valid loss: 111.78349304199219\n",
      "Epoch: 179: Train loss: 48.93766784667969 Valid loss: 111.56088256835938\n",
      "Epoch: 180: Train loss: 48.84831237792969 Valid loss: 111.3370590209961\n",
      "Epoch: 181: Train loss: 48.75884246826172 Valid loss: 111.11209106445312\n",
      "Epoch: 182: Train loss: 48.669254302978516 Valid loss: 110.8860092163086\n",
      "Epoch: 183: Train loss: 48.57954788208008 Valid loss: 110.65889739990234\n",
      "Epoch: 184: Train loss: 48.48971939086914 Valid loss: 110.43081665039062\n",
      "Epoch: 185: Train loss: 48.3997802734375 Valid loss: 110.201904296875\n",
      "Epoch: 186: Train loss: 48.30974578857422 Valid loss: 109.9722671508789\n",
      "Epoch: 187: Train loss: 48.21963119506836 Valid loss: 109.74203491210938\n",
      "Epoch: 188: Train loss: 48.12947082519531 Valid loss: 109.51138305664062\n",
      "Epoch: 189: Train loss: 48.03929138183594 Valid loss: 109.28045654296875\n",
      "Epoch: 190: Train loss: 47.94914245605469 Valid loss: 109.04945373535156\n",
      "Epoch: 191: Train loss: 47.85907745361328 Valid loss: 108.818603515625\n",
      "Epoch: 192: Train loss: 47.76913070678711 Valid loss: 108.5881118774414\n",
      "Epoch: 193: Train loss: 47.67939758300781 Valid loss: 108.35816955566406\n",
      "Epoch: 194: Train loss: 47.58991241455078 Valid loss: 108.12905883789062\n",
      "Epoch: 195: Train loss: 47.50075149536133 Valid loss: 107.90096282958984\n",
      "Epoch: 196: Train loss: 47.412010192871094 Valid loss: 107.67417907714844\n",
      "Epoch: 197: Train loss: 47.32373809814453 Valid loss: 107.44886779785156\n",
      "Epoch: 198: Train loss: 47.23601531982422 Valid loss: 107.22525787353516\n",
      "Epoch: 199: Train loss: 47.14891815185547 Valid loss: 107.00355529785156\n",
      "Epoch: 200: Train loss: 47.06251525878906 Valid loss: 106.78392028808594\n",
      "Epoch: 201: Train loss: 46.97684097290039 Valid loss: 106.56646728515625\n",
      "Epoch: 202: Train loss: 46.8919677734375 Valid loss: 106.35134887695312\n",
      "Epoch: 203: Train loss: 46.80792999267578 Valid loss: 106.13861083984375\n",
      "Epoch: 204: Train loss: 46.72475051879883 Valid loss: 105.92828369140625\n",
      "Epoch: 205: Train loss: 46.6424446105957 Valid loss: 105.72036743164062\n",
      "Epoch: 206: Train loss: 46.56100845336914 Valid loss: 105.51484680175781\n",
      "Epoch: 207: Train loss: 46.48043441772461 Valid loss: 105.31161499023438\n",
      "Epoch: 208: Train loss: 46.400718688964844 Valid loss: 105.11062622070312\n",
      "Epoch: 209: Train loss: 46.32181167602539 Valid loss: 104.91168212890625\n",
      "Epoch: 210: Train loss: 46.24369430541992 Valid loss: 104.71472930908203\n",
      "Epoch: 211: Train loss: 46.16630554199219 Valid loss: 104.51953125\n",
      "Epoch: 212: Train loss: 46.08961486816406 Valid loss: 104.3259506225586\n",
      "Epoch: 213: Train loss: 46.01355743408203 Valid loss: 104.13382720947266\n",
      "Epoch: 214: Train loss: 45.93808364868164 Valid loss: 103.94294738769531\n",
      "Epoch: 215: Train loss: 45.86315155029297 Valid loss: 103.753173828125\n",
      "Epoch: 216: Train loss: 45.78871154785156 Valid loss: 103.56436157226562\n",
      "Epoch: 217: Train loss: 45.7147102355957 Valid loss: 103.37629699707031\n",
      "Epoch: 218: Train loss: 45.64111328125 Valid loss: 103.1888656616211\n",
      "Epoch: 219: Train loss: 45.567867279052734 Valid loss: 103.00193786621094\n",
      "Epoch: 220: Train loss: 45.49494552612305 Valid loss: 102.81539916992188\n",
      "Epoch: 221: Train loss: 45.42231369018555 Valid loss: 102.62908935546875\n",
      "Epoch: 222: Train loss: 45.34992980957031 Valid loss: 102.44293975830078\n",
      "Epoch: 223: Train loss: 45.27778625488281 Valid loss: 102.25685119628906\n",
      "Epoch: 224: Train loss: 45.20583724975586 Valid loss: 102.07073974609375\n",
      "Epoch: 225: Train loss: 45.13407897949219 Valid loss: 101.88450622558594\n",
      "Epoch: 226: Train loss: 45.06249237060547 Valid loss: 101.6981201171875\n",
      "Epoch: 227: Train loss: 44.99104309082031 Valid loss: 101.51152038574219\n",
      "Epoch: 228: Train loss: 44.919715881347656 Valid loss: 101.32461547851562\n",
      "Epoch: 229: Train loss: 44.84852600097656 Valid loss: 101.13738250732422\n",
      "Epoch: 230: Train loss: 44.777435302734375 Valid loss: 100.94976806640625\n",
      "Epoch: 231: Train loss: 44.70643997192383 Valid loss: 100.76174926757812\n",
      "Epoch: 232: Train loss: 44.63551330566406 Valid loss: 100.57327270507812\n",
      "Epoch: 233: Train loss: 44.564674377441406 Valid loss: 100.38430786132812\n",
      "Epoch: 234: Train loss: 44.49388885498047 Valid loss: 100.19483947753906\n",
      "Epoch: 235: Train loss: 44.42316818237305 Valid loss: 100.00481414794922\n",
      "Epoch: 236: Train loss: 44.35248565673828 Valid loss: 99.81423950195312\n",
      "Epoch: 237: Train loss: 44.28184127807617 Valid loss: 99.623046875\n",
      "Epoch: 238: Train loss: 44.21122360229492 Valid loss: 99.43122863769531\n",
      "Epoch: 239: Train loss: 44.14063262939453 Valid loss: 99.23880004882812\n",
      "Epoch: 240: Train loss: 44.07005310058594 Valid loss: 99.04570770263672\n",
      "Epoch: 241: Train loss: 43.99946975708008 Valid loss: 98.8519287109375\n",
      "Epoch: 242: Train loss: 43.92890167236328 Valid loss: 98.657470703125\n",
      "Epoch: 243: Train loss: 43.85832214355469 Valid loss: 98.46224975585938\n",
      "Epoch: 244: Train loss: 43.787715911865234 Valid loss: 98.26632690429688\n",
      "Epoch: 245: Train loss: 43.71710205078125 Valid loss: 98.06964111328125\n",
      "Epoch: 246: Train loss: 43.64645004272461 Valid loss: 97.8721923828125\n",
      "Epoch: 247: Train loss: 43.57575988769531 Valid loss: 97.67394256591797\n",
      "Epoch: 248: Train loss: 43.50502395629883 Valid loss: 97.47489929199219\n",
      "Epoch: 249: Train loss: 43.43424987792969 Valid loss: 97.27500915527344\n",
      "Epoch: 250: Train loss: 43.363399505615234 Valid loss: 97.07426452636719\n",
      "Epoch: 251: Train loss: 43.2924919128418 Valid loss: 96.8726806640625\n",
      "Epoch: 252: Train loss: 43.22151184082031 Valid loss: 96.67019653320312\n",
      "Epoch: 253: Train loss: 43.15043640136719 Valid loss: 96.46678924560547\n",
      "Epoch: 254: Train loss: 43.07929229736328 Valid loss: 96.26248168945312\n",
      "Epoch: 255: Train loss: 43.00803756713867 Valid loss: 96.05722045898438\n",
      "Epoch: 256: Train loss: 42.93667984008789 Valid loss: 95.85101318359375\n",
      "Epoch: 257: Train loss: 42.86520767211914 Valid loss: 95.64379119873047\n",
      "Epoch: 258: Train loss: 42.79362106323242 Valid loss: 95.43558502197266\n",
      "Epoch: 259: Train loss: 42.721900939941406 Valid loss: 95.22632598876953\n",
      "Epoch: 260: Train loss: 42.65003967285156 Valid loss: 95.01600646972656\n",
      "Epoch: 261: Train loss: 42.578033447265625 Valid loss: 94.80464172363281\n",
      "Epoch: 262: Train loss: 42.50587463378906 Valid loss: 94.59215545654297\n",
      "Epoch: 263: Train loss: 42.43354797363281 Valid loss: 94.37857055664062\n",
      "Epoch: 264: Train loss: 42.36104965209961 Valid loss: 94.16380310058594\n",
      "Epoch: 265: Train loss: 42.28836441040039 Valid loss: 93.9478759765625\n",
      "Epoch: 266: Train loss: 42.21548843383789 Valid loss: 93.73077392578125\n",
      "Epoch: 267: Train loss: 42.142417907714844 Valid loss: 93.51240539550781\n",
      "Epoch: 268: Train loss: 42.069122314453125 Valid loss: 93.29278564453125\n",
      "Epoch: 269: Train loss: 41.99560546875 Valid loss: 93.07191467285156\n",
      "Epoch: 270: Train loss: 41.92185974121094 Valid loss: 92.84971618652344\n",
      "Epoch: 271: Train loss: 41.84787368774414 Valid loss: 92.62615966796875\n",
      "Epoch: 272: Train loss: 41.77361297607422 Valid loss: 92.40122985839844\n",
      "Epoch: 273: Train loss: 41.6990966796875 Valid loss: 92.17491149902344\n",
      "Epoch: 274: Train loss: 41.624290466308594 Valid loss: 91.94713592529297\n",
      "Epoch: 275: Train loss: 41.549198150634766 Valid loss: 91.71791076660156\n",
      "Epoch: 276: Train loss: 41.47380065917969 Valid loss: 91.4871826171875\n",
      "Epoch: 277: Train loss: 41.39808654785156 Valid loss: 91.2548828125\n",
      "Epoch: 278: Train loss: 41.32204055786133 Valid loss: 91.02104187011719\n",
      "Epoch: 279: Train loss: 41.24563980102539 Valid loss: 90.78556823730469\n",
      "Epoch: 280: Train loss: 41.16889190673828 Valid loss: 90.5484848022461\n",
      "Epoch: 281: Train loss: 41.09176254272461 Valid loss: 90.3096923828125\n",
      "Epoch: 282: Train loss: 41.01424026489258 Valid loss: 90.06918334960938\n",
      "Epoch: 283: Train loss: 40.93632125854492 Valid loss: 89.82691955566406\n",
      "Epoch: 284: Train loss: 40.85797882080078 Valid loss: 89.58287048339844\n",
      "Epoch: 285: Train loss: 40.77918243408203 Valid loss: 89.33697509765625\n",
      "Epoch: 286: Train loss: 40.699951171875 Valid loss: 89.08920288085938\n",
      "Epoch: 287: Train loss: 40.62024688720703 Valid loss: 88.83953094482422\n",
      "Epoch: 288: Train loss: 40.54005432128906 Valid loss: 88.58792114257812\n",
      "Epoch: 289: Train loss: 40.4593620300293 Valid loss: 88.33431243896484\n",
      "Epoch: 290: Train loss: 40.37813186645508 Valid loss: 88.07868194580078\n",
      "Epoch: 291: Train loss: 40.2963752746582 Valid loss: 87.82101440429688\n",
      "Epoch: 292: Train loss: 40.21406555175781 Valid loss: 87.56123352050781\n",
      "Epoch: 293: Train loss: 40.13117599487305 Valid loss: 87.29933166503906\n",
      "Epoch: 294: Train loss: 40.04768753051758 Valid loss: 87.0352783203125\n",
      "Epoch: 295: Train loss: 39.96360397338867 Valid loss: 86.76905822753906\n",
      "Epoch: 296: Train loss: 39.87888717651367 Valid loss: 86.50060272216797\n",
      "Epoch: 297: Train loss: 39.79352569580078 Valid loss: 86.22991943359375\n",
      "Epoch: 298: Train loss: 39.70751190185547 Valid loss: 85.95699310302734\n",
      "Epoch: 299: Train loss: 39.62082290649414 Valid loss: 85.68179321289062\n",
      "Epoch: 300: Train loss: 39.5334587097168 Valid loss: 85.40435028076172\n",
      "Epoch: 301: Train loss: 39.445396423339844 Valid loss: 85.12459564208984\n",
      "Epoch: 302: Train loss: 39.35662841796875 Valid loss: 84.84259033203125\n",
      "Epoch: 303: Train loss: 39.267147064208984 Valid loss: 84.55831909179688\n",
      "Epoch: 304: Train loss: 39.17694854736328 Valid loss: 84.27183532714844\n",
      "Epoch: 305: Train loss: 39.08603286743164 Valid loss: 83.98310852050781\n",
      "Epoch: 306: Train loss: 38.99440002441406 Valid loss: 83.69219970703125\n",
      "Epoch: 307: Train loss: 38.90204620361328 Valid loss: 83.39918518066406\n",
      "Epoch: 308: Train loss: 38.80899429321289 Valid loss: 83.10406494140625\n",
      "Epoch: 309: Train loss: 38.71523666381836 Valid loss: 82.80696105957031\n",
      "Epoch: 310: Train loss: 38.62080001831055 Valid loss: 82.50790405273438\n",
      "Epoch: 311: Train loss: 38.52570343017578 Valid loss: 82.20700073242188\n",
      "Epoch: 312: Train loss: 38.429969787597656 Valid loss: 81.90432739257812\n",
      "Epoch: 313: Train loss: 38.3336296081543 Valid loss: 81.60002136230469\n",
      "Epoch: 314: Train loss: 38.23670959472656 Valid loss: 81.29415893554688\n",
      "Epoch: 315: Train loss: 38.13923645019531 Valid loss: 80.98687744140625\n",
      "Epoch: 316: Train loss: 38.04126739501953 Valid loss: 80.67826843261719\n",
      "Epoch: 317: Train loss: 37.94282531738281 Valid loss: 80.36848449707031\n",
      "Epoch: 318: Train loss: 37.84395217895508 Valid loss: 80.0576171875\n",
      "Epoch: 319: Train loss: 37.74468994140625 Valid loss: 79.74581146240234\n",
      "Epoch: 320: Train loss: 37.64507293701172 Valid loss: 79.43313598632812\n",
      "Epoch: 321: Train loss: 37.54513931274414 Valid loss: 79.11972045898438\n",
      "Epoch: 322: Train loss: 37.44490432739258 Valid loss: 78.80561065673828\n",
      "Epoch: 323: Train loss: 37.344417572021484 Valid loss: 78.49092102050781\n",
      "Epoch: 324: Train loss: 37.2436637878418 Valid loss: 78.1756591796875\n",
      "Epoch: 325: Train loss: 37.142669677734375 Valid loss: 77.85987854003906\n",
      "Epoch: 326: Train loss: 37.04142379760742 Valid loss: 77.54359436035156\n",
      "Epoch: 327: Train loss: 36.93992233276367 Valid loss: 77.22682189941406\n",
      "Epoch: 328: Train loss: 36.83812713623047 Valid loss: 76.90950775146484\n",
      "Epoch: 329: Train loss: 36.73603820800781 Valid loss: 76.5916519165039\n",
      "Epoch: 330: Train loss: 36.63359832763672 Valid loss: 76.27320861816406\n",
      "Epoch: 331: Train loss: 36.53078079223633 Valid loss: 75.95417785644531\n",
      "Epoch: 332: Train loss: 36.42754364013672 Valid loss: 75.63449096679688\n",
      "Epoch: 333: Train loss: 36.32386016845703 Valid loss: 75.314208984375\n",
      "Epoch: 334: Train loss: 36.21967315673828 Valid loss: 74.99331665039062\n",
      "Epoch: 335: Train loss: 36.11497116088867 Valid loss: 74.67186737060547\n",
      "Epoch: 336: Train loss: 36.00973892211914 Valid loss: 74.34996032714844\n",
      "Epoch: 337: Train loss: 35.903987884521484 Valid loss: 74.02777099609375\n",
      "Epoch: 338: Train loss: 35.79773712158203 Valid loss: 73.70548248291016\n",
      "Epoch: 339: Train loss: 35.6910400390625 Valid loss: 73.38340759277344\n",
      "Epoch: 340: Train loss: 35.58396911621094 Valid loss: 73.06181335449219\n",
      "Epoch: 341: Train loss: 35.47663879394531 Valid loss: 72.7412109375\n",
      "Epoch: 342: Train loss: 35.36918640136719 Valid loss: 72.42202758789062\n",
      "Epoch: 343: Train loss: 35.26177215576172 Valid loss: 72.10479736328125\n",
      "Epoch: 344: Train loss: 35.154605865478516 Valid loss: 71.7901840209961\n",
      "Epoch: 345: Train loss: 35.04790496826172 Valid loss: 71.47879028320312\n",
      "Epoch: 346: Train loss: 34.94192123413086 Valid loss: 71.17127990722656\n",
      "Epoch: 347: Train loss: 34.836917877197266 Valid loss: 70.86831665039062\n",
      "Epoch: 348: Train loss: 34.73312759399414 Valid loss: 70.57051086425781\n",
      "Epoch: 349: Train loss: 34.63081359863281 Valid loss: 70.27842712402344\n",
      "Epoch: 350: Train loss: 34.53019332885742 Valid loss: 69.99256896972656\n",
      "Epoch: 351: Train loss: 34.43144226074219 Valid loss: 69.71327209472656\n",
      "Epoch: 352: Train loss: 34.33470153808594 Valid loss: 69.44076538085938\n",
      "Epoch: 353: Train loss: 34.24005126953125 Valid loss: 69.17513275146484\n",
      "Epoch: 354: Train loss: 34.147525787353516 Valid loss: 68.91633605957031\n",
      "Epoch: 355: Train loss: 34.05707931518555 Valid loss: 68.66415405273438\n",
      "Epoch: 356: Train loss: 33.96864318847656 Valid loss: 68.41831970214844\n",
      "Epoch: 357: Train loss: 33.882080078125 Valid loss: 68.17839050292969\n",
      "Epoch: 358: Train loss: 33.7972526550293 Valid loss: 67.94393157958984\n",
      "Epoch: 359: Train loss: 33.71397399902344 Valid loss: 67.71441650390625\n",
      "Epoch: 360: Train loss: 33.6320686340332 Valid loss: 67.48934173583984\n",
      "Epoch: 361: Train loss: 33.55136489868164 Valid loss: 67.26817321777344\n",
      "Epoch: 362: Train loss: 33.47167205810547 Valid loss: 67.05043029785156\n",
      "Epoch: 363: Train loss: 33.39284133911133 Valid loss: 66.83566284179688\n",
      "Epoch: 364: Train loss: 33.31474304199219 Valid loss: 66.62344360351562\n",
      "Epoch: 365: Train loss: 33.23722839355469 Valid loss: 66.41339874267578\n",
      "Epoch: 366: Train loss: 33.160221099853516 Valid loss: 66.20518493652344\n",
      "Epoch: 367: Train loss: 33.0836067199707 Valid loss: 65.99855041503906\n",
      "Epoch: 368: Train loss: 33.00733947753906 Valid loss: 65.79322052001953\n",
      "Epoch: 369: Train loss: 32.931339263916016 Valid loss: 65.58900451660156\n",
      "Epoch: 370: Train loss: 32.8555793762207 Valid loss: 65.3857192993164\n",
      "Epoch: 371: Train loss: 32.78001022338867 Valid loss: 65.18321228027344\n",
      "Epoch: 372: Train loss: 32.70459747314453 Valid loss: 64.98135375976562\n",
      "Epoch: 373: Train loss: 32.629329681396484 Valid loss: 64.78005981445312\n",
      "Epoch: 374: Train loss: 32.55419158935547 Valid loss: 64.5792236328125\n",
      "Epoch: 375: Train loss: 32.47915267944336 Valid loss: 64.3787841796875\n",
      "Epoch: 376: Train loss: 32.40422439575195 Valid loss: 64.17868041992188\n",
      "Epoch: 377: Train loss: 32.329376220703125 Valid loss: 63.97884750366211\n",
      "Epoch: 378: Train loss: 32.254615783691406 Valid loss: 63.77927017211914\n",
      "Epoch: 379: Train loss: 32.179935455322266 Valid loss: 63.57992935180664\n",
      "Epoch: 380: Train loss: 32.105323791503906 Valid loss: 63.380767822265625\n",
      "Epoch: 381: Train loss: 32.03079605102539 Valid loss: 63.181785583496094\n",
      "Epoch: 382: Train loss: 31.95632553100586 Valid loss: 62.982940673828125\n",
      "Epoch: 383: Train loss: 31.881927490234375 Valid loss: 62.784263610839844\n",
      "Epoch: 384: Train loss: 31.80760383605957 Valid loss: 62.58570098876953\n",
      "Epoch: 385: Train loss: 31.733341217041016 Valid loss: 62.38728332519531\n",
      "Epoch: 386: Train loss: 31.659135818481445 Valid loss: 62.18898010253906\n",
      "Epoch: 387: Train loss: 31.585004806518555 Valid loss: 61.99078369140625\n",
      "Epoch: 388: Train loss: 31.510934829711914 Valid loss: 61.79270935058594\n",
      "Epoch: 389: Train loss: 31.43692970275879 Valid loss: 61.594749450683594\n",
      "Epoch: 390: Train loss: 31.36298942565918 Valid loss: 61.39690017700195\n",
      "Epoch: 391: Train loss: 31.28911590576172 Valid loss: 61.19916534423828\n",
      "Epoch: 392: Train loss: 31.215293884277344 Valid loss: 61.00154113769531\n",
      "Epoch: 393: Train loss: 31.141544342041016 Valid loss: 60.80403518676758\n",
      "Epoch: 394: Train loss: 31.06785011291504 Valid loss: 60.60664367675781\n",
      "Epoch: 395: Train loss: 30.99422836303711 Valid loss: 60.409385681152344\n",
      "Epoch: 396: Train loss: 30.92066764831543 Valid loss: 60.21223449707031\n",
      "Epoch: 397: Train loss: 30.84717559814453 Valid loss: 60.01520538330078\n",
      "Epoch: 398: Train loss: 30.773746490478516 Valid loss: 59.81831359863281\n",
      "Epoch: 399: Train loss: 30.70037841796875 Valid loss: 59.621551513671875\n",
      "Epoch: 400: Train loss: 30.62706756591797 Valid loss: 59.4249267578125\n",
      "Epoch: 401: Train loss: 30.553829193115234 Valid loss: 59.228424072265625\n",
      "Epoch: 402: Train loss: 30.48065948486328 Valid loss: 59.03205871582031\n",
      "Epoch: 403: Train loss: 30.40754508972168 Valid loss: 58.835845947265625\n",
      "Epoch: 404: Train loss: 30.33449935913086 Valid loss: 58.63977813720703\n",
      "Epoch: 405: Train loss: 30.261518478393555 Valid loss: 58.44386291503906\n",
      "Epoch: 406: Train loss: 30.18861198425293 Valid loss: 58.24808883666992\n",
      "Epoch: 407: Train loss: 30.11576271057129 Valid loss: 58.05247497558594\n",
      "Epoch: 408: Train loss: 30.04298210144043 Valid loss: 57.85701370239258\n",
      "Epoch: 409: Train loss: 29.97026824951172 Valid loss: 57.661720275878906\n",
      "Epoch: 410: Train loss: 29.89762306213379 Valid loss: 57.466590881347656\n",
      "Epoch: 411: Train loss: 29.825042724609375 Valid loss: 57.27164840698242\n",
      "Epoch: 412: Train loss: 29.752532958984375 Valid loss: 57.07685852050781\n",
      "Epoch: 413: Train loss: 29.68009376525879 Valid loss: 56.88225173950195\n",
      "Epoch: 414: Train loss: 29.607717514038086 Valid loss: 56.68780517578125\n",
      "Epoch: 415: Train loss: 29.53540802001953 Valid loss: 56.49354553222656\n",
      "Epoch: 416: Train loss: 29.46317481994629 Valid loss: 56.29947280883789\n",
      "Epoch: 417: Train loss: 29.39101219177246 Valid loss: 56.10560607910156\n",
      "Epoch: 418: Train loss: 29.31891632080078 Valid loss: 55.911895751953125\n",
      "Epoch: 419: Train loss: 29.246896743774414 Valid loss: 55.718414306640625\n",
      "Epoch: 420: Train loss: 29.174943923950195 Valid loss: 55.52509307861328\n",
      "Epoch: 421: Train loss: 29.103065490722656 Valid loss: 55.33198547363281\n",
      "Epoch: 422: Train loss: 29.03125762939453 Valid loss: 55.13909912109375\n",
      "Epoch: 423: Train loss: 28.95952033996582 Valid loss: 54.946388244628906\n",
      "Epoch: 424: Train loss: 28.887861251831055 Valid loss: 54.75391387939453\n",
      "Epoch: 425: Train loss: 28.81627082824707 Valid loss: 54.561622619628906\n",
      "Epoch: 426: Train loss: 28.74475860595703 Valid loss: 54.36957550048828\n",
      "Epoch: 427: Train loss: 28.673316955566406 Valid loss: 54.177730560302734\n",
      "Epoch: 428: Train loss: 28.60195541381836 Valid loss: 53.986106872558594\n",
      "Epoch: 429: Train loss: 28.53066635131836 Valid loss: 53.794715881347656\n",
      "Epoch: 430: Train loss: 28.459455490112305 Valid loss: 53.603538513183594\n",
      "Epoch: 431: Train loss: 28.38831901550293 Valid loss: 53.41259765625\n",
      "Epoch: 432: Train loss: 28.3172607421875 Valid loss: 53.221885681152344\n",
      "Epoch: 433: Train loss: 28.24628448486328 Valid loss: 53.03141784667969\n",
      "Epoch: 434: Train loss: 28.175376892089844 Valid loss: 52.84117889404297\n",
      "Epoch: 435: Train loss: 28.10456085205078 Valid loss: 52.65118408203125\n",
      "Epoch: 436: Train loss: 28.033815383911133 Valid loss: 52.46142578125\n",
      "Epoch: 437: Train loss: 27.96315574645996 Valid loss: 52.271934509277344\n",
      "Epoch: 438: Train loss: 27.892574310302734 Valid loss: 52.082679748535156\n",
      "Epoch: 439: Train loss: 27.82206916809082 Valid loss: 51.89366912841797\n",
      "Epoch: 440: Train loss: 27.751651763916016 Valid loss: 51.704933166503906\n",
      "Epoch: 441: Train loss: 27.681312561035156 Valid loss: 51.51644515991211\n",
      "Epoch: 442: Train loss: 27.61106300354004 Valid loss: 51.32821273803711\n",
      "Epoch: 443: Train loss: 27.540889739990234 Valid loss: 51.14025115966797\n",
      "Epoch: 444: Train loss: 27.47080421447754 Valid loss: 50.952552795410156\n",
      "Epoch: 445: Train loss: 27.40080451965332 Valid loss: 50.76512908935547\n",
      "Epoch: 446: Train loss: 27.33088493347168 Valid loss: 50.577980041503906\n",
      "Epoch: 447: Train loss: 27.261056900024414 Valid loss: 50.39109420776367\n",
      "Epoch: 448: Train loss: 27.19131088256836 Valid loss: 50.20450210571289\n",
      "Epoch: 449: Train loss: 27.121652603149414 Valid loss: 50.01818084716797\n",
      "Epoch: 450: Train loss: 27.052080154418945 Valid loss: 49.83213806152344\n",
      "Epoch: 451: Train loss: 26.98260498046875 Valid loss: 49.64638900756836\n",
      "Epoch: 452: Train loss: 26.913211822509766 Valid loss: 49.46092224121094\n",
      "Epoch: 453: Train loss: 26.84391212463379 Valid loss: 49.275753021240234\n",
      "Epoch: 454: Train loss: 26.774696350097656 Valid loss: 49.09086608886719\n",
      "Epoch: 455: Train loss: 26.70557403564453 Valid loss: 48.906288146972656\n",
      "Epoch: 456: Train loss: 26.636545181274414 Valid loss: 48.722015380859375\n",
      "Epoch: 457: Train loss: 26.567604064941406 Valid loss: 48.53803253173828\n",
      "Epoch: 458: Train loss: 26.498756408691406 Valid loss: 48.35435104370117\n",
      "Epoch: 459: Train loss: 26.430007934570312 Valid loss: 48.17098617553711\n",
      "Epoch: 460: Train loss: 26.36134910583496 Valid loss: 47.98792266845703\n",
      "Epoch: 461: Train loss: 26.292783737182617 Valid loss: 47.805179595947266\n",
      "Epoch: 462: Train loss: 26.22431182861328 Valid loss: 47.62274169921875\n",
      "Epoch: 463: Train loss: 26.15593719482422 Valid loss: 47.44061279296875\n",
      "Epoch: 464: Train loss: 26.08765983581543 Valid loss: 47.258827209472656\n",
      "Epoch: 465: Train loss: 26.019474029541016 Valid loss: 47.07733917236328\n",
      "Epoch: 466: Train loss: 25.951393127441406 Valid loss: 46.89619445800781\n",
      "Epoch: 467: Train loss: 25.883407592773438 Valid loss: 46.715370178222656\n",
      "Epoch: 468: Train loss: 25.815513610839844 Valid loss: 46.53486633300781\n",
      "Epoch: 469: Train loss: 25.747732162475586 Valid loss: 46.35470962524414\n",
      "Epoch: 470: Train loss: 25.68004035949707 Valid loss: 46.174869537353516\n",
      "Epoch: 471: Train loss: 25.61245346069336 Valid loss: 45.99538803100586\n",
      "Epoch: 472: Train loss: 25.544965744018555 Valid loss: 45.81623077392578\n",
      "Epoch: 473: Train loss: 25.477577209472656 Valid loss: 45.637420654296875\n",
      "Epoch: 474: Train loss: 25.41029930114746 Valid loss: 45.458946228027344\n",
      "Epoch: 475: Train loss: 25.343109130859375 Valid loss: 45.28082275390625\n",
      "Epoch: 476: Train loss: 25.27603530883789 Valid loss: 45.10304641723633\n",
      "Epoch: 477: Train loss: 25.209068298339844 Valid loss: 44.925621032714844\n",
      "Epoch: 478: Train loss: 25.14219093322754 Valid loss: 44.74855041503906\n",
      "Epoch: 479: Train loss: 25.07543182373047 Valid loss: 44.57183837890625\n",
      "Epoch: 480: Train loss: 25.008779525756836 Valid loss: 44.395469665527344\n",
      "Epoch: 481: Train loss: 24.942230224609375 Valid loss: 44.21946716308594\n",
      "Epoch: 482: Train loss: 24.87578582763672 Valid loss: 44.04383850097656\n",
      "Epoch: 483: Train loss: 24.8094482421875 Valid loss: 43.868553161621094\n",
      "Epoch: 484: Train loss: 24.74321746826172 Valid loss: 43.693641662597656\n",
      "Epoch: 485: Train loss: 24.67709732055664 Valid loss: 43.51910400390625\n",
      "Epoch: 486: Train loss: 24.611093521118164 Valid loss: 43.34493637084961\n",
      "Epoch: 487: Train loss: 24.545194625854492 Valid loss: 43.17113494873047\n",
      "Epoch: 488: Train loss: 24.479400634765625 Valid loss: 42.99770736694336\n",
      "Epoch: 489: Train loss: 24.413726806640625 Valid loss: 42.82466125488281\n",
      "Epoch: 490: Train loss: 24.34816551208496 Valid loss: 42.6519889831543\n",
      "Epoch: 491: Train loss: 24.282711029052734 Valid loss: 42.479705810546875\n",
      "Epoch: 492: Train loss: 24.21737289428711 Valid loss: 42.30779266357422\n",
      "Epoch: 493: Train loss: 24.152145385742188 Valid loss: 42.136260986328125\n",
      "Epoch: 494: Train loss: 24.087038040161133 Valid loss: 41.96513366699219\n",
      "Epoch: 495: Train loss: 24.02204132080078 Valid loss: 41.794403076171875\n",
      "Epoch: 496: Train loss: 23.95716094970703 Valid loss: 41.62405014038086\n",
      "Epoch: 497: Train loss: 23.892391204833984 Valid loss: 41.454078674316406\n",
      "Epoch: 498: Train loss: 23.827749252319336 Valid loss: 41.284515380859375\n",
      "Epoch: 499: Train loss: 23.763216018676758 Valid loss: 41.115333557128906\n",
      "Epoch: 500: Train loss: 23.698801040649414 Valid loss: 40.946571350097656\n",
      "Epoch: 501: Train loss: 23.634504318237305 Valid loss: 40.7781867980957\n",
      "Epoch: 502: Train loss: 23.57033348083496 Valid loss: 40.61022186279297\n",
      "Epoch: 503: Train loss: 23.506269454956055 Valid loss: 40.44265365600586\n",
      "Epoch: 504: Train loss: 23.442331314086914 Valid loss: 40.27549743652344\n",
      "Epoch: 505: Train loss: 23.378517150878906 Valid loss: 40.108734130859375\n",
      "Epoch: 506: Train loss: 23.314817428588867 Valid loss: 39.94237518310547\n",
      "Epoch: 507: Train loss: 23.251243591308594 Valid loss: 39.77643966674805\n",
      "Epoch: 508: Train loss: 23.18779182434082 Valid loss: 39.61091613769531\n",
      "Epoch: 509: Train loss: 23.124454498291016 Valid loss: 39.4458122253418\n",
      "Epoch: 510: Train loss: 23.061250686645508 Valid loss: 39.28110885620117\n",
      "Epoch: 511: Train loss: 22.998165130615234 Valid loss: 39.11682891845703\n",
      "Epoch: 512: Train loss: 22.935203552246094 Valid loss: 38.952972412109375\n",
      "Epoch: 513: Train loss: 22.87236785888672 Valid loss: 38.78953552246094\n",
      "Epoch: 514: Train loss: 22.80965805053711 Valid loss: 38.62651443481445\n",
      "Epoch: 515: Train loss: 22.7470703125 Valid loss: 38.46390914916992\n",
      "Epoch: 516: Train loss: 22.684608459472656 Valid loss: 38.30175018310547\n",
      "Epoch: 517: Train loss: 22.622276306152344 Valid loss: 38.139991760253906\n",
      "Epoch: 518: Train loss: 22.56006622314453 Valid loss: 37.978675842285156\n",
      "Epoch: 519: Train loss: 22.49799156188965 Valid loss: 37.817787170410156\n",
      "Epoch: 520: Train loss: 22.436038970947266 Valid loss: 37.6573371887207\n",
      "Epoch: 521: Train loss: 22.37421989440918 Valid loss: 37.497318267822266\n",
      "Epoch: 522: Train loss: 22.312522888183594 Valid loss: 37.33770751953125\n",
      "Epoch: 523: Train loss: 22.250967025756836 Valid loss: 37.17855453491211\n",
      "Epoch: 524: Train loss: 22.18952751159668 Valid loss: 37.01983642578125\n",
      "Epoch: 525: Train loss: 22.12822723388672 Valid loss: 36.86155700683594\n",
      "Epoch: 526: Train loss: 22.067054748535156 Valid loss: 36.7037239074707\n",
      "Epoch: 527: Train loss: 22.006010055541992 Valid loss: 36.54631805419922\n",
      "Epoch: 528: Train loss: 21.945100784301758 Valid loss: 36.38935089111328\n",
      "Epoch: 529: Train loss: 21.884328842163086 Valid loss: 36.232826232910156\n",
      "Epoch: 530: Train loss: 21.823680877685547 Valid loss: 36.076759338378906\n",
      "Epoch: 531: Train loss: 21.763166427612305 Valid loss: 35.92112350463867\n",
      "Epoch: 532: Train loss: 21.702789306640625 Valid loss: 35.76593780517578\n",
      "Epoch: 533: Train loss: 21.642545700073242 Valid loss: 35.6112060546875\n",
      "Epoch: 534: Train loss: 21.582435607910156 Valid loss: 35.456912994384766\n",
      "Epoch: 535: Train loss: 21.52245330810547 Valid loss: 35.303070068359375\n",
      "Epoch: 536: Train loss: 21.462615966796875 Valid loss: 35.14968490600586\n",
      "Epoch: 537: Train loss: 21.402910232543945 Valid loss: 34.99674606323242\n",
      "Epoch: 538: Train loss: 21.343339920043945 Valid loss: 34.844261169433594\n",
      "Epoch: 539: Train loss: 21.283905029296875 Valid loss: 34.692222595214844\n",
      "Epoch: 540: Train loss: 21.224605560302734 Valid loss: 34.540645599365234\n",
      "Epoch: 541: Train loss: 21.165451049804688 Valid loss: 34.3895378112793\n",
      "Epoch: 542: Train loss: 21.106428146362305 Valid loss: 34.23886489868164\n",
      "Epoch: 543: Train loss: 21.047542572021484 Valid loss: 34.088653564453125\n",
      "Epoch: 544: Train loss: 20.98879623413086 Valid loss: 33.93890380859375\n",
      "Epoch: 545: Train loss: 20.93018341064453 Valid loss: 33.78961944580078\n",
      "Epoch: 546: Train loss: 20.871721267700195 Valid loss: 33.640785217285156\n",
      "Epoch: 547: Train loss: 20.81338882446289 Valid loss: 33.49242401123047\n",
      "Epoch: 548: Train loss: 20.75519561767578 Valid loss: 33.344505310058594\n",
      "Epoch: 549: Train loss: 20.697145462036133 Valid loss: 33.19706344604492\n",
      "Epoch: 550: Train loss: 20.639238357543945 Valid loss: 33.05009460449219\n",
      "Epoch: 551: Train loss: 20.581464767456055 Valid loss: 32.903560638427734\n",
      "Epoch: 552: Train loss: 20.523834228515625 Valid loss: 32.75751876831055\n",
      "Epoch: 553: Train loss: 20.46634864807129 Valid loss: 32.61193084716797\n",
      "Epoch: 554: Train loss: 20.40900421142578 Valid loss: 32.46681213378906\n",
      "Epoch: 555: Train loss: 20.351806640625 Valid loss: 32.32215881347656\n",
      "Epoch: 556: Train loss: 20.29474449157715 Valid loss: 32.177978515625\n",
      "Epoch: 557: Train loss: 20.237817764282227 Valid loss: 32.03425598144531\n",
      "Epoch: 558: Train loss: 20.18103790283203 Valid loss: 31.891008377075195\n",
      "Epoch: 559: Train loss: 20.124408721923828 Valid loss: 31.748233795166016\n",
      "Epoch: 560: Train loss: 20.067916870117188 Valid loss: 31.605918884277344\n",
      "Epoch: 561: Train loss: 20.011571884155273 Valid loss: 31.46409034729004\n",
      "Epoch: 562: Train loss: 19.955371856689453 Valid loss: 31.32271957397461\n",
      "Epoch: 563: Train loss: 19.899309158325195 Valid loss: 31.18181037902832\n",
      "Epoch: 564: Train loss: 19.843395233154297 Valid loss: 31.041404724121094\n",
      "Epoch: 565: Train loss: 19.787626266479492 Valid loss: 30.90145492553711\n",
      "Epoch: 566: Train loss: 19.73200225830078 Valid loss: 30.76197052001953\n",
      "Epoch: 567: Train loss: 19.676523208618164 Valid loss: 30.622966766357422\n",
      "Epoch: 568: Train loss: 19.621183395385742 Valid loss: 30.48444175720215\n",
      "Epoch: 569: Train loss: 19.565994262695312 Valid loss: 30.346378326416016\n",
      "Epoch: 570: Train loss: 19.510950088500977 Valid loss: 30.208805084228516\n",
      "Epoch: 571: Train loss: 19.456058502197266 Valid loss: 30.071693420410156\n",
      "Epoch: 572: Train loss: 19.401308059692383 Valid loss: 29.93506622314453\n",
      "Epoch: 573: Train loss: 19.346708297729492 Valid loss: 29.798908233642578\n",
      "Epoch: 574: Train loss: 19.29224395751953 Valid loss: 29.663227081298828\n",
      "Epoch: 575: Train loss: 19.237937927246094 Valid loss: 29.52802276611328\n",
      "Epoch: 576: Train loss: 19.183773040771484 Valid loss: 29.393295288085938\n",
      "Epoch: 577: Train loss: 19.129756927490234 Valid loss: 29.259048461914062\n",
      "Epoch: 578: Train loss: 19.075889587402344 Valid loss: 29.12527084350586\n",
      "Epoch: 579: Train loss: 19.022167205810547 Valid loss: 28.99197006225586\n",
      "Epoch: 580: Train loss: 18.968597412109375 Valid loss: 28.859149932861328\n",
      "Epoch: 581: Train loss: 18.915170669555664 Valid loss: 28.726802825927734\n",
      "Epoch: 582: Train loss: 18.861892700195312 Valid loss: 28.594940185546875\n",
      "Epoch: 583: Train loss: 18.808759689331055 Valid loss: 28.46355628967285\n",
      "Epoch: 584: Train loss: 18.75577735900879 Valid loss: 28.332643508911133\n",
      "Epoch: 585: Train loss: 18.702951431274414 Valid loss: 28.202213287353516\n",
      "Epoch: 586: Train loss: 18.650272369384766 Valid loss: 28.07225799560547\n",
      "Epoch: 587: Train loss: 18.597736358642578 Valid loss: 27.942777633666992\n",
      "Epoch: 588: Train loss: 18.545347213745117 Valid loss: 27.81377601623535\n",
      "Epoch: 589: Train loss: 18.493120193481445 Valid loss: 27.685256958007812\n",
      "Epoch: 590: Train loss: 18.44103240966797 Valid loss: 27.557214736938477\n",
      "Epoch: 591: Train loss: 18.389095306396484 Valid loss: 27.429645538330078\n",
      "Epoch: 592: Train loss: 18.337305068969727 Valid loss: 27.30255889892578\n",
      "Epoch: 593: Train loss: 18.285673141479492 Valid loss: 27.175949096679688\n",
      "Epoch: 594: Train loss: 18.234180450439453 Valid loss: 27.049802780151367\n",
      "Epoch: 595: Train loss: 18.182844161987305 Valid loss: 26.924152374267578\n",
      "Epoch: 596: Train loss: 18.131656646728516 Valid loss: 26.798969268798828\n",
      "Epoch: 597: Train loss: 18.08061408996582 Valid loss: 26.674270629882812\n",
      "Epoch: 598: Train loss: 18.029727935791016 Valid loss: 26.55004119873047\n",
      "Epoch: 599: Train loss: 17.978992462158203 Valid loss: 26.426280975341797\n",
      "Epoch: 600: Train loss: 17.92839813232422 Valid loss: 26.30300521850586\n",
      "Epoch: 601: Train loss: 17.877967834472656 Valid loss: 26.180206298828125\n",
      "Epoch: 602: Train loss: 17.82767677307129 Valid loss: 26.057884216308594\n",
      "Epoch: 603: Train loss: 17.777536392211914 Valid loss: 25.93604278564453\n",
      "Epoch: 604: Train loss: 17.72755241394043 Valid loss: 25.814668655395508\n",
      "Epoch: 605: Train loss: 17.677715301513672 Valid loss: 25.693782806396484\n",
      "Epoch: 606: Train loss: 17.62803077697754 Valid loss: 25.573345184326172\n",
      "Epoch: 607: Train loss: 17.578495025634766 Valid loss: 25.45340347290039\n",
      "Epoch: 608: Train loss: 17.52910804748535 Valid loss: 25.33392333984375\n",
      "Epoch: 609: Train loss: 17.479877471923828 Valid loss: 25.21493148803711\n",
      "Epoch: 610: Train loss: 17.43079376220703 Valid loss: 25.096406936645508\n",
      "Epoch: 611: Train loss: 17.381858825683594 Valid loss: 24.97835350036621\n",
      "Epoch: 612: Train loss: 17.33308219909668 Valid loss: 24.86077308654785\n",
      "Epoch: 613: Train loss: 17.284448623657227 Valid loss: 24.743671417236328\n",
      "Epoch: 614: Train loss: 17.235960006713867 Valid loss: 24.627029418945312\n",
      "Epoch: 615: Train loss: 17.18763542175293 Valid loss: 24.510866165161133\n",
      "Epoch: 616: Train loss: 17.139455795288086 Valid loss: 24.395174026489258\n",
      "Epoch: 617: Train loss: 17.0914249420166 Valid loss: 24.279945373535156\n",
      "Epoch: 618: Train loss: 17.04354476928711 Valid loss: 24.165191650390625\n",
      "Epoch: 619: Train loss: 16.99581527709961 Valid loss: 24.050907135009766\n",
      "Epoch: 620: Train loss: 16.948238372802734 Valid loss: 23.937091827392578\n",
      "Epoch: 621: Train loss: 16.90081024169922 Valid loss: 23.823749542236328\n",
      "Epoch: 622: Train loss: 16.85353660583496 Valid loss: 23.710866928100586\n",
      "Epoch: 623: Train loss: 16.80641746520996 Valid loss: 23.59846305847168\n",
      "Epoch: 624: Train loss: 16.759437561035156 Valid loss: 23.486526489257812\n",
      "Epoch: 625: Train loss: 16.71261215209961 Valid loss: 23.375049591064453\n",
      "Epoch: 626: Train loss: 16.665935516357422 Valid loss: 23.2640323638916\n",
      "Epoch: 627: Train loss: 16.61941146850586 Valid loss: 23.153472900390625\n",
      "Epoch: 628: Train loss: 16.573040008544922 Valid loss: 23.04339599609375\n",
      "Epoch: 629: Train loss: 16.526809692382812 Valid loss: 22.933780670166016\n",
      "Epoch: 630: Train loss: 16.480735778808594 Valid loss: 22.824630737304688\n",
      "Epoch: 631: Train loss: 16.434812545776367 Valid loss: 22.715930938720703\n",
      "Epoch: 632: Train loss: 16.3890380859375 Valid loss: 22.607698440551758\n",
      "Epoch: 633: Train loss: 16.343416213989258 Valid loss: 22.49993324279785\n",
      "Epoch: 634: Train loss: 16.297929763793945 Valid loss: 22.392623901367188\n",
      "Epoch: 635: Train loss: 16.252609252929688 Valid loss: 22.28575897216797\n",
      "Epoch: 636: Train loss: 16.207435607910156 Valid loss: 22.179378509521484\n",
      "Epoch: 637: Train loss: 16.16240692138672 Valid loss: 22.073440551757812\n",
      "Epoch: 638: Train loss: 16.11752700805664 Valid loss: 21.967960357666016\n",
      "Epoch: 639: Train loss: 16.072799682617188 Valid loss: 21.862943649291992\n",
      "Epoch: 640: Train loss: 16.028215408325195 Valid loss: 21.758380889892578\n",
      "Epoch: 641: Train loss: 15.983790397644043 Valid loss: 21.654264450073242\n",
      "Epoch: 642: Train loss: 15.93950366973877 Valid loss: 21.55060577392578\n",
      "Epoch: 643: Train loss: 15.895370483398438 Valid loss: 21.44740867614746\n",
      "Epoch: 644: Train loss: 15.851386070251465 Valid loss: 21.344648361206055\n",
      "Epoch: 645: Train loss: 15.807549476623535 Valid loss: 21.242351531982422\n",
      "Epoch: 646: Train loss: 15.763861656188965 Valid loss: 21.1405029296875\n",
      "Epoch: 647: Train loss: 15.720319747924805 Valid loss: 21.039106369018555\n",
      "Epoch: 648: Train loss: 15.676929473876953 Valid loss: 20.938156127929688\n",
      "Epoch: 649: Train loss: 15.633682250976562 Valid loss: 20.837650299072266\n",
      "Epoch: 650: Train loss: 15.590580940246582 Valid loss: 20.73760986328125\n",
      "Epoch: 651: Train loss: 15.547627449035645 Valid loss: 20.637996673583984\n",
      "Epoch: 652: Train loss: 15.504828453063965 Valid loss: 20.538848876953125\n",
      "Epoch: 653: Train loss: 15.462175369262695 Valid loss: 20.44011878967285\n",
      "Epoch: 654: Train loss: 15.419660568237305 Valid loss: 20.34184455871582\n",
      "Epoch: 655: Train loss: 15.377299308776855 Valid loss: 20.244014739990234\n",
      "Epoch: 656: Train loss: 15.335084915161133 Valid loss: 20.146621704101562\n",
      "Epoch: 657: Train loss: 15.29301643371582 Valid loss: 20.049671173095703\n",
      "Epoch: 658: Train loss: 15.251090049743652 Valid loss: 19.953170776367188\n",
      "Epoch: 659: Train loss: 15.20931339263916 Valid loss: 19.857097625732422\n",
      "Epoch: 660: Train loss: 15.16767692565918 Valid loss: 19.761465072631836\n",
      "Epoch: 661: Train loss: 15.126190185546875 Valid loss: 19.66626739501953\n",
      "Epoch: 662: Train loss: 15.08484935760498 Valid loss: 19.57151222229004\n",
      "Epoch: 663: Train loss: 15.04365062713623 Valid loss: 19.47718620300293\n",
      "Epoch: 664: Train loss: 15.002602577209473 Valid loss: 19.38328742980957\n",
      "Epoch: 665: Train loss: 14.96169376373291 Valid loss: 19.289833068847656\n",
      "Epoch: 666: Train loss: 14.920939445495605 Valid loss: 19.196809768676758\n",
      "Epoch: 667: Train loss: 14.88032054901123 Valid loss: 19.104211807250977\n",
      "Epoch: 668: Train loss: 14.839847564697266 Valid loss: 19.012042999267578\n",
      "Epoch: 669: Train loss: 14.799519538879395 Valid loss: 18.920299530029297\n",
      "Epoch: 670: Train loss: 14.75932788848877 Valid loss: 18.82900047302246\n",
      "Epoch: 671: Train loss: 14.719287872314453 Valid loss: 18.738113403320312\n",
      "Epoch: 672: Train loss: 14.679388999938965 Valid loss: 18.647647857666016\n",
      "Epoch: 673: Train loss: 14.63962459564209 Valid loss: 18.5576171875\n",
      "Epoch: 674: Train loss: 14.600017547607422 Valid loss: 18.468006134033203\n",
      "Epoch: 675: Train loss: 14.560546875 Valid loss: 18.378814697265625\n",
      "Epoch: 676: Train loss: 14.52121639251709 Valid loss: 18.290042877197266\n",
      "Epoch: 677: Train loss: 14.48203182220459 Valid loss: 18.201692581176758\n",
      "Epoch: 678: Train loss: 14.44297981262207 Valid loss: 18.11376190185547\n",
      "Epoch: 679: Train loss: 14.404074668884277 Valid loss: 18.026247024536133\n",
      "Epoch: 680: Train loss: 14.36531925201416 Valid loss: 17.939151763916016\n",
      "Epoch: 681: Train loss: 14.32668685913086 Valid loss: 17.85247039794922\n",
      "Epoch: 682: Train loss: 14.288204193115234 Valid loss: 17.766189575195312\n",
      "Epoch: 683: Train loss: 14.24986743927002 Valid loss: 17.680339813232422\n",
      "Epoch: 684: Train loss: 14.211666107177734 Valid loss: 17.594894409179688\n",
      "Epoch: 685: Train loss: 14.17360782623291 Valid loss: 17.509862899780273\n",
      "Epoch: 686: Train loss: 14.135682106018066 Valid loss: 17.42523193359375\n",
      "Epoch: 687: Train loss: 14.097902297973633 Valid loss: 17.341022491455078\n",
      "Epoch: 688: Train loss: 14.060256958007812 Valid loss: 17.257205963134766\n",
      "Epoch: 689: Train loss: 14.022748947143555 Valid loss: 17.173805236816406\n",
      "Epoch: 690: Train loss: 13.985383987426758 Valid loss: 17.09079933166504\n",
      "Epoch: 691: Train loss: 13.948156356811523 Valid loss: 17.00820541381836\n",
      "Epoch: 692: Train loss: 13.911067962646484 Valid loss: 16.926006317138672\n",
      "Epoch: 693: Train loss: 13.874113082885742 Valid loss: 16.844213485717773\n",
      "Epoch: 694: Train loss: 13.837302207946777 Valid loss: 16.76283073425293\n",
      "Epoch: 695: Train loss: 13.800617218017578 Valid loss: 16.68181610107422\n",
      "Epoch: 696: Train loss: 13.76407527923584 Valid loss: 16.60121726989746\n",
      "Epoch: 697: Train loss: 13.727670669555664 Valid loss: 16.521018981933594\n",
      "Epoch: 698: Train loss: 13.69140338897705 Valid loss: 16.441207885742188\n",
      "Epoch: 699: Train loss: 13.655261993408203 Valid loss: 16.361791610717773\n",
      "Epoch: 700: Train loss: 13.619266510009766 Valid loss: 16.282760620117188\n",
      "Epoch: 701: Train loss: 13.58340072631836 Valid loss: 16.204133987426758\n",
      "Epoch: 702: Train loss: 13.547677993774414 Valid loss: 16.125883102416992\n",
      "Epoch: 703: Train loss: 13.512083053588867 Valid loss: 16.048023223876953\n",
      "Epoch: 704: Train loss: 13.476625442504883 Valid loss: 15.970556259155273\n",
      "Epoch: 705: Train loss: 13.441288948059082 Valid loss: 15.893479347229004\n",
      "Epoch: 706: Train loss: 13.40610408782959 Valid loss: 15.816784858703613\n",
      "Epoch: 707: Train loss: 13.371040344238281 Valid loss: 15.740464210510254\n",
      "Epoch: 708: Train loss: 13.336112022399902 Valid loss: 15.664534568786621\n",
      "Epoch: 709: Train loss: 13.301319122314453 Valid loss: 15.588968276977539\n",
      "Epoch: 710: Train loss: 13.26665210723877 Valid loss: 15.513797760009766\n",
      "Epoch: 711: Train loss: 13.232128143310547 Valid loss: 15.439004898071289\n",
      "Epoch: 712: Train loss: 13.19773006439209 Valid loss: 15.364578247070312\n",
      "Epoch: 713: Train loss: 13.163460731506348 Valid loss: 15.290534973144531\n",
      "Epoch: 714: Train loss: 13.12932300567627 Valid loss: 15.216859817504883\n",
      "Epoch: 715: Train loss: 13.095316886901855 Valid loss: 15.143567085266113\n",
      "Epoch: 716: Train loss: 13.061440467834473 Valid loss: 15.07063102722168\n",
      "Epoch: 717: Train loss: 13.027697563171387 Valid loss: 14.998080253601074\n",
      "Epoch: 718: Train loss: 12.994075775146484 Valid loss: 14.925882339477539\n",
      "Epoch: 719: Train loss: 12.960593223571777 Valid loss: 14.854068756103516\n",
      "Epoch: 720: Train loss: 12.92723560333252 Valid loss: 14.782611846923828\n",
      "Epoch: 721: Train loss: 12.894001960754395 Valid loss: 14.711517333984375\n",
      "Epoch: 722: Train loss: 12.860902786254883 Valid loss: 14.640799522399902\n",
      "Epoch: 723: Train loss: 12.827926635742188 Valid loss: 14.570428848266602\n",
      "Epoch: 724: Train loss: 12.795085906982422 Valid loss: 14.500419616699219\n",
      "Epoch: 725: Train loss: 12.762358665466309 Valid loss: 14.430784225463867\n",
      "Epoch: 726: Train loss: 12.729768753051758 Valid loss: 14.361495971679688\n",
      "Epoch: 727: Train loss: 12.69729995727539 Valid loss: 14.292559623718262\n",
      "Epoch: 728: Train loss: 12.664963722229004 Valid loss: 14.223984718322754\n",
      "Epoch: 729: Train loss: 12.632745742797852 Valid loss: 14.15576171875\n",
      "Epoch: 730: Train loss: 12.600655555725098 Valid loss: 14.087898254394531\n",
      "Epoch: 731: Train loss: 12.568693161010742 Valid loss: 14.02038288116455\n",
      "Epoch: 732: Train loss: 12.536847114562988 Valid loss: 13.95321273803711\n",
      "Epoch: 733: Train loss: 12.505132675170898 Valid loss: 13.886397361755371\n",
      "Epoch: 734: Train loss: 12.47354507446289 Valid loss: 13.819927215576172\n",
      "Epoch: 735: Train loss: 12.442072868347168 Valid loss: 13.753806114196777\n",
      "Epoch: 736: Train loss: 12.41072940826416 Valid loss: 13.688018798828125\n",
      "Epoch: 737: Train loss: 12.379508018493652 Valid loss: 13.622579574584961\n",
      "Epoch: 738: Train loss: 12.34840202331543 Valid loss: 13.557477951049805\n",
      "Epoch: 739: Train loss: 12.31743049621582 Valid loss: 13.492728233337402\n",
      "Epoch: 740: Train loss: 12.286566734313965 Valid loss: 13.428314208984375\n",
      "Epoch: 741: Train loss: 12.255833625793457 Valid loss: 13.364236831665039\n",
      "Epoch: 742: Train loss: 12.225221633911133 Valid loss: 13.300498962402344\n",
      "Epoch: 743: Train loss: 12.194726943969727 Valid loss: 13.237090110778809\n",
      "Epoch: 744: Train loss: 12.164348602294922 Valid loss: 13.174013137817383\n",
      "Epoch: 745: Train loss: 12.13409423828125 Valid loss: 13.11128044128418\n",
      "Epoch: 746: Train loss: 12.103961944580078 Valid loss: 13.04885482788086\n",
      "Epoch: 747: Train loss: 12.073941230773926 Valid loss: 12.986793518066406\n",
      "Epoch: 748: Train loss: 12.044048309326172 Valid loss: 12.92503547668457\n",
      "Epoch: 749: Train loss: 12.014266967773438 Valid loss: 12.863606452941895\n",
      "Epoch: 750: Train loss: 11.984607696533203 Valid loss: 12.802509307861328\n",
      "Epoch: 751: Train loss: 11.95506477355957 Valid loss: 12.74173355102539\n",
      "Epoch: 752: Train loss: 11.925637245178223 Valid loss: 12.681288719177246\n",
      "Epoch: 753: Train loss: 11.896330833435059 Valid loss: 12.621158599853516\n",
      "Epoch: 754: Train loss: 11.867136001586914 Valid loss: 12.561349868774414\n",
      "Epoch: 755: Train loss: 11.838058471679688 Valid loss: 12.50185775756836\n",
      "Epoch: 756: Train loss: 11.809093475341797 Valid loss: 12.44268798828125\n",
      "Epoch: 757: Train loss: 11.780250549316406 Valid loss: 12.383819580078125\n",
      "Epoch: 758: Train loss: 11.751513481140137 Valid loss: 12.325279235839844\n",
      "Epoch: 759: Train loss: 11.722896575927734 Valid loss: 12.267065048217773\n",
      "Epoch: 760: Train loss: 11.694397926330566 Valid loss: 12.209146499633789\n",
      "Epoch: 761: Train loss: 11.666006088256836 Valid loss: 12.151533126831055\n",
      "Epoch: 762: Train loss: 11.63772964477539 Valid loss: 12.09423828125\n",
      "Epoch: 763: Train loss: 11.609564781188965 Valid loss: 12.037250518798828\n",
      "Epoch: 764: Train loss: 11.581513404846191 Valid loss: 11.980568885803223\n",
      "Epoch: 765: Train loss: 11.553576469421387 Valid loss: 11.924186706542969\n",
      "Epoch: 766: Train loss: 11.525745391845703 Valid loss: 11.868112564086914\n",
      "Epoch: 767: Train loss: 11.498029708862305 Valid loss: 11.812348365783691\n",
      "Epoch: 768: Train loss: 11.470426559448242 Valid loss: 11.75688362121582\n",
      "Epoch: 769: Train loss: 11.442933082580566 Valid loss: 11.701717376708984\n",
      "Epoch: 770: Train loss: 11.415553092956543 Valid loss: 11.646833419799805\n",
      "Epoch: 771: Train loss: 11.38827896118164 Valid loss: 11.592275619506836\n",
      "Epoch: 772: Train loss: 11.36111068725586 Valid loss: 11.537985801696777\n",
      "Epoch: 773: Train loss: 11.33405590057373 Valid loss: 11.484006881713867\n",
      "Epoch: 774: Train loss: 11.307110786437988 Valid loss: 11.430325508117676\n",
      "Epoch: 775: Train loss: 11.280271530151367 Valid loss: 11.376927375793457\n",
      "Epoch: 776: Train loss: 11.253538131713867 Valid loss: 11.323819160461426\n",
      "Epoch: 777: Train loss: 11.22691822052002 Valid loss: 11.271013259887695\n",
      "Epoch: 778: Train loss: 11.200398445129395 Valid loss: 11.218482971191406\n",
      "Epoch: 779: Train loss: 11.173994064331055 Valid loss: 11.166248321533203\n",
      "Epoch: 780: Train loss: 11.147689819335938 Valid loss: 11.114285469055176\n",
      "Epoch: 781: Train loss: 11.121492385864258 Valid loss: 11.062615394592285\n",
      "Epoch: 782: Train loss: 11.095399856567383 Valid loss: 11.011222839355469\n",
      "Epoch: 783: Train loss: 11.069413185119629 Valid loss: 10.960121154785156\n",
      "Epoch: 784: Train loss: 11.043533325195312 Valid loss: 10.909292221069336\n",
      "Epoch: 785: Train loss: 11.0177583694458 Valid loss: 10.858743667602539\n",
      "Epoch: 786: Train loss: 10.992082595825195 Valid loss: 10.808473587036133\n",
      "Epoch: 787: Train loss: 10.966513633728027 Valid loss: 10.758476257324219\n",
      "Epoch: 788: Train loss: 10.941046714782715 Valid loss: 10.708765983581543\n",
      "Epoch: 789: Train loss: 10.915679931640625 Valid loss: 10.659311294555664\n",
      "Epoch: 790: Train loss: 10.89041805267334 Valid loss: 10.610135078430176\n",
      "Epoch: 791: Train loss: 10.865262985229492 Valid loss: 10.561233520507812\n",
      "Epoch: 792: Train loss: 10.840203285217285 Valid loss: 10.512602806091309\n",
      "Epoch: 793: Train loss: 10.815248489379883 Valid loss: 10.464241027832031\n",
      "Epoch: 794: Train loss: 10.79039192199707 Valid loss: 10.416141510009766\n",
      "Epoch: 795: Train loss: 10.765636444091797 Valid loss: 10.368308067321777\n",
      "Epoch: 796: Train loss: 10.740982055664062 Valid loss: 10.320746421813965\n",
      "Epoch: 797: Train loss: 10.716426849365234 Valid loss: 10.27343463897705\n",
      "Epoch: 798: Train loss: 10.691971778869629 Valid loss: 10.226394653320312\n",
      "Epoch: 799: Train loss: 10.667619705200195 Valid loss: 10.17961597442627\n",
      "Epoch: 800: Train loss: 10.643356323242188 Valid loss: 10.133097648620605\n",
      "Epoch: 801: Train loss: 10.619196891784668 Valid loss: 10.08683967590332\n",
      "Epoch: 802: Train loss: 10.595134735107422 Valid loss: 10.040830612182617\n",
      "Epoch: 803: Train loss: 10.571170806884766 Valid loss: 9.995079040527344\n",
      "Epoch: 804: Train loss: 10.547307014465332 Valid loss: 9.949584007263184\n",
      "Epoch: 805: Train loss: 10.52353286743164 Valid loss: 9.904335021972656\n",
      "Epoch: 806: Train loss: 10.499857902526855 Valid loss: 9.859339714050293\n",
      "Epoch: 807: Train loss: 10.476278305053711 Valid loss: 9.814616203308105\n",
      "Epoch: 808: Train loss: 10.45279598236084 Valid loss: 9.77011489868164\n",
      "Epoch: 809: Train loss: 10.42940902709961 Valid loss: 9.72587776184082\n",
      "Epoch: 810: Train loss: 10.40611457824707 Valid loss: 9.681880950927734\n",
      "Epoch: 811: Train loss: 10.382914543151855 Valid loss: 9.638132095336914\n",
      "Epoch: 812: Train loss: 10.359808921813965 Valid loss: 9.594618797302246\n",
      "Epoch: 813: Train loss: 10.336801528930664 Valid loss: 9.55136775970459\n",
      "Epoch: 814: Train loss: 10.313881874084473 Valid loss: 9.50835132598877\n",
      "Epoch: 815: Train loss: 10.291057586669922 Valid loss: 9.465560913085938\n",
      "Epoch: 816: Train loss: 10.268325805664062 Valid loss: 9.423027038574219\n",
      "Epoch: 817: Train loss: 10.245683670043945 Valid loss: 9.380730628967285\n",
      "Epoch: 818: Train loss: 10.223135948181152 Valid loss: 9.338661193847656\n",
      "Epoch: 819: Train loss: 10.200678825378418 Valid loss: 9.296833992004395\n",
      "Epoch: 820: Train loss: 10.178309440612793 Valid loss: 9.255242347717285\n",
      "Epoch: 821: Train loss: 10.156038284301758 Valid loss: 9.21387767791748\n",
      "Epoch: 822: Train loss: 10.133852005004883 Valid loss: 9.172754287719727\n",
      "Epoch: 823: Train loss: 10.11175537109375 Valid loss: 9.131856918334961\n",
      "Epoch: 824: Train loss: 10.089750289916992 Valid loss: 9.09118938446045\n",
      "Epoch: 825: Train loss: 10.067832946777344 Valid loss: 9.050759315490723\n",
      "Epoch: 826: Train loss: 10.046004295349121 Valid loss: 9.010550498962402\n",
      "Epoch: 827: Train loss: 10.024266242980957 Valid loss: 8.970569610595703\n",
      "Epoch: 828: Train loss: 10.002613067626953 Valid loss: 8.930809020996094\n",
      "Epoch: 829: Train loss: 9.98105239868164 Valid loss: 8.891263008117676\n",
      "Epoch: 830: Train loss: 9.959574699401855 Valid loss: 8.851972579956055\n",
      "Epoch: 831: Train loss: 9.938180923461914 Valid loss: 8.812871932983398\n",
      "Epoch: 832: Train loss: 9.91688060760498 Valid loss: 8.774002075195312\n",
      "Epoch: 833: Train loss: 9.895665168762207 Valid loss: 8.735355377197266\n",
      "Epoch: 834: Train loss: 9.874529838562012 Valid loss: 8.696919441223145\n",
      "Epoch: 835: Train loss: 9.853490829467773 Valid loss: 8.65870475769043\n",
      "Epoch: 836: Train loss: 9.832526206970215 Valid loss: 8.620711326599121\n",
      "Epoch: 837: Train loss: 9.811653137207031 Valid loss: 8.582929611206055\n",
      "Epoch: 838: Train loss: 9.790858268737793 Valid loss: 8.545368194580078\n",
      "Epoch: 839: Train loss: 9.770147323608398 Valid loss: 8.5079927444458\n",
      "Epoch: 840: Train loss: 9.749526023864746 Valid loss: 8.47085952758789\n",
      "Epoch: 841: Train loss: 9.728989601135254 Valid loss: 8.433915138244629\n",
      "Epoch: 842: Train loss: 9.708525657653809 Valid loss: 8.397189140319824\n",
      "Epoch: 843: Train loss: 9.688156127929688 Valid loss: 8.360666275024414\n",
      "Epoch: 844: Train loss: 9.667863845825195 Valid loss: 8.324357986450195\n",
      "Epoch: 845: Train loss: 9.647655487060547 Valid loss: 8.288257598876953\n",
      "Epoch: 846: Train loss: 9.627522468566895 Valid loss: 8.25235652923584\n",
      "Epoch: 847: Train loss: 9.607474327087402 Valid loss: 8.216652870178223\n",
      "Epoch: 848: Train loss: 9.587505340576172 Valid loss: 8.18117618560791\n",
      "Epoch: 849: Train loss: 9.567621231079102 Valid loss: 8.145865440368652\n",
      "Epoch: 850: Train loss: 9.547812461853027 Valid loss: 8.110771179199219\n",
      "Epoch: 851: Train loss: 9.52808666229248 Valid loss: 8.075872421264648\n",
      "Epoch: 852: Train loss: 9.508440017700195 Valid loss: 8.041187286376953\n",
      "Epoch: 853: Train loss: 9.488869667053223 Valid loss: 8.006685256958008\n",
      "Epoch: 854: Train loss: 9.469379425048828 Valid loss: 7.9723734855651855\n",
      "Epoch: 855: Train loss: 9.449970245361328 Valid loss: 7.938281059265137\n",
      "Epoch: 856: Train loss: 9.430633544921875 Valid loss: 7.904351234436035\n",
      "Epoch: 857: Train loss: 9.411375999450684 Valid loss: 7.870633125305176\n",
      "Epoch: 858: Train loss: 9.392205238342285 Valid loss: 7.837105751037598\n",
      "Epoch: 859: Train loss: 9.373101234436035 Valid loss: 7.803768634796143\n",
      "Epoch: 860: Train loss: 9.354072570800781 Valid loss: 7.77061128616333\n",
      "Epoch: 861: Train loss: 9.335128784179688 Valid loss: 7.737663269042969\n",
      "Epoch: 862: Train loss: 9.316251754760742 Valid loss: 7.70487117767334\n",
      "Epoch: 863: Train loss: 9.297459602355957 Valid loss: 7.672295570373535\n",
      "Epoch: 864: Train loss: 9.278736114501953 Valid loss: 7.639886379241943\n",
      "Epoch: 865: Train loss: 9.260090827941895 Valid loss: 7.607670783996582\n",
      "Epoch: 866: Train loss: 9.241518020629883 Valid loss: 7.5756354331970215\n",
      "Epoch: 867: Train loss: 9.223023414611816 Valid loss: 7.5437846183776855\n",
      "Epoch: 868: Train loss: 9.204596519470215 Valid loss: 7.512109756469727\n",
      "Epoch: 869: Train loss: 9.186246871948242 Valid loss: 7.480617046356201\n",
      "Epoch: 870: Train loss: 9.167967796325684 Valid loss: 7.449307441711426\n",
      "Epoch: 871: Train loss: 9.149762153625488 Valid loss: 7.418186187744141\n",
      "Epoch: 872: Train loss: 9.131633758544922 Valid loss: 7.387208938598633\n",
      "Epoch: 873: Train loss: 9.113570213317871 Valid loss: 7.356437683105469\n",
      "Epoch: 874: Train loss: 9.095585823059082 Valid loss: 7.325818061828613\n",
      "Epoch: 875: Train loss: 9.077672004699707 Valid loss: 7.29539680480957\n",
      "Epoch: 876: Train loss: 9.059829711914062 Valid loss: 7.265138149261475\n",
      "Epoch: 877: Train loss: 9.042057037353516 Valid loss: 7.2350544929504395\n",
      "Epoch: 878: Train loss: 9.02435302734375 Valid loss: 7.205143928527832\n",
      "Epoch: 879: Train loss: 9.006719589233398 Valid loss: 7.175394535064697\n",
      "Epoch: 880: Train loss: 8.98915958404541 Valid loss: 7.145825386047363\n",
      "Epoch: 881: Train loss: 8.971665382385254 Valid loss: 7.1164116859436035\n",
      "Epoch: 882: Train loss: 8.954243659973145 Valid loss: 7.08717679977417\n",
      "Epoch: 883: Train loss: 8.93688678741455 Valid loss: 7.05810546875\n",
      "Epoch: 884: Train loss: 8.919604301452637 Valid loss: 7.029195308685303\n",
      "Epoch: 885: Train loss: 8.902385711669922 Valid loss: 7.000453948974609\n",
      "Epoch: 886: Train loss: 8.885241508483887 Valid loss: 6.971871376037598\n",
      "Epoch: 887: Train loss: 8.868159294128418 Valid loss: 6.943456649780273\n",
      "Epoch: 888: Train loss: 8.85114574432373 Valid loss: 6.915213584899902\n",
      "Epoch: 889: Train loss: 8.834198951721191 Valid loss: 6.887114524841309\n",
      "Epoch: 890: Train loss: 8.817317962646484 Valid loss: 6.8591790199279785\n",
      "Epoch: 891: Train loss: 8.800505638122559 Valid loss: 6.831406593322754\n",
      "Epoch: 892: Train loss: 8.783759117126465 Valid loss: 6.803782939910889\n",
      "Epoch: 893: Train loss: 8.767083168029785 Valid loss: 6.776331901550293\n",
      "Epoch: 894: Train loss: 8.75046157836914 Valid loss: 6.749015808105469\n",
      "Epoch: 895: Train loss: 8.73391342163086 Valid loss: 6.721879005432129\n",
      "Epoch: 896: Train loss: 8.717427253723145 Valid loss: 6.694880962371826\n",
      "Epoch: 897: Train loss: 8.701009750366211 Valid loss: 6.6680402755737305\n",
      "Epoch: 898: Train loss: 8.684654235839844 Valid loss: 6.641355991363525\n",
      "Epoch: 899: Train loss: 8.668367385864258 Valid loss: 6.614815711975098\n",
      "Epoch: 900: Train loss: 8.652140617370605 Valid loss: 6.588435173034668\n",
      "Epoch: 901: Train loss: 8.635978698730469 Valid loss: 6.562196254730225\n",
      "Epoch: 902: Train loss: 8.619871139526367 Valid loss: 6.53610372543335\n",
      "Epoch: 903: Train loss: 8.603837013244629 Valid loss: 6.510171890258789\n",
      "Epoch: 904: Train loss: 8.587857246398926 Valid loss: 6.484372615814209\n",
      "Epoch: 905: Train loss: 8.571949005126953 Valid loss: 6.458741664886475\n",
      "Epoch: 906: Train loss: 8.556096076965332 Valid loss: 6.433223724365234\n",
      "Epoch: 907: Train loss: 8.540308952331543 Valid loss: 6.407886505126953\n",
      "Epoch: 908: Train loss: 8.524578094482422 Valid loss: 6.38266658782959\n",
      "Epoch: 909: Train loss: 8.508913040161133 Valid loss: 6.357600212097168\n",
      "Epoch: 910: Train loss: 8.493308067321777 Valid loss: 6.332675457000732\n",
      "Epoch: 911: Train loss: 8.477761268615723 Valid loss: 6.307889461517334\n",
      "Epoch: 912: Train loss: 8.462276458740234 Valid loss: 6.283250331878662\n",
      "Epoch: 913: Train loss: 8.446853637695312 Valid loss: 6.258735656738281\n",
      "Epoch: 914: Train loss: 8.431488037109375 Valid loss: 6.234377861022949\n",
      "Epoch: 915: Train loss: 8.41618537902832 Valid loss: 6.210150718688965\n",
      "Epoch: 916: Train loss: 8.400941848754883 Valid loss: 6.186053276062012\n",
      "Epoch: 917: Train loss: 8.385748863220215 Valid loss: 6.162103176116943\n",
      "Epoch: 918: Train loss: 8.370622634887695 Valid loss: 6.138288497924805\n",
      "Epoch: 919: Train loss: 8.355551719665527 Valid loss: 6.114594459533691\n",
      "Epoch: 920: Train loss: 8.34054183959961 Valid loss: 6.091052532196045\n",
      "Epoch: 921: Train loss: 8.32558822631836 Valid loss: 6.067624568939209\n",
      "Epoch: 922: Train loss: 8.310687065124512 Valid loss: 6.044345855712891\n",
      "Epoch: 923: Train loss: 8.295848846435547 Valid loss: 6.021193504333496\n",
      "Epoch: 924: Train loss: 8.28106689453125 Valid loss: 5.998164176940918\n",
      "Epoch: 925: Train loss: 8.266340255737305 Valid loss: 5.975277900695801\n",
      "Epoch: 926: Train loss: 8.251667976379395 Valid loss: 5.952507019042969\n",
      "Epoch: 927: Train loss: 8.237055778503418 Valid loss: 5.929877281188965\n",
      "Epoch: 928: Train loss: 8.222496032714844 Valid loss: 5.907368183135986\n",
      "Epoch: 929: Train loss: 8.207993507385254 Valid loss: 5.884980201721191\n",
      "Epoch: 930: Train loss: 8.193547248840332 Valid loss: 5.862735271453857\n",
      "Epoch: 931: Train loss: 8.179160118103027 Valid loss: 5.84060001373291\n",
      "Epoch: 932: Train loss: 8.164825439453125 Valid loss: 5.818600177764893\n",
      "Epoch: 933: Train loss: 8.150537490844727 Valid loss: 5.796719074249268\n",
      "Epoch: 934: Train loss: 8.136316299438477 Valid loss: 5.774959564208984\n",
      "Epoch: 935: Train loss: 8.122142791748047 Valid loss: 5.753317832946777\n",
      "Epoch: 936: Train loss: 8.108020782470703 Valid loss: 5.731815814971924\n",
      "Epoch: 937: Train loss: 8.093952178955078 Valid loss: 5.710411548614502\n",
      "Epoch: 938: Train loss: 8.079941749572754 Valid loss: 5.689139366149902\n",
      "Epoch: 939: Train loss: 8.065977096557617 Valid loss: 5.667981147766113\n",
      "Epoch: 940: Train loss: 8.052074432373047 Valid loss: 5.646949291229248\n",
      "Epoch: 941: Train loss: 8.038225173950195 Valid loss: 5.626029014587402\n",
      "Epoch: 942: Train loss: 8.024415969848633 Valid loss: 5.605233669281006\n",
      "Epoch: 943: Train loss: 8.01066780090332 Valid loss: 5.584542751312256\n",
      "Epoch: 944: Train loss: 7.996967792510986 Valid loss: 5.563981056213379\n",
      "Epoch: 945: Train loss: 7.983320236206055 Valid loss: 5.543524265289307\n",
      "Epoch: 946: Train loss: 7.9697265625 Valid loss: 5.523181915283203\n",
      "Epoch: 947: Train loss: 7.956181049346924 Valid loss: 5.502955436706543\n",
      "Epoch: 948: Train loss: 7.942688941955566 Valid loss: 5.482854843139648\n",
      "Epoch: 949: Train loss: 7.929240703582764 Valid loss: 5.462838172912598\n",
      "Epoch: 950: Train loss: 7.9158453941345215 Valid loss: 5.442957401275635\n",
      "Epoch: 951: Train loss: 7.902504920959473 Valid loss: 5.423185348510742\n",
      "Epoch: 952: Train loss: 7.889212608337402 Valid loss: 5.403511047363281\n",
      "Epoch: 953: Train loss: 7.875964641571045 Valid loss: 5.383947849273682\n",
      "Epoch: 954: Train loss: 7.862772464752197 Valid loss: 5.364508628845215\n",
      "Epoch: 955: Train loss: 7.849628925323486 Valid loss: 5.345154762268066\n",
      "Epoch: 956: Train loss: 7.836530685424805 Valid loss: 5.325935363769531\n",
      "Epoch: 957: Train loss: 7.823485374450684 Valid loss: 5.306793689727783\n",
      "Epoch: 958: Train loss: 7.810484886169434 Valid loss: 5.287777900695801\n",
      "Epoch: 959: Train loss: 7.7975335121154785 Valid loss: 5.268859386444092\n",
      "Epoch: 960: Train loss: 7.784628868103027 Valid loss: 5.250053405761719\n",
      "Epoch: 961: Train loss: 7.7717742919921875 Valid loss: 5.23134183883667\n",
      "Epoch: 962: Train loss: 7.758962154388428 Valid loss: 5.2127366065979\n",
      "Epoch: 963: Train loss: 7.7462005615234375 Valid loss: 5.194242477416992\n",
      "Epoch: 964: Train loss: 7.733491897583008 Valid loss: 5.175841808319092\n",
      "Epoch: 965: Train loss: 7.720820903778076 Valid loss: 5.1575446128845215\n",
      "Epoch: 966: Train loss: 7.708200454711914 Valid loss: 5.139352321624756\n",
      "Epoch: 967: Train loss: 7.69562292098999 Valid loss: 5.121257305145264\n",
      "Epoch: 968: Train loss: 7.683093547821045 Valid loss: 5.10325813293457\n",
      "Epoch: 969: Train loss: 7.670612812042236 Valid loss: 5.085367679595947\n",
      "Epoch: 970: Train loss: 7.658173561096191 Valid loss: 5.067566871643066\n",
      "Epoch: 971: Train loss: 7.645783424377441 Valid loss: 5.049867153167725\n",
      "Epoch: 972: Train loss: 7.63343620300293 Valid loss: 5.032261848449707\n",
      "Epoch: 973: Train loss: 7.621133804321289 Valid loss: 5.01475715637207\n",
      "Epoch: 974: Train loss: 7.608874797821045 Valid loss: 4.997333526611328\n",
      "Epoch: 975: Train loss: 7.596662521362305 Valid loss: 4.9800262451171875\n",
      "Epoch: 976: Train loss: 7.58449125289917 Valid loss: 4.962795257568359\n",
      "Epoch: 977: Train loss: 7.572370529174805 Valid loss: 4.945684432983398\n",
      "Epoch: 978: Train loss: 7.56028938293457 Valid loss: 4.928642749786377\n",
      "Epoch: 979: Train loss: 7.548250675201416 Valid loss: 4.911703586578369\n",
      "Epoch: 980: Train loss: 7.536256790161133 Valid loss: 4.894854545593262\n",
      "Epoch: 981: Train loss: 7.52430534362793 Valid loss: 4.878107070922852\n",
      "Epoch: 982: Train loss: 7.512396812438965 Valid loss: 4.861437797546387\n",
      "Epoch: 983: Train loss: 7.50053596496582 Valid loss: 4.844869136810303\n",
      "Epoch: 984: Train loss: 7.488714218139648 Valid loss: 4.828383922576904\n",
      "Epoch: 985: Train loss: 7.47693395614624 Valid loss: 4.811997413635254\n",
      "Epoch: 986: Train loss: 7.465198993682861 Valid loss: 4.795681953430176\n",
      "Epoch: 987: Train loss: 7.453498840332031 Valid loss: 4.779476642608643\n",
      "Epoch: 988: Train loss: 7.441850662231445 Valid loss: 4.763340950012207\n",
      "Epoch: 989: Train loss: 7.430240154266357 Valid loss: 4.747308254241943\n",
      "Epoch: 990: Train loss: 7.418670654296875 Valid loss: 4.731344699859619\n",
      "Epoch: 991: Train loss: 7.407140731811523 Valid loss: 4.715488433837891\n",
      "Epoch: 992: Train loss: 7.395650386810303 Valid loss: 4.699695587158203\n",
      "Epoch: 993: Train loss: 7.3842010498046875 Valid loss: 4.684008598327637\n",
      "Epoch: 994: Train loss: 7.372798442840576 Valid loss: 4.668388843536377\n",
      "Epoch: 995: Train loss: 7.36142635345459 Valid loss: 4.6528639793396\n",
      "Epoch: 996: Train loss: 7.350103378295898 Valid loss: 4.637422561645508\n",
      "Epoch: 997: Train loss: 7.338818073272705 Valid loss: 4.622069358825684\n",
      "Epoch: 998: Train loss: 7.327572345733643 Valid loss: 4.606775283813477\n",
      "Epoch: 999: Train loss: 7.31636381149292 Valid loss: 4.591590881347656\n",
      "Epoch: 1000: Train loss: 7.305197715759277 Valid loss: 4.576470851898193\n",
      "Epoch: 1001: Train loss: 7.2940673828125 Valid loss: 4.561442852020264\n",
      "Epoch: 1002: Train loss: 7.282979965209961 Valid loss: 4.546478748321533\n",
      "Epoch: 1003: Train loss: 7.271928787231445 Valid loss: 4.531617164611816\n",
      "Epoch: 1004: Train loss: 7.260914325714111 Valid loss: 4.516812324523926\n",
      "Epoch: 1005: Train loss: 7.24993896484375 Valid loss: 4.502098083496094\n",
      "Epoch: 1006: Train loss: 7.239007949829102 Valid loss: 4.487468242645264\n",
      "Epoch: 1007: Train loss: 7.228113174438477 Valid loss: 4.472904205322266\n",
      "Epoch: 1008: Train loss: 7.217253684997559 Valid loss: 4.458425521850586\n",
      "Epoch: 1009: Train loss: 7.206429481506348 Valid loss: 4.444023132324219\n",
      "Epoch: 1010: Train loss: 7.195644855499268 Valid loss: 4.429691791534424\n",
      "Epoch: 1011: Train loss: 7.184900760650635 Valid loss: 4.415437698364258\n",
      "Epoch: 1012: Train loss: 7.1741943359375 Valid loss: 4.401259899139404\n",
      "Epoch: 1013: Train loss: 7.163512229919434 Valid loss: 4.387157917022705\n",
      "Epoch: 1014: Train loss: 7.152885437011719 Valid loss: 4.373131275177002\n",
      "Epoch: 1015: Train loss: 7.142282485961914 Valid loss: 4.359185218811035\n",
      "Epoch: 1016: Train loss: 7.13171911239624 Valid loss: 4.345296859741211\n",
      "Epoch: 1017: Train loss: 7.121191024780273 Valid loss: 4.331488132476807\n",
      "Epoch: 1018: Train loss: 7.110703945159912 Valid loss: 4.3177595138549805\n",
      "Epoch: 1019: Train loss: 7.100248336791992 Valid loss: 4.304099082946777\n",
      "Epoch: 1020: Train loss: 7.089831352233887 Valid loss: 4.290500164031982\n",
      "Epoch: 1021: Train loss: 7.079447269439697 Valid loss: 4.2769904136657715\n",
      "Epoch: 1022: Train loss: 7.069097995758057 Valid loss: 4.26353645324707\n",
      "Epoch: 1023: Train loss: 7.05879020690918 Valid loss: 4.2501606941223145\n",
      "Epoch: 1024: Train loss: 7.04850959777832 Valid loss: 4.236855983734131\n",
      "Epoch: 1025: Train loss: 7.038265705108643 Valid loss: 4.223616600036621\n",
      "Epoch: 1026: Train loss: 7.028055667877197 Valid loss: 4.210443496704102\n",
      "Epoch: 1027: Train loss: 7.01788854598999 Valid loss: 4.197344779968262\n",
      "Epoch: 1028: Train loss: 7.007745742797852 Valid loss: 4.184306621551514\n",
      "Epoch: 1029: Train loss: 6.997642517089844 Valid loss: 4.171342849731445\n",
      "Epoch: 1030: Train loss: 6.987571716308594 Valid loss: 4.158449172973633\n",
      "Epoch: 1031: Train loss: 6.977537155151367 Valid loss: 4.1456193923950195\n",
      "Epoch: 1032: Train loss: 6.967531681060791 Valid loss: 4.132852077484131\n",
      "Epoch: 1033: Train loss: 6.957563400268555 Valid loss: 4.120153427124023\n",
      "Epoch: 1034: Train loss: 6.947624206542969 Valid loss: 4.107516765594482\n",
      "Epoch: 1035: Train loss: 6.937723636627197 Valid loss: 4.094954013824463\n",
      "Epoch: 1036: Train loss: 6.927852630615234 Valid loss: 4.082442283630371\n",
      "Epoch: 1037: Train loss: 6.91801643371582 Valid loss: 4.070003509521484\n",
      "Epoch: 1038: Train loss: 6.908212661743164 Valid loss: 4.057624816894531\n",
      "Epoch: 1039: Train loss: 6.898443222045898 Valid loss: 4.045323371887207\n",
      "Epoch: 1040: Train loss: 6.888702392578125 Valid loss: 4.033061504364014\n",
      "Epoch: 1041: Train loss: 6.878995418548584 Valid loss: 4.0208916664123535\n",
      "Epoch: 1042: Train loss: 6.869322776794434 Valid loss: 4.008755207061768\n",
      "Epoch: 1043: Train loss: 6.859678268432617 Valid loss: 3.9967052936553955\n",
      "Epoch: 1044: Train loss: 6.850069999694824 Valid loss: 3.98469877243042\n",
      "Epoch: 1045: Train loss: 6.840486526489258 Valid loss: 3.972766876220703\n",
      "Epoch: 1046: Train loss: 6.830935001373291 Valid loss: 3.9608774185180664\n",
      "Epoch: 1047: Train loss: 6.8214240074157715 Valid loss: 3.9490573406219482\n",
      "Epoch: 1048: Train loss: 6.811938762664795 Valid loss: 3.937300682067871\n",
      "Epoch: 1049: Train loss: 6.802483558654785 Valid loss: 3.9256019592285156\n",
      "Epoch: 1050: Train loss: 6.793059825897217 Valid loss: 3.9139606952667236\n",
      "Epoch: 1051: Train loss: 6.783666610717773 Valid loss: 3.902381420135498\n",
      "Epoch: 1052: Train loss: 6.774303913116455 Valid loss: 3.8908541202545166\n",
      "Epoch: 1053: Train loss: 6.764973163604736 Valid loss: 3.8793883323669434\n",
      "Epoch: 1054: Train loss: 6.755672931671143 Valid loss: 3.867979049682617\n",
      "Epoch: 1055: Train loss: 6.746399879455566 Valid loss: 3.856630802154541\n",
      "Epoch: 1056: Train loss: 6.73715877532959 Valid loss: 3.8453335762023926\n",
      "Epoch: 1057: Train loss: 6.72794771194458 Valid loss: 3.834106922149658\n",
      "Epoch: 1058: Train loss: 6.718767166137695 Valid loss: 3.8229196071624756\n",
      "Epoch: 1059: Train loss: 6.709615230560303 Valid loss: 3.8117973804473877\n",
      "Epoch: 1060: Train loss: 6.700496196746826 Valid loss: 3.800724983215332\n",
      "Epoch: 1061: Train loss: 6.691398620605469 Valid loss: 3.7897119522094727\n",
      "Epoch: 1062: Train loss: 6.68233585357666 Valid loss: 3.77874755859375\n",
      "Epoch: 1063: Train loss: 6.67330265045166 Valid loss: 3.7678472995758057\n",
      "Epoch: 1064: Train loss: 6.664296627044678 Valid loss: 3.7569851875305176\n",
      "Epoch: 1065: Train loss: 6.65531587600708 Valid loss: 3.7461965084075928\n",
      "Epoch: 1066: Train loss: 6.646366596221924 Valid loss: 3.7354507446289062\n",
      "Epoch: 1067: Train loss: 6.637450218200684 Valid loss: 3.724757432937622\n",
      "Epoch: 1068: Train loss: 6.628559589385986 Valid loss: 3.714111566543579\n",
      "Epoch: 1069: Train loss: 6.619691848754883 Valid loss: 3.7035326957702637\n",
      "Epoch: 1070: Train loss: 6.6108574867248535 Valid loss: 3.69299578666687\n",
      "Epoch: 1071: Train loss: 6.602048873901367 Valid loss: 3.682520627975464\n",
      "Epoch: 1072: Train loss: 6.593270301818848 Valid loss: 3.6720759868621826\n",
      "Epoch: 1073: Train loss: 6.5845184326171875 Valid loss: 3.661703109741211\n",
      "Epoch: 1074: Train loss: 6.575791835784912 Valid loss: 3.6513707637786865\n",
      "Epoch: 1075: Train loss: 6.567094802856445 Valid loss: 3.641099214553833\n",
      "Epoch: 1076: Train loss: 6.5584259033203125 Valid loss: 3.630857229232788\n",
      "Epoch: 1077: Train loss: 6.549783706665039 Valid loss: 3.6206860542297363\n",
      "Epoch: 1078: Train loss: 6.541162490844727 Valid loss: 3.610538959503174\n",
      "Epoch: 1079: Train loss: 6.5325775146484375 Valid loss: 3.6004669666290283\n",
      "Epoch: 1080: Train loss: 6.524019718170166 Valid loss: 3.590423822402954\n",
      "Epoch: 1081: Train loss: 6.515481948852539 Valid loss: 3.5804550647735596\n",
      "Epoch: 1082: Train loss: 6.50697135925293 Valid loss: 3.570504665374756\n",
      "Epoch: 1083: Train loss: 6.498490810394287 Valid loss: 3.5606231689453125\n",
      "Epoch: 1084: Train loss: 6.490030288696289 Valid loss: 3.550764560699463\n",
      "Epoch: 1085: Train loss: 6.481603145599365 Valid loss: 3.540994644165039\n",
      "Epoch: 1086: Train loss: 6.473198413848877 Valid loss: 3.531222105026245\n",
      "Epoch: 1087: Train loss: 6.464819431304932 Valid loss: 3.521542549133301\n",
      "Epoch: 1088: Train loss: 6.456467628479004 Valid loss: 3.5118701457977295\n",
      "Epoch: 1089: Train loss: 6.448139190673828 Valid loss: 3.5022799968719482\n",
      "Epoch: 1090: Train loss: 6.43983793258667 Valid loss: 3.492696762084961\n",
      "Epoch: 1091: Train loss: 6.43156099319458 Valid loss: 3.4832000732421875\n",
      "Epoch: 1092: Train loss: 6.423311233520508 Valid loss: 3.473710060119629\n",
      "Epoch: 1093: Train loss: 6.41508674621582 Valid loss: 3.4642908573150635\n",
      "Epoch: 1094: Train loss: 6.406886577606201 Valid loss: 3.4549031257629395\n",
      "Epoch: 1095: Train loss: 6.398706436157227 Valid loss: 3.445566177368164\n",
      "Epoch: 1096: Train loss: 6.390561580657959 Valid loss: 3.4362597465515137\n",
      "Epoch: 1097: Train loss: 6.382431983947754 Valid loss: 3.4270236492156982\n",
      "Epoch: 1098: Train loss: 6.374325275421143 Valid loss: 3.417797803878784\n",
      "Epoch: 1099: Train loss: 6.366254806518555 Valid loss: 3.408646821975708\n",
      "Epoch: 1100: Train loss: 6.3581976890563965 Valid loss: 3.399505853652954\n",
      "Epoch: 1101: Train loss: 6.350172996520996 Valid loss: 3.390449285507202\n",
      "Epoch: 1102: Train loss: 6.342165946960449 Valid loss: 3.381392002105713\n",
      "Epoch: 1103: Train loss: 6.334183692932129 Valid loss: 3.3724136352539062\n",
      "Epoch: 1104: Train loss: 6.326228618621826 Valid loss: 3.3634445667266846\n",
      "Epoch: 1105: Train loss: 6.318297386169434 Valid loss: 3.354548931121826\n",
      "Epoch: 1106: Train loss: 6.310388565063477 Valid loss: 3.3456573486328125\n",
      "Epoch: 1107: Train loss: 6.302504539489746 Valid loss: 3.336843490600586\n",
      "Epoch: 1108: Train loss: 6.294641017913818 Valid loss: 3.328033447265625\n",
      "Epoch: 1109: Train loss: 6.286802291870117 Valid loss: 3.319300651550293\n",
      "Epoch: 1110: Train loss: 6.278985023498535 Valid loss: 3.3105714321136475\n",
      "Epoch: 1111: Train loss: 6.27119255065918 Valid loss: 3.3019237518310547\n",
      "Epoch: 1112: Train loss: 6.263423442840576 Valid loss: 3.293264627456665\n",
      "Epoch: 1113: Train loss: 6.255678653717041 Valid loss: 3.2847111225128174\n",
      "Epoch: 1114: Train loss: 6.247950077056885 Valid loss: 3.2761266231536865\n",
      "Epoch: 1115: Train loss: 6.240250587463379 Valid loss: 3.267637252807617\n",
      "Epoch: 1116: Train loss: 6.232576370239258 Valid loss: 3.259140729904175\n",
      "Epoch: 1117: Train loss: 6.224916458129883 Valid loss: 3.2507247924804688\n",
      "Epoch: 1118: Train loss: 6.217283248901367 Valid loss: 3.2423107624053955\n",
      "Epoch: 1119: Train loss: 6.2096710205078125 Valid loss: 3.233962297439575\n",
      "Epoch: 1120: Train loss: 6.202081203460693 Valid loss: 3.225630283355713\n",
      "Epoch: 1121: Train loss: 6.194514751434326 Valid loss: 3.217358350753784\n",
      "Epoch: 1122: Train loss: 6.186971187591553 Valid loss: 3.2090976238250732\n",
      "Epoch: 1123: Train loss: 6.17944860458374 Valid loss: 3.200901508331299\n",
      "Epoch: 1124: Train loss: 6.171945571899414 Valid loss: 3.192707061767578\n",
      "Epoch: 1125: Train loss: 6.164465427398682 Valid loss: 3.1845908164978027\n",
      "Epoch: 1126: Train loss: 6.157008647918701 Valid loss: 3.17647123336792\n",
      "Epoch: 1127: Train loss: 6.149575233459473 Valid loss: 3.168424606323242\n",
      "Epoch: 1128: Train loss: 6.142159461975098 Valid loss: 3.160369634628296\n",
      "Epoch: 1129: Train loss: 6.13476037979126 Valid loss: 3.152406930923462\n",
      "Epoch: 1130: Train loss: 6.127390384674072 Valid loss: 3.1444156169891357\n",
      "Epoch: 1131: Train loss: 6.120039939880371 Valid loss: 3.1365160942077637\n",
      "Epoch: 1132: Train loss: 6.112706661224365 Valid loss: 3.128607749938965\n",
      "Epoch: 1133: Train loss: 6.105400085449219 Valid loss: 3.1207759380340576\n",
      "Epoch: 1134: Train loss: 6.098111629486084 Valid loss: 3.1129395961761475\n",
      "Epoch: 1135: Train loss: 6.090841293334961 Valid loss: 3.105165481567383\n",
      "Epoch: 1136: Train loss: 6.083594799041748 Valid loss: 3.097391128540039\n",
      "Epoch: 1137: Train loss: 6.076365947723389 Valid loss: 3.089707136154175\n",
      "Epoch: 1138: Train loss: 6.069155693054199 Valid loss: 3.0819849967956543\n",
      "Epoch: 1139: Train loss: 6.06197452545166 Valid loss: 3.074376344680786\n",
      "Epoch: 1140: Train loss: 6.054807662963867 Valid loss: 3.0667150020599365\n",
      "Epoch: 1141: Train loss: 6.047662734985352 Valid loss: 3.059171438217163\n",
      "Epoch: 1142: Train loss: 6.040534496307373 Valid loss: 3.0515706539154053\n",
      "Epoch: 1143: Train loss: 6.03342866897583 Valid loss: 3.0441105365753174\n",
      "Epoch: 1144: Train loss: 6.026340007781982 Valid loss: 3.036564826965332\n",
      "Epoch: 1145: Train loss: 6.019275188446045 Valid loss: 3.0291640758514404\n",
      "Epoch: 1146: Train loss: 6.012228012084961 Valid loss: 3.0216727256774902\n",
      "Epoch: 1147: Train loss: 6.005203723907471 Valid loss: 3.0143587589263916\n",
      "Epoch: 1148: Train loss: 5.998195171356201 Valid loss: 3.0069167613983154\n",
      "Epoch: 1149: Train loss: 5.991206645965576 Valid loss: 2.9996705055236816\n",
      "Epoch: 1150: Train loss: 5.9842376708984375 Valid loss: 2.992286443710327\n",
      "Epoch: 1151: Train loss: 5.977287292480469 Valid loss: 2.985121250152588\n",
      "Epoch: 1152: Train loss: 5.970357418060303 Valid loss: 2.977776050567627\n",
      "Epoch: 1153: Train loss: 5.963445663452148 Valid loss: 2.970676898956299\n",
      "Epoch: 1154: Train loss: 5.95655632019043 Valid loss: 2.9633843898773193\n",
      "Epoch: 1155: Train loss: 5.949682712554932 Valid loss: 2.9563651084899902\n",
      "Epoch: 1156: Train loss: 5.9428277015686035 Valid loss: 2.949124336242676\n",
      "Epoch: 1157: Train loss: 5.9359917640686035 Valid loss: 2.9421794414520264\n",
      "Epoch: 1158: Train loss: 5.929172992706299 Valid loss: 2.934966802597046\n",
      "Epoch: 1159: Train loss: 5.9223761558532715 Valid loss: 2.928105354309082\n",
      "Epoch: 1160: Train loss: 5.915594577789307 Valid loss: 2.9209251403808594\n",
      "Epoch: 1161: Train loss: 5.9088335037231445 Valid loss: 2.9141554832458496\n",
      "Epoch: 1162: Train loss: 5.902089595794678 Valid loss: 2.9069981575012207\n",
      "Epoch: 1163: Train loss: 5.895363807678223 Valid loss: 2.9003288745880127\n",
      "Epoch: 1164: Train loss: 5.888660907745361 Valid loss: 2.8931844234466553\n",
      "Epoch: 1165: Train loss: 5.8819708824157715 Valid loss: 2.886605739593506\n",
      "Epoch: 1166: Train loss: 5.875297546386719 Valid loss: 2.879488229751587\n",
      "Epoch: 1167: Train loss: 5.868648052215576 Valid loss: 2.873004198074341\n",
      "Epoch: 1168: Train loss: 5.862011909484863 Valid loss: 2.8658852577209473\n",
      "Epoch: 1169: Train loss: 5.855393409729004 Valid loss: 2.859522581100464\n",
      "Epoch: 1170: Train loss: 5.848791122436523 Valid loss: 2.8523976802825928\n",
      "Epoch: 1171: Train loss: 5.8422112464904785 Valid loss: 2.846151113510132\n",
      "Epoch: 1172: Train loss: 5.8356475830078125 Valid loss: 2.83901047706604\n",
      "Epoch: 1173: Train loss: 5.829099655151367 Valid loss: 2.832902431488037\n",
      "Epoch: 1174: Train loss: 5.822571754455566 Valid loss: 2.825714111328125\n",
      "Epoch: 1175: Train loss: 5.8160576820373535 Valid loss: 2.8197662830352783\n",
      "Epoch: 1176: Train loss: 5.809564590454102 Valid loss: 2.812520742416382\n",
      "Epoch: 1177: Train loss: 5.8030853271484375 Valid loss: 2.806751251220703\n",
      "Epoch: 1178: Train loss: 5.796622276306152 Valid loss: 2.799412250518799\n",
      "Epoch: 1179: Train loss: 5.790180683135986 Valid loss: 2.793851137161255\n",
      "Epoch: 1180: Train loss: 5.783755779266357 Valid loss: 2.78640079498291\n",
      "Epoch: 1181: Train loss: 5.777345180511475 Valid loss: 2.7810702323913574\n",
      "Epoch: 1182: Train loss: 5.770951271057129 Valid loss: 2.7734720706939697\n",
      "Epoch: 1183: Train loss: 5.764575481414795 Valid loss: 2.768420696258545\n",
      "Epoch: 1184: Train loss: 5.758214473724365 Valid loss: 2.7606122493743896\n",
      "Epoch: 1185: Train loss: 5.751869201660156 Valid loss: 2.755892753601074\n",
      "Epoch: 1186: Train loss: 5.745542526245117 Valid loss: 2.7478244304656982\n",
      "Epoch: 1187: Train loss: 5.739230632781982 Valid loss: 2.7434897422790527\n",
      "Epoch: 1188: Train loss: 5.732941150665283 Valid loss: 2.7351086139678955\n",
      "Epoch: 1189: Train loss: 5.726661682128906 Valid loss: 2.731229066848755\n",
      "Epoch: 1190: Train loss: 5.72039794921875 Valid loss: 2.7224414348602295\n",
      "Epoch: 1191: Train loss: 5.714156150817871 Valid loss: 2.7191097736358643\n",
      "Epoch: 1192: Train loss: 5.707928657531738 Valid loss: 2.709813117980957\n",
      "Epoch: 1193: Train loss: 5.701711654663086 Valid loss: 2.7071521282196045\n",
      "Epoch: 1194: Train loss: 5.6955132484436035 Valid loss: 2.697218894958496\n",
      "Epoch: 1195: Train loss: 5.689334392547607 Valid loss: 2.695374011993408\n",
      "Epoch: 1196: Train loss: 5.683164119720459 Valid loss: 2.6846232414245605\n",
      "Epoch: 1197: Train loss: 5.677018165588379 Valid loss: 2.68377423286438\n",
      "Epoch: 1198: Train loss: 5.670881271362305 Valid loss: 2.672011613845825\n",
      "Epoch: 1199: Train loss: 5.664764404296875 Valid loss: 2.6724040508270264\n",
      "Epoch: 1200: Train loss: 5.658663749694824 Valid loss: 2.659348964691162\n",
      "Epoch: 1201: Train loss: 5.652575969696045 Valid loss: 2.6612765789031982\n",
      "Epoch: 1202: Train loss: 5.646501541137695 Valid loss: 2.6465954780578613\n",
      "Epoch: 1203: Train loss: 5.64044713973999 Valid loss: 2.650442600250244\n",
      "Epoch: 1204: Train loss: 5.63440465927124 Valid loss: 2.6337201595306396\n",
      "Epoch: 1205: Train loss: 5.628379821777344 Valid loss: 2.6399497985839844\n",
      "Epoch: 1206: Train loss: 5.622373104095459 Valid loss: 2.6206448078155518\n",
      "Epoch: 1207: Train loss: 5.616378307342529 Valid loss: 2.6298491954803467\n",
      "Epoch: 1208: Train loss: 5.610398292541504 Valid loss: 2.6073036193847656\n",
      "Epoch: 1209: Train loss: 5.60443639755249 Valid loss: 2.620257616043091\n",
      "Epoch: 1210: Train loss: 5.598485946655273 Valid loss: 2.5935893058776855\n",
      "Epoch: 1211: Train loss: 5.592556476593018 Valid loss: 2.6112611293792725\n",
      "Epoch: 1212: Train loss: 5.586641788482666 Valid loss: 2.579385280609131\n",
      "Epoch: 1213: Train loss: 5.580741882324219 Valid loss: 2.603015661239624\n",
      "Epoch: 1214: Train loss: 5.574859619140625 Valid loss: 2.5645203590393066\n",
      "Epoch: 1215: Train loss: 5.568994998931885 Valid loss: 2.595702648162842\n",
      "Epoch: 1216: Train loss: 5.563150882720947 Valid loss: 2.548790693283081\n",
      "Epoch: 1217: Train loss: 5.557322978973389 Valid loss: 2.589569091796875\n",
      "Epoch: 1218: Train loss: 5.551516056060791 Valid loss: 2.5319204330444336\n",
      "Epoch: 1219: Train loss: 5.545731544494629 Valid loss: 2.584937334060669\n",
      "Epoch: 1220: Train loss: 5.539971351623535 Valid loss: 2.513561248779297\n",
      "Epoch: 1221: Train loss: 5.534239768981934 Valid loss: 2.582212209701538\n",
      "Epoch: 1222: Train loss: 5.528534412384033 Valid loss: 2.4932701587677\n",
      "Epoch: 1223: Train loss: 5.52286434173584 Valid loss: 2.581941843032837\n",
      "Epoch: 1224: Train loss: 5.517240524291992 Valid loss: 2.470451831817627\n",
      "Epoch: 1225: Train loss: 5.511661052703857 Valid loss: 2.5848567485809326\n",
      "Epoch: 1226: Train loss: 5.506139278411865 Valid loss: 2.4443533420562744\n",
      "Epoch: 1227: Train loss: 5.500688552856445 Valid loss: 2.591907262802124\n",
      "Epoch: 1228: Train loss: 5.49533224105835 Valid loss: 2.414001941680908\n",
      "Epoch: 1229: Train loss: 5.490076065063477 Valid loss: 2.6043622493743896\n",
      "Epoch: 1230: Train loss: 5.484968662261963 Valid loss: 2.378131866455078\n",
      "Epoch: 1231: Train loss: 5.480021953582764 Valid loss: 2.6239047050476074\n",
      "Epoch: 1232: Train loss: 5.475318908691406 Valid loss: 2.33512806892395\n",
      "Epoch: 1233: Train loss: 5.470864295959473 Valid loss: 2.6528332233428955\n",
      "Epoch: 1234: Train loss: 5.466832637786865 Valid loss: 2.282937526702881\n",
      "Epoch: 1235: Train loss: 5.463197708129883 Valid loss: 2.6942412853240967\n",
      "Epoch: 1236: Train loss: 5.460282802581787 Valid loss: 2.218947172164917\n",
      "Epoch: 1237: Train loss: 5.458004474639893 Valid loss: 2.7523767948150635\n",
      "Epoch: 1238: Train loss: 5.45699405670166 Valid loss: 2.1399412155151367\n",
      "Epoch: 1239: Train loss: 5.456981182098389 Valid loss: 2.8330888748168945\n",
      "Epoch: 1240: Train loss: 5.459234714508057 Valid loss: 2.042004108428955\n",
      "Epoch: 1241: Train loss: 5.463021755218506 Valid loss: 2.9445290565490723\n",
      "Epoch: 1242: Train loss: 5.470908164978027 Valid loss: 1.920624852180481\n",
      "Epoch: 1243: Train loss: 5.481028079986572 Valid loss: 3.0981428623199463\n",
      "Epoch: 1244: Train loss: 5.498675346374512 Valid loss: 1.7709600925445557\n",
      "Epoch: 1245: Train loss: 5.519273281097412 Valid loss: 3.3100781440734863\n",
      "Epoch: 1246: Train loss: 5.553834915161133 Valid loss: 1.5886256694793701\n",
      "Epoch: 1247: Train loss: 5.591442584991455 Valid loss: 3.6030755043029785\n",
      "Epoch: 1248: Train loss: 5.655162811279297 Valid loss: 1.3712189197540283\n",
      "Epoch: 1249: Train loss: 5.71937370300293 Valid loss: 4.008569717407227\n",
      "Epoch: 1250: Train loss: 5.832518577575684 Valid loss: 1.1211251020431519\n",
      "Epoch: 1251: Train loss: 5.935684680938721 Valid loss: 4.567897796630859\n",
      "Epoch: 1252: Train loss: 6.129754543304443 Valid loss: 0.8495619297027588\n",
      "Epoch: 1253: Train loss: 6.282863616943359 Valid loss: 5.328866481781006\n",
      "Epoch: 1254: Train loss: 6.600855350494385 Valid loss: 0.5805472135543823\n",
      "Epoch: 1255: Train loss: 6.800889492034912 Valid loss: 6.329605579376221\n",
      "Epoch: 1256: Train loss: 7.286722660064697 Valid loss: 0.3502962291240692\n",
      "Epoch: 1257: Train loss: 7.492085933685303 Valid loss: 7.557858943939209\n",
      "Epoch: 1258: Train loss: 8.15876579284668 Valid loss: 0.19472993910312653\n",
      "Epoch: 1259: Train loss: 8.26755142211914 Valid loss: 8.888569831848145\n",
      "Epoch: 1260: Train loss: 9.051605224609375 Valid loss: 0.12523376941680908\n",
      "Epoch: 1261: Train loss: 8.930830001831055 Valid loss: 10.057336807250977\n",
      "Epoch: 1262: Train loss: 9.68667984008789 Valid loss: 0.11573848873376846\n",
      "Epoch: 1263: Train loss: 9.27633285522461 Valid loss: 10.765186309814453\n",
      "Epoch: 1264: Train loss: 9.862435340881348 Valid loss: 0.12750911712646484\n",
      "Epoch: 1265: Train loss: 9.24773120880127 Valid loss: 10.884503364562988\n",
      "Epoch: 1266: Train loss: 9.630922317504883 Valid loss: 0.14352378249168396\n",
      "Epoch: 1267: Train loss: 8.970503807067871 Valid loss: 10.550653457641602\n",
      "Epoch: 1268: Train loss: 9.219855308532715 Valid loss: 0.1637236475944519\n",
      "Epoch: 1269: Train loss: 8.62779712677002 Valid loss: 10.033068656921387\n",
      "Epoch: 1270: Train loss: 8.829625129699707 Valid loss: 0.18437504768371582\n",
      "Epoch: 1271: Train loss: 8.341699600219727 Valid loss: 9.550677299499512\n",
      "Epoch: 1272: Train loss: 8.549211502075195 Valid loss: 0.19815024733543396\n",
      "Epoch: 1273: Train loss: 8.153288841247559 Valid loss: 9.20367431640625\n",
      "Epoch: 1274: Train loss: 8.387508392333984 Valid loss: 0.20172715187072754\n",
      "Epoch: 1275: Train loss: 8.056102752685547 Valid loss: 9.004558563232422\n",
      "Epoch: 1276: Train loss: 8.320444107055664 Valid loss: 0.19671007990837097\n",
      "Epoch: 1277: Train loss: 8.026429176330566 Valid loss: 8.92500114440918\n",
      "Epoch: 1278: Train loss: 8.316311836242676 Valid loss: 0.1868625283241272\n",
      "Epoch: 1279: Train loss: 8.037860870361328 Valid loss: 8.924585342407227\n",
      "Epoch: 1280: Train loss: 8.345232009887695 Valid loss: 0.17581439018249512\n",
      "Epoch: 1281: Train loss: 8.066953659057617 Valid loss: 8.963934898376465\n",
      "Epoch: 1282: Train loss: 8.382731437683105 Valid loss: 0.16601578891277313\n",
      "Epoch: 1283: Train loss: 8.095688819885254 Valid loss: 9.01110553741455\n",
      "Epoch: 1284: Train loss: 8.411589622497559 Valid loss: 0.15861713886260986\n",
      "Epoch: 1285: Train loss: 8.112735748291016 Valid loss: 9.044429779052734\n",
      "Epoch: 1286: Train loss: 8.422710418701172 Valid loss: 0.15376335382461548\n",
      "Epoch: 1287: Train loss: 8.113533020019531 Valid loss: 9.053393363952637\n",
      "Epoch: 1288: Train loss: 8.414454460144043 Valid loss: 0.15099649131298065\n",
      "Epoch: 1289: Train loss: 8.099053382873535 Valid loss: 9.037126541137695\n",
      "Epoch: 1290: Train loss: 8.390460968017578 Valid loss: 0.1496167778968811\n",
      "Epoch: 1291: Train loss: 8.0736083984375 Valid loss: 9.001145362854004\n",
      "Epoch: 1292: Train loss: 8.356863021850586 Valid loss: 0.14891521632671356\n",
      "Epoch: 1293: Train loss: 8.042557716369629 Valid loss: 8.953719139099121\n",
      "Epoch: 1294: Train loss: 8.319795608520508 Valid loss: 0.14831297099590302\n",
      "Epoch: 1295: Train loss: 8.010570526123047 Valid loss: 8.9027738571167\n",
      "Epoch: 1296: Train loss: 8.283895492553711 Valid loss: 0.14744096994400024\n",
      "Epoch: 1297: Train loss: 7.980795860290527 Valid loss: 8.854134559631348\n",
      "Epoch: 1298: Train loss: 8.251794815063477 Valid loss: 0.14613483846187592\n",
      "Epoch: 1299: Train loss: 7.954721927642822 Valid loss: 8.810999870300293\n",
      "Epoch: 1300: Train loss: 8.224332809448242 Valid loss: 0.1444016396999359\n",
      "Epoch: 1301: Train loss: 7.932494163513184 Valid loss: 8.774214744567871\n",
      "Epoch: 1302: Train loss: 8.201067924499512 Valid loss: 0.14235107600688934\n",
      "Epoch: 1303: Train loss: 7.9134368896484375 Valid loss: 8.743060111999512\n",
      "Epoch: 1304: Train loss: 8.18090534210205 Valid loss: 0.14012929797172546\n",
      "Epoch: 1305: Train loss: 7.896509170532227 Valid loss: 8.715996742248535\n",
      "Epoch: 1306: Train loss: 8.162569046020508 Valid loss: 0.13787081837654114\n",
      "Epoch: 1307: Train loss: 7.880664348602295 Valid loss: 8.691283226013184\n",
      "Epoch: 1308: Train loss: 8.14496898651123 Valid loss: 0.13567622005939484\n",
      "Epoch: 1309: Train loss: 7.865110397338867 Valid loss: 8.66751480102539\n",
      "Epoch: 1310: Train loss: 8.127358436584473 Valid loss: 0.1335974633693695\n",
      "Epoch: 1311: Train loss: 7.849355220794678 Valid loss: 8.64373779296875\n",
      "Epoch: 1312: Train loss: 8.10934066772461 Valid loss: 0.13164937496185303\n",
      "Epoch: 1313: Train loss: 7.833197593688965 Valid loss: 8.619463920593262\n",
      "Epoch: 1314: Train loss: 8.09084415435791 Valid loss: 0.12981688976287842\n",
      "Epoch: 1315: Train loss: 7.816660404205322 Valid loss: 8.594623565673828\n",
      "Epoch: 1316: Train loss: 8.07198715209961 Valid loss: 0.12807607650756836\n",
      "Epoch: 1317: Train loss: 7.799894332885742 Valid loss: 8.569395065307617\n",
      "Epoch: 1318: Train loss: 8.052984237670898 Valid loss: 0.1263921558856964\n",
      "Epoch: 1319: Train loss: 7.78309440612793 Valid loss: 8.544072151184082\n",
      "Epoch: 1320: Train loss: 8.034067153930664 Valid loss: 0.12473981082439423\n",
      "Epoch: 1321: Train loss: 7.7664313316345215 Valid loss: 8.51895809173584\n",
      "Epoch: 1322: Train loss: 8.015397071838379 Valid loss: 0.12310301512479782\n",
      "Epoch: 1323: Train loss: 7.750040054321289 Valid loss: 8.494282722473145\n",
      "Epoch: 1324: Train loss: 7.997092247009277 Valid loss: 0.12147151678800583\n",
      "Epoch: 1325: Train loss: 7.733975887298584 Valid loss: 8.470158576965332\n",
      "Epoch: 1326: Train loss: 7.979192733764648 Valid loss: 0.1198444664478302\n",
      "Epoch: 1327: Train loss: 7.718249797821045 Valid loss: 8.44664478302002\n",
      "Epoch: 1328: Train loss: 7.961679935455322 Valid loss: 0.11822296679019928\n",
      "Epoch: 1329: Train loss: 7.70283317565918 Valid loss: 8.423691749572754\n",
      "Epoch: 1330: Train loss: 7.9445013999938965 Valid loss: 0.11661605536937714\n",
      "Epoch: 1331: Train loss: 7.687676429748535 Valid loss: 8.401233673095703\n",
      "Epoch: 1332: Train loss: 7.927601337432861 Valid loss: 0.11502488702535629\n",
      "Epoch: 1333: Train loss: 7.672731399536133 Valid loss: 8.379176139831543\n",
      "Epoch: 1334: Train loss: 7.910922527313232 Valid loss: 0.11345609277486801\n",
      "Epoch: 1335: Train loss: 7.657958984375 Valid loss: 8.357455253601074\n",
      "Epoch: 1336: Train loss: 7.894428730010986 Valid loss: 0.11190879344940186\n",
      "Epoch: 1337: Train loss: 7.643332481384277 Valid loss: 8.336016654968262\n",
      "Epoch: 1338: Train loss: 7.878090858459473 Valid loss: 0.11038772761821747\n",
      "Epoch: 1339: Train loss: 7.628824234008789 Valid loss: 8.314814567565918\n",
      "Epoch: 1340: Train loss: 7.861906051635742 Valid loss: 0.10888826102018356\n",
      "Epoch: 1341: Train loss: 7.614444255828857 Valid loss: 8.293851852416992\n",
      "Epoch: 1342: Train loss: 7.845870018005371 Valid loss: 0.10741057991981506\n",
      "Epoch: 1343: Train loss: 7.600189208984375 Valid loss: 8.273117065429688\n",
      "Epoch: 1344: Train loss: 7.829975605010986 Valid loss: 0.10595265030860901\n",
      "Epoch: 1345: Train loss: 7.586049556732178 Valid loss: 8.252591133117676\n",
      "Epoch: 1346: Train loss: 7.814229488372803 Valid loss: 0.10451427847146988\n",
      "Epoch: 1347: Train loss: 7.572043418884277 Valid loss: 8.232309341430664\n",
      "Epoch: 1348: Train loss: 7.798643112182617 Valid loss: 0.10309157520532608\n",
      "Epoch: 1349: Train loss: 7.558164119720459 Valid loss: 8.212258338928223\n",
      "Epoch: 1350: Train loss: 7.783201217651367 Valid loss: 0.10168910771608353\n",
      "Epoch: 1351: Train loss: 7.54440975189209 Valid loss: 8.192422866821289\n",
      "Epoch: 1352: Train loss: 7.767908573150635 Valid loss: 0.10030079632997513\n",
      "Epoch: 1353: Train loss: 7.53078031539917 Valid loss: 8.172816276550293\n",
      "Epoch: 1354: Train loss: 7.75276517868042 Valid loss: 0.09893065690994263\n",
      "Epoch: 1355: Train loss: 7.517269611358643 Valid loss: 8.153423309326172\n",
      "Epoch: 1356: Train loss: 7.737757205963135 Valid loss: 0.09757541865110397\n",
      "Epoch: 1357: Train loss: 7.503864765167236 Valid loss: 8.134230613708496\n",
      "Epoch: 1358: Train loss: 7.722880840301514 Valid loss: 0.09623610973358154\n",
      "Epoch: 1359: Train loss: 7.490578651428223 Valid loss: 8.115225791931152\n",
      "Epoch: 1360: Train loss: 7.708125591278076 Valid loss: 0.09491420537233353\n",
      "Epoch: 1361: Train loss: 7.477388858795166 Valid loss: 8.096421241760254\n",
      "Epoch: 1362: Train loss: 7.693498611450195 Valid loss: 0.093608058989048\n",
      "Epoch: 1363: Train loss: 7.464297771453857 Valid loss: 8.077767372131348\n",
      "Epoch: 1364: Train loss: 7.678987503051758 Valid loss: 0.09231694042682648\n",
      "Epoch: 1365: Train loss: 7.45131254196167 Valid loss: 8.059320449829102\n",
      "Epoch: 1366: Train loss: 7.664595603942871 Valid loss: 0.09104045480489731\n",
      "Epoch: 1367: Train loss: 7.438425540924072 Valid loss: 8.041031837463379\n",
      "Epoch: 1368: Train loss: 7.65031623840332 Valid loss: 0.08977984637022018\n",
      "Epoch: 1369: Train loss: 7.425620079040527 Valid loss: 8.022894859313965\n",
      "Epoch: 1370: Train loss: 7.636137962341309 Valid loss: 0.08853611350059509\n",
      "Epoch: 1371: Train loss: 7.412906169891357 Valid loss: 8.004918098449707\n",
      "Epoch: 1372: Train loss: 7.622064590454102 Valid loss: 0.08730517327785492\n",
      "Epoch: 1373: Train loss: 7.400282859802246 Valid loss: 7.987086296081543\n",
      "Epoch: 1374: Train loss: 7.608102798461914 Valid loss: 0.08608835190534592\n",
      "Epoch: 1375: Train loss: 7.387747764587402 Valid loss: 7.969428062438965\n",
      "Epoch: 1376: Train loss: 7.594244956970215 Valid loss: 0.08488531410694122\n",
      "Epoch: 1377: Train loss: 7.3752899169921875 Valid loss: 7.951890468597412\n",
      "Epoch: 1378: Train loss: 7.580474853515625 Valid loss: 0.08369790762662888\n",
      "Epoch: 1379: Train loss: 7.362918376922607 Valid loss: 7.9344964027404785\n",
      "Epoch: 1380: Train loss: 7.566804885864258 Valid loss: 0.0825219377875328\n",
      "Epoch: 1381: Train loss: 7.350620746612549 Valid loss: 7.917240142822266\n",
      "Epoch: 1382: Train loss: 7.553225040435791 Valid loss: 0.08136218041181564\n",
      "Epoch: 1383: Train loss: 7.338403701782227 Valid loss: 7.900111198425293\n",
      "Epoch: 1384: Train loss: 7.539745807647705 Valid loss: 0.08021379262208939\n",
      "Epoch: 1385: Train loss: 7.326265335083008 Valid loss: 7.8831257820129395\n",
      "Epoch: 1386: Train loss: 7.526350021362305 Valid loss: 0.07908061891794205\n",
      "Epoch: 1387: Train loss: 7.31419563293457 Valid loss: 7.866260528564453\n",
      "Epoch: 1388: Train loss: 7.51303768157959 Valid loss: 0.07795821875333786\n",
      "Epoch: 1389: Train loss: 7.302206993103027 Valid loss: 7.849523544311523\n",
      "Epoch: 1390: Train loss: 7.499823570251465 Valid loss: 0.07684852182865143\n",
      "Epoch: 1391: Train loss: 7.290289878845215 Valid loss: 7.832921504974365\n",
      "Epoch: 1392: Train loss: 7.486684799194336 Valid loss: 0.0757523775100708\n",
      "Epoch: 1393: Train loss: 7.278432369232178 Valid loss: 7.816401958465576\n",
      "Epoch: 1394: Train loss: 7.47361946105957 Valid loss: 0.07466983050107956\n",
      "Epoch: 1395: Train loss: 7.266636371612549 Valid loss: 7.800002098083496\n",
      "Epoch: 1396: Train loss: 7.460634708404541 Valid loss: 0.07360032200813293\n",
      "Epoch: 1397: Train loss: 7.254903793334961 Valid loss: 7.783694267272949\n",
      "Epoch: 1398: Train loss: 7.447723388671875 Valid loss: 0.07254257053136826\n",
      "Epoch: 1399: Train loss: 7.24323844909668 Valid loss: 7.767496585845947\n",
      "Epoch: 1400: Train loss: 7.43488883972168 Valid loss: 0.07149669528007507\n",
      "Epoch: 1401: Train loss: 7.2316436767578125 Valid loss: 7.751419544219971\n",
      "Epoch: 1402: Train loss: 7.422140121459961 Valid loss: 0.07046118378639221\n",
      "Epoch: 1403: Train loss: 7.2201080322265625 Valid loss: 7.7354559898376465\n",
      "Epoch: 1404: Train loss: 7.409464359283447 Valid loss: 0.06943873316049576\n",
      "Epoch: 1405: Train loss: 7.208640098571777 Valid loss: 7.719581604003906\n",
      "Epoch: 1406: Train loss: 7.396853923797607 Valid loss: 0.06842765212059021\n",
      "Epoch: 1407: Train loss: 7.197225093841553 Valid loss: 7.7038044929504395\n",
      "Epoch: 1408: Train loss: 7.384317398071289 Valid loss: 0.06742843240499496\n",
      "Epoch: 1409: Train loss: 7.185866355895996 Valid loss: 7.688116073608398\n",
      "Epoch: 1410: Train loss: 7.3718438148498535 Valid loss: 0.06644006818532944\n",
      "Epoch: 1411: Train loss: 7.174567222595215 Valid loss: 7.672539710998535\n",
      "Epoch: 1412: Train loss: 7.359439373016357 Valid loss: 0.06546313315629959\n",
      "Epoch: 1413: Train loss: 7.16332483291626 Valid loss: 7.657032012939453\n",
      "Epoch: 1414: Train loss: 7.347109317779541 Valid loss: 0.06449690461158752\n",
      "Epoch: 1415: Train loss: 7.152137756347656 Valid loss: 7.641626358032227\n",
      "Epoch: 1416: Train loss: 7.334835529327393 Valid loss: 0.06354229152202606\n",
      "Epoch: 1417: Train loss: 7.140994548797607 Valid loss: 7.626288890838623\n",
      "Epoch: 1418: Train loss: 7.322619915008545 Valid loss: 0.06259974837303162\n",
      "Epoch: 1419: Train loss: 7.129910945892334 Valid loss: 7.611054420471191\n",
      "Epoch: 1420: Train loss: 7.3104705810546875 Valid loss: 0.06166597455739975\n",
      "Epoch: 1421: Train loss: 7.118884563446045 Valid loss: 7.595901966094971\n",
      "Epoch: 1422: Train loss: 7.2983880043029785 Valid loss: 0.060743339359760284\n",
      "Epoch: 1423: Train loss: 7.107905864715576 Valid loss: 7.580825328826904\n",
      "Epoch: 1424: Train loss: 7.286365032196045 Valid loss: 0.05983053520321846\n",
      "Epoch: 1425: Train loss: 7.096979141235352 Valid loss: 7.565845012664795\n",
      "Epoch: 1426: Train loss: 7.274406433105469 Valid loss: 0.05892913043498993\n",
      "Epoch: 1427: Train loss: 7.08610725402832 Valid loss: 7.550938129425049\n",
      "Epoch: 1428: Train loss: 7.262505531311035 Valid loss: 0.05803663283586502\n",
      "Epoch: 1429: Train loss: 7.075283050537109 Valid loss: 7.536111831665039\n",
      "Epoch: 1430: Train loss: 7.2506585121154785 Valid loss: 0.05715453624725342\n",
      "Epoch: 1431: Train loss: 7.064499378204346 Valid loss: 7.52133846282959\n",
      "Epoch: 1432: Train loss: 7.238862991333008 Valid loss: 0.05628397688269615\n",
      "Epoch: 1433: Train loss: 7.053765296936035 Valid loss: 7.5066609382629395\n",
      "Epoch: 1434: Train loss: 7.227132320404053 Valid loss: 0.05542182922363281\n",
      "Epoch: 1435: Train loss: 7.043079376220703 Valid loss: 7.492051124572754\n",
      "Epoch: 1436: Train loss: 7.215450763702393 Valid loss: 0.054569143801927567\n",
      "Epoch: 1437: Train loss: 7.032432556152344 Valid loss: 7.4775071144104\n",
      "Epoch: 1438: Train loss: 7.203821182250977 Valid loss: 0.053728289902210236\n",
      "Epoch: 1439: Train loss: 7.0218377113342285 Valid loss: 7.463029861450195\n",
      "Epoch: 1440: Train loss: 7.19224214553833 Valid loss: 0.05289595201611519\n",
      "Epoch: 1441: Train loss: 7.011284828186035 Valid loss: 7.448640823364258\n",
      "Epoch: 1442: Train loss: 7.180720806121826 Valid loss: 0.052072685211896896\n",
      "Epoch: 1443: Train loss: 7.000777244567871 Valid loss: 7.434303283691406\n",
      "Epoch: 1444: Train loss: 7.169248580932617 Valid loss: 0.05125894397497177\n",
      "Epoch: 1445: Train loss: 6.990305423736572 Valid loss: 7.420032978057861\n",
      "Epoch: 1446: Train loss: 7.1578240394592285 Valid loss: 0.05045454204082489\n",
      "Epoch: 1447: Train loss: 6.97988224029541 Valid loss: 7.405813217163086\n",
      "Epoch: 1448: Train loss: 7.146448612213135 Valid loss: 0.04965948686003685\n",
      "Epoch: 1449: Train loss: 6.969498634338379 Valid loss: 7.3916754722595215\n",
      "Epoch: 1450: Train loss: 7.135126113891602 Valid loss: 0.04887303709983826\n",
      "Epoch: 1451: Train loss: 6.959156513214111 Valid loss: 7.377597332000732\n",
      "Epoch: 1452: Train loss: 7.123851299285889 Valid loss: 0.048096176236867905\n",
      "Epoch: 1453: Train loss: 6.948850631713867 Valid loss: 7.363570213317871\n",
      "Epoch: 1454: Train loss: 7.112616539001465 Valid loss: 0.04732880741357803\n",
      "Epoch: 1455: Train loss: 6.938587665557861 Valid loss: 7.349608421325684\n",
      "Epoch: 1456: Train loss: 7.101436138153076 Valid loss: 0.04656911641359329\n",
      "Epoch: 1457: Train loss: 6.928369998931885 Valid loss: 7.335707187652588\n",
      "Epoch: 1458: Train loss: 7.090311050415039 Valid loss: 0.04581749811768532\n",
      "Epoch: 1459: Train loss: 6.918193817138672 Valid loss: 7.321893692016602\n",
      "Epoch: 1460: Train loss: 7.079224586486816 Valid loss: 0.04507577046751976\n",
      "Epoch: 1461: Train loss: 6.908046722412109 Valid loss: 7.308109283447266\n",
      "Epoch: 1462: Train loss: 7.068180084228516 Valid loss: 0.04434194415807724\n",
      "Epoch: 1463: Train loss: 6.897937297821045 Valid loss: 7.294376850128174\n",
      "Epoch: 1464: Train loss: 7.05717658996582 Valid loss: 0.04361763596534729\n",
      "Epoch: 1465: Train loss: 6.887868404388428 Valid loss: 7.280703067779541\n",
      "Epoch: 1466: Train loss: 7.046217918395996 Valid loss: 0.04290040582418442\n",
      "Epoch: 1467: Train loss: 6.877830982208252 Valid loss: 7.267080783843994\n",
      "Epoch: 1468: Train loss: 7.035301685333252 Valid loss: 0.04219311475753784\n",
      "Epoch: 1469: Train loss: 6.867831707000732 Valid loss: 7.253510475158691\n",
      "Epoch: 1470: Train loss: 7.0244293212890625 Valid loss: 0.04149334132671356\n",
      "Epoch: 1471: Train loss: 6.85786771774292 Valid loss: 7.240013122558594\n",
      "Epoch: 1472: Train loss: 7.0135979652404785 Valid loss: 0.04080105200409889\n",
      "Epoch: 1473: Train loss: 6.847943305969238 Valid loss: 7.226552963256836\n",
      "Epoch: 1474: Train loss: 7.002805233001709 Valid loss: 0.040117740631103516\n",
      "Epoch: 1475: Train loss: 6.838048934936523 Valid loss: 7.213137626647949\n",
      "Epoch: 1476: Train loss: 6.992057800292969 Valid loss: 0.039441756904125214\n",
      "Epoch: 1477: Train loss: 6.82819128036499 Valid loss: 7.199774265289307\n",
      "Epoch: 1478: Train loss: 6.981355667114258 Valid loss: 0.03877466171979904\n",
      "Epoch: 1479: Train loss: 6.818368911743164 Valid loss: 7.186476230621338\n",
      "Epoch: 1480: Train loss: 6.9706854820251465 Valid loss: 0.03811457008123398\n",
      "Epoch: 1481: Train loss: 6.808577537536621 Valid loss: 7.173208236694336\n",
      "Epoch: 1482: Train loss: 6.960050582885742 Valid loss: 0.037462055683135986\n",
      "Epoch: 1483: Train loss: 6.798816204071045 Valid loss: 7.159989833831787\n",
      "Epoch: 1484: Train loss: 6.949456691741943 Valid loss: 0.03681812062859535\n",
      "Epoch: 1485: Train loss: 6.789089679718018 Valid loss: 7.146810531616211\n",
      "Epoch: 1486: Train loss: 6.938900470733643 Valid loss: 0.036181092262268066\n",
      "Epoch: 1487: Train loss: 6.779390335083008 Valid loss: 7.133689880371094\n",
      "Epoch: 1488: Train loss: 6.928380012512207 Valid loss: 0.03555234894156456\n",
      "Epoch: 1489: Train loss: 6.769723892211914 Valid loss: 7.120599269866943\n",
      "Epoch: 1490: Train loss: 6.91789436340332 Valid loss: 0.03493139520287514\n",
      "Epoch: 1491: Train loss: 6.760092258453369 Valid loss: 7.1075663566589355\n",
      "Epoch: 1492: Train loss: 6.907449722290039 Valid loss: 0.03431718051433563\n",
      "Epoch: 1493: Train loss: 6.75049352645874 Valid loss: 7.094572067260742\n",
      "Epoch: 1494: Train loss: 6.8970465660095215 Valid loss: 0.033709559589624405\n",
      "Epoch: 1495: Train loss: 6.740925312042236 Valid loss: 7.081636905670166\n",
      "Epoch: 1496: Train loss: 6.886678218841553 Valid loss: 0.03310989588499069\n",
      "Epoch: 1497: Train loss: 6.731386184692383 Valid loss: 7.06873083114624\n",
      "Epoch: 1498: Train loss: 6.876343250274658 Valid loss: 0.032517336308956146\n",
      "Epoch: 1499: Train loss: 6.7218804359436035 Valid loss: 7.055868625640869\n",
      "Epoch: 1500: Train loss: 6.866038799285889 Valid loss: 0.03193257749080658\n",
      "Epoch: 1501: Train loss: 6.712393760681152 Valid loss: 7.043043613433838\n",
      "Epoch: 1502: Train loss: 6.855763912200928 Valid loss: 0.031355150043964386\n",
      "Epoch: 1503: Train loss: 6.702939987182617 Valid loss: 7.030248641967773\n",
      "Epoch: 1504: Train loss: 6.845526695251465 Valid loss: 0.03078490123152733\n",
      "Epoch: 1505: Train loss: 6.693511962890625 Valid loss: 7.017512321472168\n",
      "Epoch: 1506: Train loss: 6.835323333740234 Valid loss: 0.030220458284020424\n",
      "Epoch: 1507: Train loss: 6.684114456176758 Valid loss: 7.004806995391846\n",
      "Epoch: 1508: Train loss: 6.825150489807129 Valid loss: 0.029663562774658203\n",
      "Epoch: 1509: Train loss: 6.674745559692383 Valid loss: 6.992130756378174\n",
      "Epoch: 1510: Train loss: 6.815012454986572 Valid loss: 0.029114628210663795\n",
      "Epoch: 1511: Train loss: 6.665404319763184 Valid loss: 6.9794921875\n",
      "Epoch: 1512: Train loss: 6.804903030395508 Valid loss: 0.028571778908371925\n",
      "Epoch: 1513: Train loss: 6.656086444854736 Valid loss: 6.966898441314697\n",
      "Epoch: 1514: Train loss: 6.794832229614258 Valid loss: 0.028034858405590057\n",
      "Epoch: 1515: Train loss: 6.6468024253845215 Valid loss: 6.954341888427734\n",
      "Epoch: 1516: Train loss: 6.784789562225342 Valid loss: 0.027504879981279373\n",
      "Epoch: 1517: Train loss: 6.637547016143799 Valid loss: 6.941829681396484\n",
      "Epoch: 1518: Train loss: 6.774786949157715 Valid loss: 0.026981238275766373\n",
      "Epoch: 1519: Train loss: 6.628323078155518 Valid loss: 6.9293622970581055\n",
      "Epoch: 1520: Train loss: 6.76481819152832 Valid loss: 0.026464320719242096\n",
      "Epoch: 1521: Train loss: 6.61912727355957 Valid loss: 6.91692590713501\n",
      "Epoch: 1522: Train loss: 6.7548828125 Valid loss: 0.025953641161322594\n",
      "Epoch: 1523: Train loss: 6.609951496124268 Valid loss: 6.904531478881836\n",
      "Epoch: 1524: Train loss: 6.744967937469482 Valid loss: 0.025450890883803368\n",
      "Epoch: 1525: Train loss: 6.600798606872559 Valid loss: 6.892154216766357\n",
      "Epoch: 1526: Train loss: 6.735072612762451 Valid loss: 0.024953817948698997\n",
      "Epoch: 1527: Train loss: 6.591665744781494 Valid loss: 6.879806995391846\n",
      "Epoch: 1528: Train loss: 6.725212097167969 Valid loss: 0.024463606998324394\n",
      "Epoch: 1529: Train loss: 6.58256196975708 Valid loss: 6.867491245269775\n",
      "Epoch: 1530: Train loss: 6.715385913848877 Valid loss: 0.023979444056749344\n",
      "Epoch: 1531: Train loss: 6.573489665985107 Valid loss: 6.855219841003418\n",
      "Epoch: 1532: Train loss: 6.70558500289917 Valid loss: 0.02350081317126751\n",
      "Epoch: 1533: Train loss: 6.564435958862305 Valid loss: 6.842978000640869\n",
      "Epoch: 1534: Train loss: 6.695817947387695 Valid loss: 0.02302938885986805\n",
      "Epoch: 1535: Train loss: 6.555405616760254 Valid loss: 6.830766677856445\n",
      "Epoch: 1536: Train loss: 6.686071395874023 Valid loss: 0.022564232349395752\n",
      "Epoch: 1537: Train loss: 6.54640007019043 Valid loss: 6.818586349487305\n",
      "Epoch: 1538: Train loss: 6.676348686218262 Valid loss: 0.022104844450950623\n",
      "Epoch: 1539: Train loss: 6.537418842315674 Valid loss: 6.806429862976074\n",
      "Epoch: 1540: Train loss: 6.666666030883789 Valid loss: 0.021652091294527054\n",
      "Epoch: 1541: Train loss: 6.528458118438721 Valid loss: 6.794309616088867\n",
      "Epoch: 1542: Train loss: 6.657003402709961 Valid loss: 0.021204590797424316\n",
      "Epoch: 1543: Train loss: 6.519526958465576 Valid loss: 6.78222131729126\n",
      "Epoch: 1544: Train loss: 6.647369384765625 Valid loss: 0.020763210952281952\n",
      "Epoch: 1545: Train loss: 6.510614395141602 Valid loss: 6.770155906677246\n",
      "Epoch: 1546: Train loss: 6.637760639190674 Valid loss: 0.020328620448708534\n",
      "Epoch: 1547: Train loss: 6.501725196838379 Valid loss: 6.758126735687256\n",
      "Epoch: 1548: Train loss: 6.628183841705322 Valid loss: 0.01989922858774662\n",
      "Epoch: 1549: Train loss: 6.4928669929504395 Valid loss: 6.746130466461182\n",
      "Epoch: 1550: Train loss: 6.618631362915039 Valid loss: 0.019475029781460762\n",
      "Epoch: 1551: Train loss: 6.48402738571167 Valid loss: 6.734172344207764\n",
      "Epoch: 1552: Train loss: 6.609113693237305 Valid loss: 0.019057072699069977\n",
      "Epoch: 1553: Train loss: 6.475214004516602 Valid loss: 6.722229957580566\n",
      "Epoch: 1554: Train loss: 6.599617004394531 Valid loss: 0.018644999712705612\n",
      "Epoch: 1555: Train loss: 6.466419219970703 Valid loss: 6.710323810577393\n",
      "Epoch: 1556: Train loss: 6.590142250061035 Valid loss: 0.018238691613078117\n",
      "Epoch: 1557: Train loss: 6.457643508911133 Valid loss: 6.698435306549072\n",
      "Epoch: 1558: Train loss: 6.580690383911133 Valid loss: 0.017838522791862488\n",
      "Epoch: 1559: Train loss: 6.44888973236084 Valid loss: 6.686578750610352\n",
      "Epoch: 1560: Train loss: 6.571269512176514 Valid loss: 0.017443343997001648\n",
      "Epoch: 1561: Train loss: 6.440158843994141 Valid loss: 6.674752235412598\n",
      "Epoch: 1562: Train loss: 6.5618720054626465 Valid loss: 0.017054183408617973\n",
      "Epoch: 1563: Train loss: 6.431454658508301 Valid loss: 6.662956237792969\n",
      "Epoch: 1564: Train loss: 6.552495956420898 Valid loss: 0.016670307144522667\n",
      "Epoch: 1565: Train loss: 6.422763347625732 Valid loss: 6.651176452636719\n",
      "Epoch: 1566: Train loss: 6.54314661026001 Valid loss: 0.016292301937937737\n",
      "Epoch: 1567: Train loss: 6.414093017578125 Valid loss: 6.639420986175537\n",
      "Epoch: 1568: Train loss: 6.533814907073975 Valid loss: 0.015919538214802742\n",
      "Epoch: 1569: Train loss: 6.405444145202637 Valid loss: 6.62769079208374\n",
      "Epoch: 1570: Train loss: 6.524513244628906 Valid loss: 0.015552612952888012\n",
      "Epoch: 1571: Train loss: 6.396819114685059 Valid loss: 6.615983963012695\n",
      "Epoch: 1572: Train loss: 6.515232563018799 Valid loss: 0.01519080251455307\n",
      "Epoch: 1573: Train loss: 6.388210773468018 Valid loss: 6.604307651519775\n",
      "Epoch: 1574: Train loss: 6.5059814453125 Valid loss: 0.014834074303507805\n",
      "Epoch: 1575: Train loss: 6.379629135131836 Valid loss: 6.592663764953613\n",
      "Epoch: 1576: Train loss: 6.49675178527832 Valid loss: 0.014483394101262093\n",
      "Epoch: 1577: Train loss: 6.371062278747559 Valid loss: 6.58103609085083\n",
      "Epoch: 1578: Train loss: 6.487541675567627 Valid loss: 0.014137420803308487\n",
      "Epoch: 1579: Train loss: 6.362520694732666 Valid loss: 6.569437503814697\n",
      "Epoch: 1580: Train loss: 6.478360652923584 Valid loss: 0.01379675418138504\n",
      "Epoch: 1581: Train loss: 6.353995323181152 Valid loss: 6.557858467102051\n",
      "Epoch: 1582: Train loss: 6.469198226928711 Valid loss: 0.01346136350184679\n",
      "Epoch: 1583: Train loss: 6.345487594604492 Valid loss: 6.5463032722473145\n",
      "Epoch: 1584: Train loss: 6.460053443908691 Valid loss: 0.013131514191627502\n",
      "Epoch: 1585: Train loss: 6.337003707885742 Valid loss: 6.534771919250488\n",
      "Epoch: 1586: Train loss: 6.450940132141113 Valid loss: 0.012806257233023643\n",
      "Epoch: 1587: Train loss: 6.3285346031188965 Valid loss: 6.523265361785889\n",
      "Epoch: 1588: Train loss: 6.441839694976807 Valid loss: 0.012486758641898632\n",
      "Epoch: 1589: Train loss: 6.320090293884277 Valid loss: 6.511775493621826\n",
      "Epoch: 1590: Train loss: 6.432771682739258 Valid loss: 0.012171519920229912\n",
      "Epoch: 1591: Train loss: 6.311660289764404 Valid loss: 6.500317573547363\n",
      "Epoch: 1592: Train loss: 6.423714637756348 Valid loss: 0.011861976236104965\n",
      "Epoch: 1593: Train loss: 6.303246021270752 Valid loss: 6.488869667053223\n",
      "Epoch: 1594: Train loss: 6.414682865142822 Valid loss: 0.011557738296687603\n",
      "Epoch: 1595: Train loss: 6.294854640960693 Valid loss: 6.47744607925415\n",
      "Epoch: 1596: Train loss: 6.405674457550049 Valid loss: 0.011258254759013653\n",
      "Epoch: 1597: Train loss: 6.286479949951172 Valid loss: 6.4660539627075195\n",
      "Epoch: 1598: Train loss: 6.3966875076293945 Valid loss: 0.010963503271341324\n",
      "Epoch: 1599: Train loss: 6.278126239776611 Valid loss: 6.454679012298584\n",
      "Epoch: 1600: Train loss: 6.387719631195068 Valid loss: 0.010673699900507927\n",
      "Epoch: 1601: Train loss: 6.269789695739746 Valid loss: 6.4433159828186035\n",
      "Epoch: 1602: Train loss: 6.3787713050842285 Valid loss: 0.010389085859060287\n",
      "Epoch: 1603: Train loss: 6.261469841003418 Valid loss: 6.431990146636963\n",
      "Epoch: 1604: Train loss: 6.3698530197143555 Valid loss: 0.01010883692651987\n",
      "Epoch: 1605: Train loss: 6.253176212310791 Valid loss: 6.420682907104492\n",
      "Epoch: 1606: Train loss: 6.360954761505127 Valid loss: 0.009833737276494503\n",
      "Epoch: 1607: Train loss: 6.244893550872803 Valid loss: 6.409399032592773\n",
      "Epoch: 1608: Train loss: 6.35206413269043 Valid loss: 0.009563198313117027\n",
      "Epoch: 1609: Train loss: 6.2366228103637695 Valid loss: 6.3981194496154785\n",
      "Epoch: 1610: Train loss: 6.343197822570801 Valid loss: 0.009297724813222885\n",
      "Epoch: 1611: Train loss: 6.228376388549805 Valid loss: 6.386871814727783\n",
      "Epoch: 1612: Train loss: 6.3343586921691895 Valid loss: 0.009036773815751076\n",
      "Epoch: 1613: Train loss: 6.220147609710693 Valid loss: 6.375628471374512\n",
      "Epoch: 1614: Train loss: 6.32554292678833 Valid loss: 0.008780596777796745\n",
      "Epoch: 1615: Train loss: 6.211943626403809 Valid loss: 6.364437580108643\n",
      "Epoch: 1616: Train loss: 6.316739559173584 Valid loss: 0.008528642356395721\n",
      "Epoch: 1617: Train loss: 6.203746795654297 Valid loss: 6.353250980377197\n",
      "Epoch: 1618: Train loss: 6.307954788208008 Valid loss: 0.008282087743282318\n",
      "Epoch: 1619: Train loss: 6.195565700531006 Valid loss: 6.342074871063232\n",
      "Epoch: 1620: Train loss: 6.299192905426025 Valid loss: 0.00803971104323864\n",
      "Epoch: 1621: Train loss: 6.187412738800049 Valid loss: 6.330924987792969\n",
      "Epoch: 1622: Train loss: 6.29045295715332 Valid loss: 0.00780177116394043\n",
      "Epoch: 1623: Train loss: 6.179268836975098 Valid loss: 6.319797992706299\n",
      "Epoch: 1624: Train loss: 6.281732082366943 Valid loss: 0.007568891625851393\n",
      "Epoch: 1625: Train loss: 6.171144962310791 Valid loss: 6.308690071105957\n",
      "Epoch: 1626: Train loss: 6.273024559020996 Valid loss: 0.0073403469286859035\n",
      "Epoch: 1627: Train loss: 6.163031578063965 Valid loss: 6.297586917877197\n",
      "Epoch: 1628: Train loss: 6.264338970184326 Valid loss: 0.007116375491023064\n",
      "Epoch: 1629: Train loss: 6.154945373535156 Valid loss: 6.2865214347839355\n",
      "Epoch: 1630: Train loss: 6.255678653717041 Valid loss: 0.006896691862493753\n",
      "Epoch: 1631: Train loss: 6.146871089935303 Valid loss: 6.275468349456787\n",
      "Epoch: 1632: Train loss: 6.247034549713135 Valid loss: 0.006681729573756456\n",
      "Epoch: 1633: Train loss: 6.138811111450195 Valid loss: 6.264431953430176\n",
      "Epoch: 1634: Train loss: 6.238401412963867 Valid loss: 0.006471026223152876\n",
      "Epoch: 1635: Train loss: 6.130760669708252 Valid loss: 6.2534074783325195\n",
      "Epoch: 1636: Train loss: 6.229785919189453 Valid loss: 0.006265175063163042\n",
      "Epoch: 1637: Train loss: 6.122727870941162 Valid loss: 6.242395877838135\n",
      "Epoch: 1638: Train loss: 6.221190452575684 Valid loss: 0.006063138134777546\n",
      "Epoch: 1639: Train loss: 6.11471700668335 Valid loss: 6.2314019203186035\n",
      "Epoch: 1640: Train loss: 6.212617874145508 Valid loss: 0.0058658611960709095\n",
      "Epoch: 1641: Train loss: 6.106716632843018 Valid loss: 6.220439910888672\n",
      "Epoch: 1642: Train loss: 6.204058647155762 Valid loss: 0.005672403611242771\n",
      "Epoch: 1643: Train loss: 6.09873628616333 Valid loss: 6.2094831466674805\n",
      "Epoch: 1644: Train loss: 6.195523262023926 Valid loss: 0.005483861546963453\n",
      "Epoch: 1645: Train loss: 6.090775012969971 Valid loss: 6.198558807373047\n",
      "Epoch: 1646: Train loss: 6.187004089355469 Valid loss: 0.005299064796417952\n",
      "Epoch: 1647: Train loss: 6.082821369171143 Valid loss: 6.187637805938721\n",
      "Epoch: 1648: Train loss: 6.1784987449646 Valid loss: 0.005118954926729202\n",
      "Epoch: 1649: Train loss: 6.074887752532959 Valid loss: 6.176730632781982\n",
      "Epoch: 1650: Train loss: 6.170015811920166 Valid loss: 0.004942906089127064\n",
      "Epoch: 1651: Train loss: 6.06697416305542 Valid loss: 6.165854454040527\n",
      "Epoch: 1652: Train loss: 6.161550998687744 Valid loss: 0.00477090897038579\n",
      "Epoch: 1653: Train loss: 6.059071063995361 Valid loss: 6.154996871948242\n",
      "Epoch: 1654: Train loss: 6.153102397918701 Valid loss: 0.004603341221809387\n",
      "Epoch: 1655: Train loss: 6.051178932189941 Valid loss: 6.144139289855957\n",
      "Epoch: 1656: Train loss: 6.1446685791015625 Valid loss: 0.004439645446836948\n",
      "Epoch: 1657: Train loss: 6.043303489685059 Valid loss: 6.133298397064209\n",
      "Epoch: 1658: Train loss: 6.136247634887695 Valid loss: 0.0042806086130440235\n",
      "Epoch: 1659: Train loss: 6.035444259643555 Valid loss: 6.122469902038574\n",
      "Epoch: 1660: Train loss: 6.127851486206055 Valid loss: 0.004125394392758608\n",
      "Epoch: 1661: Train loss: 6.027602672576904 Valid loss: 6.111673355102539\n",
      "Epoch: 1662: Train loss: 6.119473934173584 Valid loss: 0.003973981365561485\n",
      "Epoch: 1663: Train loss: 6.019779205322266 Valid loss: 6.10089111328125\n",
      "Epoch: 1664: Train loss: 6.1111159324646 Valid loss: 0.0038267537020146847\n",
      "Epoch: 1665: Train loss: 6.0119733810424805 Valid loss: 6.090132713317871\n",
      "Epoch: 1666: Train loss: 6.102777481079102 Valid loss: 0.003683754475787282\n",
      "Epoch: 1667: Train loss: 6.004176616668701 Valid loss: 6.079386234283447\n",
      "Epoch: 1668: Train loss: 6.094444751739502 Valid loss: 0.0035448167473077774\n",
      "Epoch: 1669: Train loss: 5.99639368057251 Valid loss: 6.06865119934082\n",
      "Epoch: 1670: Train loss: 6.08613395690918 Valid loss: 0.003409745404496789\n",
      "Epoch: 1671: Train loss: 5.988625526428223 Valid loss: 6.057929039001465\n",
      "Epoch: 1672: Train loss: 6.077843189239502 Valid loss: 0.003278618212789297\n",
      "Epoch: 1673: Train loss: 5.980875015258789 Valid loss: 6.047232627868652\n",
      "Epoch: 1674: Train loss: 6.069564342498779 Valid loss: 0.003151555312797427\n",
      "Epoch: 1675: Train loss: 5.97313928604126 Valid loss: 6.036537170410156\n",
      "Epoch: 1676: Train loss: 6.061305522918701 Valid loss: 0.003028560196980834\n",
      "Epoch: 1677: Train loss: 5.96541690826416 Valid loss: 6.02587890625\n",
      "Epoch: 1678: Train loss: 6.05306339263916 Valid loss: 0.0029093665070831776\n",
      "Epoch: 1679: Train loss: 5.957704544067383 Valid loss: 6.015214443206787\n",
      "Epoch: 1680: Train loss: 6.044832706451416 Valid loss: 0.0027941204607486725\n",
      "Epoch: 1681: Train loss: 5.950009346008301 Valid loss: 6.004574298858643\n",
      "Epoch: 1682: Train loss: 6.036617279052734 Valid loss: 0.0026829142589122057\n",
      "Epoch: 1683: Train loss: 5.942319393157959 Valid loss: 5.993927478790283\n",
      "Epoch: 1684: Train loss: 6.028409957885742 Valid loss: 0.002575607504695654\n",
      "Epoch: 1685: Train loss: 5.9346442222595215 Valid loss: 5.983292579650879\n",
      "Epoch: 1686: Train loss: 6.020226001739502 Valid loss: 0.0024719201028347015\n",
      "Epoch: 1687: Train loss: 5.926989555358887 Valid loss: 5.972697734832764\n",
      "Epoch: 1688: Train loss: 6.012057304382324 Valid loss: 0.0023722199257463217\n",
      "Epoch: 1689: Train loss: 5.919344902038574 Valid loss: 5.9621076583862305\n",
      "Epoch: 1690: Train loss: 6.003907203674316 Valid loss: 0.002276158658787608\n",
      "Epoch: 1691: Train loss: 5.911721229553223 Valid loss: 5.95152473449707\n",
      "Epoch: 1692: Train loss: 5.995779514312744 Valid loss: 0.0021837984677404165\n",
      "Epoch: 1693: Train loss: 5.904110431671143 Valid loss: 5.940980434417725\n",
      "Epoch: 1694: Train loss: 5.987663269042969 Valid loss: 0.0020952809136360884\n",
      "Epoch: 1695: Train loss: 5.896518230438232 Valid loss: 5.930442810058594\n",
      "Epoch: 1696: Train loss: 5.97956657409668 Valid loss: 0.0020105461589992046\n",
      "Epoch: 1697: Train loss: 5.8889336585998535 Valid loss: 5.919917583465576\n",
      "Epoch: 1698: Train loss: 5.97148323059082 Valid loss: 0.001929546007886529\n",
      "Epoch: 1699: Train loss: 5.88136625289917 Valid loss: 5.9094061851501465\n",
      "Epoch: 1700: Train loss: 5.96341609954834 Valid loss: 0.001852374873124063\n",
      "Epoch: 1701: Train loss: 5.873812198638916 Valid loss: 5.898906230926514\n",
      "Epoch: 1702: Train loss: 5.955362319946289 Valid loss: 0.0017788029508665204\n",
      "Epoch: 1703: Train loss: 5.866270065307617 Valid loss: 5.888425827026367\n",
      "Epoch: 1704: Train loss: 5.947324752807617 Valid loss: 0.001708841766230762\n",
      "Epoch: 1705: Train loss: 5.858738422393799 Valid loss: 5.877959251403809\n",
      "Epoch: 1706: Train loss: 5.939300537109375 Valid loss: 0.001642838353291154\n",
      "Epoch: 1707: Train loss: 5.851221561431885 Valid loss: 5.86749792098999\n",
      "Epoch: 1708: Train loss: 5.9312944412231445 Valid loss: 0.0015802376437932253\n",
      "Epoch: 1709: Train loss: 5.843716144561768 Valid loss: 5.857043743133545\n",
      "Epoch: 1710: Train loss: 5.923300743103027 Valid loss: 0.0015214916784316301\n",
      "Epoch: 1711: Train loss: 5.836230754852295 Valid loss: 5.846622467041016\n",
      "Epoch: 1712: Train loss: 5.915327548980713 Valid loss: 0.0014660683227702975\n",
      "Epoch: 1713: Train loss: 5.8287529945373535 Valid loss: 5.8362135887146\n",
      "Epoch: 1714: Train loss: 5.907362937927246 Valid loss: 0.0014145061140879989\n",
      "Epoch: 1715: Train loss: 5.821286678314209 Valid loss: 5.825798034667969\n",
      "Epoch: 1716: Train loss: 5.899409294128418 Valid loss: 0.0013664313592016697\n",
      "Epoch: 1717: Train loss: 5.81383752822876 Valid loss: 5.815410137176514\n",
      "Epoch: 1718: Train loss: 5.891473293304443 Valid loss: 0.0013218168169260025\n",
      "Epoch: 1719: Train loss: 5.806399822235107 Valid loss: 5.80503511428833\n",
      "Epoch: 1720: Train loss: 5.88356351852417 Valid loss: 0.001280779018998146\n",
      "Epoch: 1721: Train loss: 5.798975467681885 Valid loss: 5.794666767120361\n",
      "Epoch: 1722: Train loss: 5.8756608963012695 Valid loss: 0.0012433836236596107\n",
      "Epoch: 1723: Train loss: 5.791568756103516 Valid loss: 5.784323215484619\n",
      "Epoch: 1724: Train loss: 5.8677849769592285 Valid loss: 0.0012094175908714533\n",
      "Epoch: 1725: Train loss: 5.784182071685791 Valid loss: 5.774013519287109\n",
      "Epoch: 1726: Train loss: 5.859912395477295 Valid loss: 0.0011789243435487151\n",
      "Epoch: 1727: Train loss: 5.77679443359375 Valid loss: 5.763691425323486\n",
      "Epoch: 1728: Train loss: 5.852057933807373 Valid loss: 0.0011520452098920941\n",
      "Epoch: 1729: Train loss: 5.769425392150879 Valid loss: 5.753382205963135\n",
      "Epoch: 1730: Train loss: 5.844213008880615 Valid loss: 0.0011286038206890225\n",
      "Epoch: 1731: Train loss: 5.762070655822754 Valid loss: 5.743086338043213\n",
      "Epoch: 1732: Train loss: 5.836391925811768 Valid loss: 0.0011086489539593458\n",
      "Epoch: 1733: Train loss: 5.754724502563477 Valid loss: 5.732811450958252\n",
      "Epoch: 1734: Train loss: 5.828578472137451 Valid loss: 0.0010921185603365302\n",
      "Epoch: 1735: Train loss: 5.747386932373047 Valid loss: 5.722529888153076\n",
      "Epoch: 1736: Train loss: 5.820775985717773 Valid loss: 0.0010789762018248439\n",
      "Epoch: 1737: Train loss: 5.740072727203369 Valid loss: 5.71229362487793\n",
      "Epoch: 1738: Train loss: 5.8129987716674805 Valid loss: 0.001069336198270321\n",
      "Epoch: 1739: Train loss: 5.732765197753906 Valid loss: 5.702051639556885\n",
      "Epoch: 1740: Train loss: 5.805224895477295 Valid loss: 0.0010630814358592033\n",
      "Epoch: 1741: Train loss: 5.725466251373291 Valid loss: 5.691816806793213\n",
      "Epoch: 1742: Train loss: 5.797466278076172 Valid loss: 0.0010601418325677514\n",
      "Epoch: 1743: Train loss: 5.718183994293213 Valid loss: 5.681589603424072\n",
      "Epoch: 1744: Train loss: 5.789726257324219 Valid loss: 0.0010607352014631033\n",
      "Epoch: 1745: Train loss: 5.710915565490723 Valid loss: 5.671389579772949\n",
      "Epoch: 1746: Train loss: 5.781998634338379 Valid loss: 0.0010644798167049885\n",
      "Epoch: 1747: Train loss: 5.7036590576171875 Valid loss: 5.661194801330566\n",
      "Epoch: 1748: Train loss: 5.774291038513184 Valid loss: 0.0010716672986745834\n",
      "Epoch: 1749: Train loss: 5.696420669555664 Valid loss: 5.651027679443359\n",
      "Epoch: 1750: Train loss: 5.766594409942627 Valid loss: 0.0010822205804288387\n",
      "Epoch: 1751: Train loss: 5.6891913414001465 Valid loss: 5.640868186950684\n",
      "Epoch: 1752: Train loss: 5.758916854858398 Valid loss: 0.0010960041545331478\n",
      "Epoch: 1753: Train loss: 5.681972980499268 Valid loss: 5.630715847015381\n",
      "Epoch: 1754: Train loss: 5.75124979019165 Valid loss: 0.001113215577788651\n",
      "Epoch: 1755: Train loss: 5.674764633178711 Valid loss: 5.6205830574035645\n",
      "Epoch: 1756: Train loss: 5.743590831756592 Valid loss: 0.0011337202740833163\n",
      "Epoch: 1757: Train loss: 5.667568683624268 Valid loss: 5.610445499420166\n",
      "Epoch: 1758: Train loss: 5.735951900482178 Valid loss: 0.0011574339587241411\n",
      "Epoch: 1759: Train loss: 5.660387992858887 Valid loss: 5.600341320037842\n",
      "Epoch: 1760: Train loss: 5.728326797485352 Valid loss: 0.0011844985419884324\n",
      "Epoch: 1761: Train loss: 5.6532182693481445 Valid loss: 5.590231418609619\n",
      "Epoch: 1762: Train loss: 5.72071647644043 Valid loss: 0.0012147835223004222\n",
      "Epoch: 1763: Train loss: 5.646065711975098 Valid loss: 5.580147743225098\n",
      "Epoch: 1764: Train loss: 5.7131195068359375 Valid loss: 0.001248281099833548\n",
      "Epoch: 1765: Train loss: 5.638924598693848 Valid loss: 5.570077896118164\n",
      "Epoch: 1766: Train loss: 5.705533981323242 Valid loss: 0.0012851477367803454\n",
      "Epoch: 1767: Train loss: 5.631791114807129 Valid loss: 5.560009479522705\n",
      "Epoch: 1768: Train loss: 5.697964191436768 Valid loss: 0.0013250572374090552\n",
      "Epoch: 1769: Train loss: 5.624663352966309 Valid loss: 5.549954891204834\n",
      "Epoch: 1770: Train loss: 5.690410137176514 Valid loss: 0.0013682361459359527\n",
      "Epoch: 1771: Train loss: 5.617562294006348 Valid loss: 5.539927959442139\n",
      "Epoch: 1772: Train loss: 5.682868480682373 Valid loss: 0.001414633123204112\n",
      "Epoch: 1773: Train loss: 5.610467910766602 Valid loss: 5.529894828796387\n",
      "Epoch: 1774: Train loss: 5.675339698791504 Valid loss: 0.001464268658310175\n",
      "Epoch: 1775: Train loss: 5.603382587432861 Valid loss: 5.519882678985596\n",
      "Epoch: 1776: Train loss: 5.66782283782959 Valid loss: 0.001516953227110207\n",
      "Epoch: 1777: Train loss: 5.596311569213867 Valid loss: 5.509885787963867\n",
      "Epoch: 1778: Train loss: 5.660325050354004 Valid loss: 0.001572904409840703\n",
      "Epoch: 1779: Train loss: 5.589253902435303 Valid loss: 5.49990177154541\n",
      "Epoch: 1780: Train loss: 5.652838230133057 Valid loss: 0.0016320329159498215\n",
      "Epoch: 1781: Train loss: 5.582206726074219 Valid loss: 5.489924907684326\n",
      "Epoch: 1782: Train loss: 5.645365238189697 Valid loss: 0.0016942871734499931\n",
      "Epoch: 1783: Train loss: 5.57517147064209 Valid loss: 5.479957103729248\n",
      "Epoch: 1784: Train loss: 5.637905597686768 Valid loss: 0.0017595835961401463\n",
      "Epoch: 1785: Train loss: 5.568154335021973 Valid loss: 5.470016002655029\n",
      "Epoch: 1786: Train loss: 5.630462169647217 Valid loss: 0.0018280986696481705\n",
      "Epoch: 1787: Train loss: 5.561135768890381 Valid loss: 5.460069179534912\n",
      "Epoch: 1788: Train loss: 5.623027324676514 Valid loss: 0.0018996261060237885\n",
      "Epoch: 1789: Train loss: 5.554138660430908 Valid loss: 5.450143814086914\n",
      "Epoch: 1790: Train loss: 5.615610122680664 Valid loss: 0.0019744127057492733\n",
      "Epoch: 1791: Train loss: 5.547152042388916 Valid loss: 5.4402337074279785\n",
      "Epoch: 1792: Train loss: 5.608206272125244 Valid loss: 0.0020522382110357285\n",
      "Epoch: 1793: Train loss: 5.540175914764404 Valid loss: 5.430330753326416\n",
      "Epoch: 1794: Train loss: 5.60081148147583 Valid loss: 0.0021329845767468214\n",
      "Epoch: 1795: Train loss: 5.533207416534424 Valid loss: 5.420429229736328\n",
      "Epoch: 1796: Train loss: 5.593434810638428 Valid loss: 0.002216961467638612\n",
      "Epoch: 1797: Train loss: 5.526253700256348 Valid loss: 5.410547733306885\n",
      "Epoch: 1798: Train loss: 5.586062908172607 Valid loss: 0.0023039921652525663\n",
      "Epoch: 1799: Train loss: 5.519312858581543 Valid loss: 5.400681495666504\n",
      "Epoch: 1800: Train loss: 5.578711986541748 Valid loss: 0.00239397375844419\n",
      "Epoch: 1801: Train loss: 5.512388229370117 Valid loss: 5.390836238861084\n",
      "Epoch: 1802: Train loss: 5.571374893188477 Valid loss: 0.002487035933881998\n",
      "Epoch: 1803: Train loss: 5.505475997924805 Valid loss: 5.380992889404297\n",
      "Epoch: 1804: Train loss: 5.5640549659729 Valid loss: 0.002583206631243229\n",
      "Epoch: 1805: Train loss: 5.498565673828125 Valid loss: 5.371163368225098\n",
      "Epoch: 1806: Train loss: 5.5567402839660645 Valid loss: 0.002682266989722848\n",
      "Epoch: 1807: Train loss: 5.491668701171875 Valid loss: 5.361330032348633\n",
      "Epoch: 1808: Train loss: 5.549437999725342 Valid loss: 0.0027844151481986046\n",
      "Epoch: 1809: Train loss: 5.4847846031188965 Valid loss: 5.351522922515869\n",
      "Epoch: 1810: Train loss: 5.542150974273682 Valid loss: 0.0028894594870507717\n",
      "Epoch: 1811: Train loss: 5.477911949157715 Valid loss: 5.3417253494262695\n",
      "Epoch: 1812: Train loss: 5.534876823425293 Valid loss: 0.0029975343495607376\n",
      "Epoch: 1813: Train loss: 5.4710540771484375 Valid loss: 5.331942558288574\n",
      "Epoch: 1814: Train loss: 5.527619361877441 Valid loss: 0.003108745440840721\n",
      "Epoch: 1815: Train loss: 5.464206218719482 Valid loss: 5.322173118591309\n",
      "Epoch: 1816: Train loss: 5.520376682281494 Valid loss: 0.0032229141797870398\n",
      "Epoch: 1817: Train loss: 5.457376003265381 Valid loss: 5.312430381774902\n",
      "Epoch: 1818: Train loss: 5.513144016265869 Valid loss: 0.003340073861181736\n",
      "Epoch: 1819: Train loss: 5.450547218322754 Valid loss: 5.302678108215332\n",
      "Epoch: 1820: Train loss: 5.505923748016357 Valid loss: 0.0034600819926708937\n",
      "Epoch: 1821: Train loss: 5.443735122680664 Valid loss: 5.292940139770508\n",
      "Epoch: 1822: Train loss: 5.498716831207275 Valid loss: 0.00358314486220479\n",
      "Epoch: 1823: Train loss: 5.436931610107422 Valid loss: 5.283210754394531\n",
      "Epoch: 1824: Train loss: 5.491526126861572 Valid loss: 0.003709067590534687\n",
      "Epoch: 1825: Train loss: 5.430141925811768 Valid loss: 5.273502826690674\n",
      "Epoch: 1826: Train loss: 5.484344482421875 Valid loss: 0.0038379658944904804\n",
      "Epoch: 1827: Train loss: 5.423366069793701 Valid loss: 5.263803005218506\n",
      "Epoch: 1828: Train loss: 5.477181434631348 Valid loss: 0.003969728015363216\n",
      "Epoch: 1829: Train loss: 5.416601181030273 Valid loss: 5.254124164581299\n",
      "Epoch: 1830: Train loss: 5.470029830932617 Valid loss: 0.004104621708393097\n",
      "Epoch: 1831: Train loss: 5.40985631942749 Valid loss: 5.244460105895996\n",
      "Epoch: 1832: Train loss: 5.462902545928955 Valid loss: 0.004242388065904379\n",
      "Epoch: 1833: Train loss: 5.40311861038208 Valid loss: 5.234810829162598\n",
      "Epoch: 1834: Train loss: 5.455776691436768 Valid loss: 0.004383188206702471\n",
      "Epoch: 1835: Train loss: 5.39638614654541 Valid loss: 5.22516393661499\n",
      "Epoch: 1836: Train loss: 5.448661804199219 Valid loss: 0.004526366945356131\n",
      "Epoch: 1837: Train loss: 5.38966703414917 Valid loss: 5.215531826019287\n",
      "Epoch: 1838: Train loss: 5.441565036773682 Valid loss: 0.004672813229262829\n",
      "Epoch: 1839: Train loss: 5.382962703704834 Valid loss: 5.20590877532959\n",
      "Epoch: 1840: Train loss: 5.434476852416992 Valid loss: 0.004821940325200558\n",
      "Epoch: 1841: Train loss: 5.3762664794921875 Valid loss: 5.1963067054748535\n",
      "Epoch: 1842: Train loss: 5.427407264709473 Valid loss: 0.0049742949195206165\n",
      "Epoch: 1843: Train loss: 5.369587421417236 Valid loss: 5.186718463897705\n",
      "Epoch: 1844: Train loss: 5.420349597930908 Valid loss: 0.0051294309087097645\n",
      "Epoch: 1845: Train loss: 5.362913131713867 Valid loss: 5.177134037017822\n",
      "Epoch: 1846: Train loss: 5.413300514221191 Valid loss: 0.005287350155413151\n",
      "Epoch: 1847: Train loss: 5.356250286102295 Valid loss: 5.167558193206787\n",
      "Epoch: 1848: Train loss: 5.406269550323486 Valid loss: 0.0054479120299220085\n",
      "Epoch: 1849: Train loss: 5.349600791931152 Valid loss: 5.1580023765563965\n",
      "Epoch: 1850: Train loss: 5.3992486000061035 Valid loss: 0.005611623637378216\n",
      "Epoch: 1851: Train loss: 5.34296178817749 Valid loss: 5.1484503746032715\n",
      "Epoch: 1852: Train loss: 5.3922343254089355 Valid loss: 0.005777720361948013\n",
      "Epoch: 1853: Train loss: 5.336328506469727 Valid loss: 5.138900279998779\n",
      "Epoch: 1854: Train loss: 5.38523530960083 Valid loss: 0.005947129800915718\n",
      "Epoch: 1855: Train loss: 5.329710960388184 Valid loss: 5.129371166229248\n",
      "Epoch: 1856: Train loss: 5.378252983093262 Valid loss: 0.00611900445073843\n",
      "Epoch: 1857: Train loss: 5.323104381561279 Valid loss: 5.119858264923096\n",
      "Epoch: 1858: Train loss: 5.371281623840332 Valid loss: 0.006293728947639465\n",
      "Epoch: 1859: Train loss: 5.316514015197754 Valid loss: 5.110354423522949\n",
      "Epoch: 1860: Train loss: 5.364327430725098 Valid loss: 0.006471365224570036\n",
      "Epoch: 1861: Train loss: 5.309934139251709 Valid loss: 5.100876331329346\n",
      "Epoch: 1862: Train loss: 5.357383728027344 Valid loss: 0.0066522471606731415\n",
      "Epoch: 1863: Train loss: 5.303360939025879 Valid loss: 5.091395854949951\n",
      "Epoch: 1864: Train loss: 5.350450038909912 Valid loss: 0.006835362873971462\n",
      "Epoch: 1865: Train loss: 5.296796798706055 Valid loss: 5.081930637359619\n",
      "Epoch: 1866: Train loss: 5.34352970123291 Valid loss: 0.007021430879831314\n",
      "Epoch: 1867: Train loss: 5.290248394012451 Valid loss: 5.072474002838135\n",
      "Epoch: 1868: Train loss: 5.336625576019287 Valid loss: 0.007210214622318745\n",
      "Epoch: 1869: Train loss: 5.283716201782227 Valid loss: 5.0630388259887695\n",
      "Epoch: 1870: Train loss: 5.329737663269043 Valid loss: 0.007402334362268448\n",
      "Epoch: 1871: Train loss: 5.277196884155273 Valid loss: 5.053619384765625\n",
      "Epoch: 1872: Train loss: 5.3228583335876465 Valid loss: 0.00759672187268734\n",
      "Epoch: 1873: Train loss: 5.270681381225586 Valid loss: 5.044214248657227\n",
      "Epoch: 1874: Train loss: 5.315996170043945 Valid loss: 0.007793745957314968\n",
      "Epoch: 1875: Train loss: 5.264178276062012 Valid loss: 5.034806728363037\n",
      "Epoch: 1876: Train loss: 5.309140682220459 Valid loss: 0.00799392256885767\n",
      "Epoch: 1877: Train loss: 5.257683753967285 Valid loss: 5.02541971206665\n",
      "Epoch: 1878: Train loss: 5.3022966384887695 Valid loss: 0.00819646567106247\n",
      "Epoch: 1879: Train loss: 5.251206398010254 Valid loss: 5.016042232513428\n",
      "Epoch: 1880: Train loss: 5.295466899871826 Valid loss: 0.00840182974934578\n",
      "Epoch: 1881: Train loss: 5.244733810424805 Valid loss: 5.006661891937256\n",
      "Epoch: 1882: Train loss: 5.28864860534668 Valid loss: 0.008610253222286701\n",
      "Epoch: 1883: Train loss: 5.238272190093994 Valid loss: 4.997302055358887\n",
      "Epoch: 1884: Train loss: 5.2818427085876465 Valid loss: 0.008821060881018639\n",
      "Epoch: 1885: Train loss: 5.23182487487793 Valid loss: 4.987970352172852\n",
      "Epoch: 1886: Train loss: 5.275053024291992 Valid loss: 0.009034897200763226\n",
      "Epoch: 1887: Train loss: 5.225388526916504 Valid loss: 4.978636264801025\n",
      "Epoch: 1888: Train loss: 5.268278121948242 Valid loss: 0.009251384064555168\n",
      "Epoch: 1889: Train loss: 5.218967914581299 Valid loss: 4.9693284034729\n",
      "Epoch: 1890: Train loss: 5.2615180015563965 Valid loss: 0.009471016004681587\n",
      "Epoch: 1891: Train loss: 5.212560176849365 Valid loss: 4.960030555725098\n",
      "Epoch: 1892: Train loss: 5.2547712326049805 Valid loss: 0.009693166241049767\n",
      "Epoch: 1893: Train loss: 5.2061614990234375 Valid loss: 4.950747966766357\n",
      "Epoch: 1894: Train loss: 5.248032093048096 Valid loss: 0.009917758405208588\n",
      "Epoch: 1895: Train loss: 5.199766159057617 Valid loss: 4.941467761993408\n",
      "Epoch: 1896: Train loss: 5.241300582885742 Valid loss: 0.010144569911062717\n",
      "Epoch: 1897: Train loss: 5.193387985229492 Valid loss: 4.93220329284668\n",
      "Epoch: 1898: Train loss: 5.234589576721191 Valid loss: 0.010375258512794971\n",
      "Epoch: 1899: Train loss: 5.187020301818848 Valid loss: 4.922948837280273\n",
      "Epoch: 1900: Train loss: 5.227887153625488 Valid loss: 0.01060782466083765\n",
      "Epoch: 1901: Train loss: 5.180668354034424 Valid loss: 4.913715362548828\n",
      "Epoch: 1902: Train loss: 5.22120475769043 Valid loss: 0.01084374263882637\n",
      "Epoch: 1903: Train loss: 5.17432165145874 Valid loss: 4.904490947723389\n",
      "Epoch: 1904: Train loss: 5.2145256996154785 Valid loss: 0.011081881821155548\n",
      "Epoch: 1905: Train loss: 5.1679863929748535 Valid loss: 4.8952765464782715\n",
      "Epoch: 1906: Train loss: 5.20786190032959 Valid loss: 0.011322516947984695\n",
      "Epoch: 1907: Train loss: 5.161658763885498 Valid loss: 4.886070728302002\n",
      "Epoch: 1908: Train loss: 5.201206684112549 Valid loss: 0.01156585942953825\n",
      "Epoch: 1909: Train loss: 5.15534782409668 Valid loss: 4.8768744468688965\n",
      "Epoch: 1910: Train loss: 5.194572448730469 Valid loss: 0.011812405660748482\n",
      "Epoch: 1911: Train loss: 5.149049282073975 Valid loss: 4.867700099945068\n",
      "Epoch: 1912: Train loss: 5.187948226928711 Valid loss: 0.012061608955264091\n",
      "Epoch: 1913: Train loss: 5.142758846282959 Valid loss: 4.858535289764404\n",
      "Epoch: 1914: Train loss: 5.181333541870117 Valid loss: 0.01231278758496046\n",
      "Epoch: 1915: Train loss: 5.136477947235107 Valid loss: 4.849385738372803\n",
      "Epoch: 1916: Train loss: 5.174734115600586 Valid loss: 0.012566914781928062\n",
      "Epoch: 1917: Train loss: 5.130212783813477 Valid loss: 4.840251922607422\n",
      "Epoch: 1918: Train loss: 5.168150901794434 Valid loss: 0.012824260629713535\n",
      "Epoch: 1919: Train loss: 5.12395715713501 Valid loss: 4.8311333656311035\n",
      "Epoch: 1920: Train loss: 5.161574363708496 Valid loss: 0.013083701953291893\n",
      "Epoch: 1921: Train loss: 5.117710590362549 Valid loss: 4.822011470794678\n",
      "Epoch: 1922: Train loss: 5.15501594543457 Valid loss: 0.01334596797823906\n",
      "Epoch: 1923: Train loss: 5.111476421356201 Valid loss: 4.812912464141846\n",
      "Epoch: 1924: Train loss: 5.148462772369385 Valid loss: 0.013610843569040298\n",
      "Epoch: 1925: Train loss: 5.105253219604492 Valid loss: 4.803821563720703\n",
      "Epoch: 1926: Train loss: 5.141923904418945 Valid loss: 0.01387861929833889\n",
      "Epoch: 1927: Train loss: 5.099039554595947 Valid loss: 4.794747352600098\n",
      "Epoch: 1928: Train loss: 5.135398864746094 Valid loss: 0.014148294925689697\n",
      "Epoch: 1929: Train loss: 5.092836380004883 Valid loss: 4.785676956176758\n",
      "Epoch: 1930: Train loss: 5.128886699676514 Valid loss: 0.014421219006180763\n",
      "Epoch: 1931: Train loss: 5.086645126342773 Valid loss: 4.776627063751221\n",
      "Epoch: 1932: Train loss: 5.122381687164307 Valid loss: 0.014696664176881313\n",
      "Epoch: 1933: Train loss: 5.08046293258667 Valid loss: 4.767582893371582\n",
      "Epoch: 1934: Train loss: 5.1158976554870605 Valid loss: 0.014975374564528465\n",
      "Epoch: 1935: Train loss: 5.074300765991211 Valid loss: 4.758569240570068\n",
      "Epoch: 1936: Train loss: 5.109426975250244 Valid loss: 0.015255725011229515\n",
      "Epoch: 1937: Train loss: 5.068148136138916 Valid loss: 4.749560832977295\n",
      "Epoch: 1938: Train loss: 5.102969646453857 Valid loss: 0.015539443120360374\n",
      "Epoch: 1939: Train loss: 5.062004089355469 Valid loss: 4.740562438964844\n",
      "Epoch: 1940: Train loss: 5.0965189933776855 Valid loss: 0.015825418755412102\n",
      "Epoch: 1941: Train loss: 5.055871963500977 Valid loss: 4.7315850257873535\n",
      "Epoch: 1942: Train loss: 5.090082168579102 Valid loss: 0.016114074736833572\n",
      "Epoch: 1943: Train loss: 5.04974365234375 Valid loss: 4.722611427307129\n",
      "Epoch: 1944: Train loss: 5.083658218383789 Valid loss: 0.016405273228883743\n",
      "Epoch: 1945: Train loss: 5.043634414672852 Valid loss: 4.713653564453125\n",
      "Epoch: 1946: Train loss: 5.077247142791748 Valid loss: 0.01669936813414097\n",
      "Epoch: 1947: Train loss: 5.037527084350586 Valid loss: 4.70469331741333\n",
      "Epoch: 1948: Train loss: 5.070841312408447 Valid loss: 0.016995202749967575\n",
      "Epoch: 1949: Train loss: 5.031435012817383 Valid loss: 4.695760250091553\n",
      "Epoch: 1950: Train loss: 5.064455509185791 Valid loss: 0.01729412004351616\n",
      "Epoch: 1951: Train loss: 5.025351524353027 Valid loss: 4.686825275421143\n",
      "Epoch: 1952: Train loss: 5.058075904846191 Valid loss: 0.017595559358596802\n",
      "Epoch: 1953: Train loss: 5.019279479980469 Valid loss: 4.677917957305908\n",
      "Epoch: 1954: Train loss: 5.051705360412598 Valid loss: 0.017899390310049057\n",
      "Epoch: 1955: Train loss: 5.013220310211182 Valid loss: 4.669014930725098\n",
      "Epoch: 1956: Train loss: 5.045352935791016 Valid loss: 0.018206365406513214\n",
      "Epoch: 1957: Train loss: 5.007167339324951 Valid loss: 4.660121440887451\n",
      "Epoch: 1958: Train loss: 5.039013385772705 Valid loss: 0.01851579174399376\n",
      "Epoch: 1959: Train loss: 5.001134872436523 Valid loss: 4.651261329650879\n",
      "Epoch: 1960: Train loss: 5.0326924324035645 Valid loss: 0.01882748305797577\n",
      "Epoch: 1961: Train loss: 4.995114326477051 Valid loss: 4.642410755157471\n",
      "Epoch: 1962: Train loss: 5.0263800621032715 Valid loss: 0.019143112003803253\n",
      "Epoch: 1963: Train loss: 4.989102363586426 Valid loss: 4.633563995361328\n",
      "Epoch: 1964: Train loss: 5.020080089569092 Valid loss: 0.019460080191493034\n",
      "Epoch: 1965: Train loss: 4.983099937438965 Valid loss: 4.624739170074463\n",
      "Epoch: 1966: Train loss: 5.013792037963867 Valid loss: 0.019779395312070847\n",
      "Epoch: 1967: Train loss: 4.977105140686035 Valid loss: 4.615923881530762\n",
      "Epoch: 1968: Train loss: 5.00752067565918 Valid loss: 0.02010273188352585\n",
      "Epoch: 1969: Train loss: 4.971126079559326 Valid loss: 4.607112407684326\n",
      "Epoch: 1970: Train loss: 5.001251220703125 Valid loss: 0.020427705720067024\n",
      "Epoch: 1971: Train loss: 4.965153217315674 Valid loss: 4.598317623138428\n",
      "Epoch: 1972: Train loss: 4.994996070861816 Valid loss: 0.02075459063053131\n",
      "Epoch: 1973: Train loss: 4.959190368652344 Valid loss: 4.58954381942749\n",
      "Epoch: 1974: Train loss: 4.9887590408325195 Valid loss: 0.021084897220134735\n",
      "Epoch: 1975: Train loss: 4.953242778778076 Valid loss: 4.58077335357666\n",
      "Epoch: 1976: Train loss: 4.982531547546387 Valid loss: 0.02141736075282097\n",
      "Epoch: 1977: Train loss: 4.947305679321289 Valid loss: 4.572024345397949\n",
      "Epoch: 1978: Train loss: 4.976316452026367 Valid loss: 0.021752312779426575\n",
      "Epoch: 1979: Train loss: 4.941378116607666 Valid loss: 4.563286304473877\n",
      "Epoch: 1980: Train loss: 4.970113754272461 Valid loss: 0.02209024503827095\n",
      "Epoch: 1981: Train loss: 4.935461044311523 Valid loss: 4.554558277130127\n",
      "Epoch: 1982: Train loss: 4.963921546936035 Valid loss: 0.022430483251810074\n",
      "Epoch: 1983: Train loss: 4.929557800292969 Valid loss: 4.5458455085754395\n",
      "Epoch: 1984: Train loss: 4.957745552062988 Valid loss: 0.022773917764425278\n",
      "Epoch: 1985: Train loss: 4.92365837097168 Valid loss: 4.537142276763916\n",
      "Epoch: 1986: Train loss: 4.951570987701416 Valid loss: 0.023118622601032257\n",
      "Epoch: 1987: Train loss: 4.9177703857421875 Valid loss: 4.528443813323975\n",
      "Epoch: 1988: Train loss: 4.945413589477539 Valid loss: 0.0234660841524601\n",
      "Epoch: 1989: Train loss: 4.911895751953125 Valid loss: 4.519766807556152\n",
      "Epoch: 1990: Train loss: 4.939269542694092 Valid loss: 0.023816589266061783\n",
      "Epoch: 1991: Train loss: 4.906030654907227 Valid loss: 4.511099815368652\n",
      "Epoch: 1992: Train loss: 4.933135032653809 Valid loss: 0.024169359356164932\n",
      "Epoch: 1993: Train loss: 4.900177478790283 Valid loss: 4.502453804016113\n",
      "Epoch: 1994: Train loss: 4.927020072937012 Valid loss: 0.024524815380573273\n",
      "Epoch: 1995: Train loss: 4.894339084625244 Valid loss: 4.493818283081055\n",
      "Epoch: 1996: Train loss: 4.920917987823486 Valid loss: 0.024883583188056946\n",
      "Epoch: 1997: Train loss: 4.888514041900635 Valid loss: 4.485204696655273\n",
      "Epoch: 1998: Train loss: 4.914827346801758 Valid loss: 0.025244243443012238\n",
      "Epoch: 1999: Train loss: 4.882698059082031 Valid loss: 4.476600646972656\n",
      "Epoch: 2000: Train loss: 4.908749103546143 Valid loss: 0.02560705691576004\n",
      "Epoch: 2001: Train loss: 4.876886367797852 Valid loss: 4.468000411987305\n",
      "Epoch: 2002: Train loss: 4.902675628662109 Valid loss: 0.02597356215119362\n",
      "Epoch: 2003: Train loss: 4.871092319488525 Valid loss: 4.459427356719971\n",
      "Epoch: 2004: Train loss: 4.896622657775879 Valid loss: 0.02634206786751747\n",
      "Epoch: 2005: Train loss: 4.865306377410889 Valid loss: 4.450859546661377\n",
      "Epoch: 2006: Train loss: 4.890577793121338 Valid loss: 0.02671300247311592\n",
      "Epoch: 2007: Train loss: 4.8595290184021 Valid loss: 4.4423017501831055\n",
      "Epoch: 2008: Train loss: 4.884544849395752 Valid loss: 0.027086399495601654\n",
      "Epoch: 2009: Train loss: 4.853766918182373 Valid loss: 4.433765411376953\n",
      "Epoch: 2010: Train loss: 4.878522872924805 Valid loss: 0.027462322264909744\n",
      "Epoch: 2011: Train loss: 4.84800910949707 Valid loss: 4.425232410430908\n",
      "Epoch: 2012: Train loss: 4.872514247894287 Valid loss: 0.027840247377753258\n",
      "Epoch: 2013: Train loss: 4.842264175415039 Valid loss: 4.4167046546936035\n",
      "Epoch: 2014: Train loss: 4.866508483886719 Valid loss: 0.028221148997545242\n",
      "Epoch: 2015: Train loss: 4.836528301239014 Valid loss: 4.408204078674316\n",
      "Epoch: 2016: Train loss: 4.860528945922852 Valid loss: 0.028605028986930847\n",
      "Epoch: 2017: Train loss: 4.830808162689209 Valid loss: 4.39972448348999\n",
      "Epoch: 2018: Train loss: 4.854558944702148 Valid loss: 0.028991203755140305\n",
      "Epoch: 2019: Train loss: 4.82509708404541 Valid loss: 4.391249656677246\n",
      "Epoch: 2020: Train loss: 4.848599910736084 Valid loss: 0.029379192739725113\n",
      "Epoch: 2021: Train loss: 4.819399356842041 Valid loss: 4.382796764373779\n",
      "Epoch: 2022: Train loss: 4.842654228210449 Valid loss: 0.02977091446518898\n",
      "Epoch: 2023: Train loss: 4.813717842102051 Valid loss: 4.374353408813477\n",
      "Epoch: 2024: Train loss: 4.836720943450928 Valid loss: 0.03016481176018715\n",
      "Epoch: 2025: Train loss: 4.808034896850586 Valid loss: 4.3659257888793945\n",
      "Epoch: 2026: Train loss: 4.83079719543457 Valid loss: 0.0305603239685297\n",
      "Epoch: 2027: Train loss: 4.8023681640625 Valid loss: 4.357508182525635\n",
      "Epoch: 2028: Train loss: 4.824882984161377 Valid loss: 0.030959390103816986\n",
      "Epoch: 2029: Train loss: 4.7967119216918945 Valid loss: 4.349095344543457\n",
      "Epoch: 2030: Train loss: 4.8189826011657715 Valid loss: 0.03136007487773895\n",
      "Epoch: 2031: Train loss: 4.791060447692871 Valid loss: 4.340698719024658\n",
      "Epoch: 2032: Train loss: 4.813090801239014 Valid loss: 0.031763382256031036\n",
      "Epoch: 2033: Train loss: 4.78542423248291 Valid loss: 4.332317352294922\n",
      "Epoch: 2034: Train loss: 4.807214736938477 Valid loss: 0.03216956555843353\n",
      "Epoch: 2035: Train loss: 4.779801845550537 Valid loss: 4.3239521980285645\n",
      "Epoch: 2036: Train loss: 4.801351547241211 Valid loss: 0.03257802128791809\n",
      "Epoch: 2037: Train loss: 4.77418327331543 Valid loss: 4.315597057342529\n",
      "Epoch: 2038: Train loss: 4.795500755310059 Valid loss: 0.03298882395029068\n",
      "Epoch: 2039: Train loss: 4.768584251403809 Valid loss: 4.307262897491455\n",
      "Epoch: 2040: Train loss: 4.789661407470703 Valid loss: 0.033402249217033386\n",
      "Epoch: 2041: Train loss: 4.762990474700928 Valid loss: 4.298939228057861\n",
      "Epoch: 2042: Train loss: 4.783832550048828 Valid loss: 0.03381906822323799\n",
      "Epoch: 2043: Train loss: 4.75740909576416 Valid loss: 4.290626049041748\n",
      "Epoch: 2044: Train loss: 4.778016090393066 Valid loss: 0.034237369894981384\n",
      "Epoch: 2045: Train loss: 4.751834392547607 Valid loss: 4.282328128814697\n",
      "Epoch: 2046: Train loss: 4.772210121154785 Valid loss: 0.03465801849961281\n",
      "Epoch: 2047: Train loss: 4.746268272399902 Valid loss: 4.274040699005127\n",
      "Epoch: 2048: Train loss: 4.766416549682617 Valid loss: 0.03508174419403076\n",
      "Epoch: 2049: Train loss: 4.740719318389893 Valid loss: 4.265763282775879\n",
      "Epoch: 2050: Train loss: 4.7606353759765625 Valid loss: 0.03550807014107704\n",
      "Epoch: 2051: Train loss: 4.735182285308838 Valid loss: 4.257518768310547\n",
      "Epoch: 2052: Train loss: 4.754870891571045 Valid loss: 0.0359363928437233\n",
      "Epoch: 2053: Train loss: 4.729653358459473 Valid loss: 4.249278545379639\n",
      "Epoch: 2054: Train loss: 4.749110221862793 Valid loss: 0.03636709228157997\n",
      "Epoch: 2055: Train loss: 4.72413444519043 Valid loss: 4.2410430908203125\n",
      "Epoch: 2056: Train loss: 4.743365287780762 Valid loss: 0.036800771951675415\n",
      "Epoch: 2057: Train loss: 4.718626499176025 Valid loss: 4.232823371887207\n",
      "Epoch: 2058: Train loss: 4.7376298904418945 Valid loss: 0.03723716735839844\n",
      "Epoch: 2059: Train loss: 4.713124752044678 Valid loss: 4.224618911743164\n",
      "Epoch: 2060: Train loss: 4.731907844543457 Valid loss: 0.03767511248588562\n",
      "Epoch: 2061: Train loss: 4.707638263702393 Valid loss: 4.216436862945557\n",
      "Epoch: 2062: Train loss: 4.726196765899658 Valid loss: 0.03811576962471008\n",
      "Epoch: 2063: Train loss: 4.70216178894043 Valid loss: 4.208258628845215\n",
      "Epoch: 2064: Train loss: 4.72049617767334 Valid loss: 0.038558442145586014\n",
      "Epoch: 2065: Train loss: 4.696695327758789 Valid loss: 4.200097560882568\n",
      "Epoch: 2066: Train loss: 4.714809894561768 Valid loss: 0.039004601538181305\n",
      "Epoch: 2067: Train loss: 4.6912360191345215 Valid loss: 4.191951274871826\n",
      "Epoch: 2068: Train loss: 4.709133625030518 Valid loss: 0.039452336728572845\n",
      "Epoch: 2069: Train loss: 4.685789108276367 Valid loss: 4.183809280395508\n",
      "Epoch: 2070: Train loss: 4.703469276428223 Valid loss: 0.0399034284055233\n",
      "Epoch: 2071: Train loss: 4.680356502532959 Valid loss: 4.175689697265625\n",
      "Epoch: 2072: Train loss: 4.697817325592041 Valid loss: 0.040356867015361786\n",
      "Epoch: 2073: Train loss: 4.674929141998291 Valid loss: 4.167579650878906\n",
      "Epoch: 2074: Train loss: 4.6921772956848145 Valid loss: 0.04081231355667114\n",
      "Epoch: 2075: Train loss: 4.669515609741211 Valid loss: 4.15947961807251\n",
      "Epoch: 2076: Train loss: 4.686549663543701 Valid loss: 0.041270576417446136\n",
      "Epoch: 2077: Train loss: 4.664115905761719 Valid loss: 4.151407241821289\n",
      "Epoch: 2078: Train loss: 4.680936336517334 Valid loss: 0.04173068702220917\n",
      "Epoch: 2079: Train loss: 4.658728122711182 Valid loss: 4.143350124359131\n",
      "Epoch: 2080: Train loss: 4.67533540725708 Valid loss: 0.04219435155391693\n",
      "Epoch: 2081: Train loss: 4.653343677520752 Valid loss: 4.1352972984313965\n",
      "Epoch: 2082: Train loss: 4.669740200042725 Valid loss: 0.04266028478741646\n",
      "Epoch: 2083: Train loss: 4.647974014282227 Valid loss: 4.127261161804199\n",
      "Epoch: 2084: Train loss: 4.664158821105957 Valid loss: 0.0431278795003891\n",
      "Epoch: 2085: Train loss: 4.642606258392334 Valid loss: 4.119234561920166\n",
      "Epoch: 2086: Train loss: 4.65858268737793 Valid loss: 0.04359673336148262\n",
      "Epoch: 2087: Train loss: 4.6372528076171875 Valid loss: 4.111213684082031\n",
      "Epoch: 2088: Train loss: 4.653019428253174 Valid loss: 0.044070128351449966\n",
      "Epoch: 2089: Train loss: 4.631908416748047 Valid loss: 4.103213310241699\n",
      "Epoch: 2090: Train loss: 4.647468090057373 Valid loss: 0.04454497992992401\n",
      "Epoch: 2091: Train loss: 4.626574516296387 Valid loss: 4.0952229499816895\n",
      "Epoch: 2092: Train loss: 4.641931056976318 Valid loss: 0.045021988451480865\n",
      "Epoch: 2093: Train loss: 4.621253490447998 Valid loss: 4.087248802185059\n",
      "Epoch: 2094: Train loss: 4.636406898498535 Valid loss: 0.0455021858215332\n",
      "Epoch: 2095: Train loss: 4.615943431854248 Valid loss: 4.079290866851807\n",
      "Epoch: 2096: Train loss: 4.630897045135498 Valid loss: 0.0459851399064064\n",
      "Epoch: 2097: Train loss: 4.6106462478637695 Valid loss: 4.071353435516357\n",
      "Epoch: 2098: Train loss: 4.625397682189941 Valid loss: 0.046470195055007935\n",
      "Epoch: 2099: Train loss: 4.60536003112793 Valid loss: 4.063437461853027\n",
      "Epoch: 2100: Train loss: 4.619913578033447 Valid loss: 0.04695811867713928\n",
      "Epoch: 2101: Train loss: 4.600082874298096 Valid loss: 4.055521011352539\n",
      "Epoch: 2102: Train loss: 4.614429950714111 Valid loss: 0.04744759202003479\n",
      "Epoch: 2103: Train loss: 4.5948076248168945 Valid loss: 4.047614097595215\n",
      "Epoch: 2104: Train loss: 4.60896110534668 Valid loss: 0.047939855605363846\n",
      "Epoch: 2105: Train loss: 4.589552402496338 Valid loss: 4.039729595184326\n",
      "Epoch: 2106: Train loss: 4.603509426116943 Valid loss: 0.04843525215983391\n",
      "Epoch: 2107: Train loss: 4.5843048095703125 Valid loss: 4.0318603515625\n",
      "Epoch: 2108: Train loss: 4.598063945770264 Valid loss: 0.04893163591623306\n",
      "Epoch: 2109: Train loss: 4.579069137573242 Valid loss: 4.023995876312256\n",
      "Epoch: 2110: Train loss: 4.592630386352539 Valid loss: 0.04943092167377472\n",
      "Epoch: 2111: Train loss: 4.573836803436279 Valid loss: 4.016146659851074\n",
      "Epoch: 2112: Train loss: 4.587208271026611 Valid loss: 0.049932949244976044\n",
      "Epoch: 2113: Train loss: 4.568624973297119 Valid loss: 4.008318901062012\n",
      "Epoch: 2114: Train loss: 4.58180046081543 Valid loss: 0.05043809860944748\n",
      "Epoch: 2115: Train loss: 4.563413143157959 Valid loss: 4.000502109527588\n",
      "Epoch: 2116: Train loss: 4.576397895812988 Valid loss: 0.0509442463517189\n",
      "Epoch: 2117: Train loss: 4.5582146644592285 Valid loss: 3.992689371109009\n",
      "Epoch: 2118: Train loss: 4.571005344390869 Valid loss: 0.051452554762363434\n",
      "Epoch: 2119: Train loss: 4.5530266761779785 Valid loss: 3.9848978519439697\n",
      "Epoch: 2120: Train loss: 4.565632343292236 Valid loss: 0.05196492373943329\n",
      "Epoch: 2121: Train loss: 4.547849655151367 Valid loss: 3.9771223068237305\n",
      "Epoch: 2122: Train loss: 4.560266017913818 Valid loss: 0.05247866362333298\n",
      "Epoch: 2123: Train loss: 4.542681694030762 Valid loss: 3.969346046447754\n",
      "Epoch: 2124: Train loss: 4.554914474487305 Valid loss: 0.0529952347278595\n",
      "Epoch: 2125: Train loss: 4.537530422210693 Valid loss: 3.9616072177886963\n",
      "Epoch: 2126: Train loss: 4.549575328826904 Valid loss: 0.05351413041353226\n",
      "Epoch: 2127: Train loss: 4.532385349273682 Valid loss: 3.953873634338379\n",
      "Epoch: 2128: Train loss: 4.54424524307251 Valid loss: 0.05403543636202812\n",
      "Epoch: 2129: Train loss: 4.527251243591309 Valid loss: 3.946155071258545\n",
      "Epoch: 2130: Train loss: 4.53892707824707 Valid loss: 0.05455920845270157\n",
      "Epoch: 2131: Train loss: 4.522129535675049 Valid loss: 3.9384524822235107\n",
      "Epoch: 2132: Train loss: 4.533624172210693 Valid loss: 0.05508583411574364\n",
      "Epoch: 2133: Train loss: 4.517011642456055 Valid loss: 3.9307546615600586\n",
      "Epoch: 2134: Train loss: 4.528318881988525 Valid loss: 0.05561389774084091\n",
      "Epoch: 2135: Train loss: 4.511900901794434 Valid loss: 3.9230730533599854\n",
      "Epoch: 2136: Train loss: 4.523029804229736 Valid loss: 0.056144826114177704\n",
      "Epoch: 2137: Train loss: 4.506803512573242 Valid loss: 3.915395498275757\n",
      "Epoch: 2138: Train loss: 4.517752647399902 Valid loss: 0.05667761340737343\n",
      "Epoch: 2139: Train loss: 4.501717567443848 Valid loss: 3.9077396392822266\n",
      "Epoch: 2140: Train loss: 4.512485980987549 Valid loss: 0.057212263345718384\n",
      "Epoch: 2141: Train loss: 4.496641159057617 Valid loss: 3.900099039077759\n",
      "Epoch: 2142: Train loss: 4.507238864898682 Valid loss: 0.05775110051035881\n",
      "Epoch: 2143: Train loss: 4.491580009460449 Valid loss: 3.892484426498413\n",
      "Epoch: 2144: Train loss: 4.5019965171813965 Valid loss: 0.05829162523150444\n",
      "Epoch: 2145: Train loss: 4.486520290374756 Valid loss: 3.8848648071289062\n",
      "Epoch: 2146: Train loss: 4.496761798858643 Valid loss: 0.05883412808179855\n",
      "Epoch: 2147: Train loss: 4.481473445892334 Valid loss: 3.8772659301757812\n",
      "Epoch: 2148: Train loss: 4.4915385246276855 Valid loss: 0.05937831103801727\n",
      "Epoch: 2149: Train loss: 4.476436138153076 Valid loss: 3.8696722984313965\n",
      "Epoch: 2150: Train loss: 4.486331462860107 Valid loss: 0.059926025569438934\n",
      "Epoch: 2151: Train loss: 4.471409320831299 Valid loss: 3.8620994091033936\n",
      "Epoch: 2152: Train loss: 4.481133460998535 Valid loss: 0.06047571077942848\n",
      "Epoch: 2153: Train loss: 4.466394424438477 Valid loss: 3.8545472621917725\n",
      "Epoch: 2154: Train loss: 4.475948333740234 Valid loss: 0.06102794036269188\n",
      "Epoch: 2155: Train loss: 4.461394309997559 Valid loss: 3.847005605697632\n",
      "Epoch: 2156: Train loss: 4.470781326293945 Valid loss: 0.06158333644270897\n",
      "Epoch: 2157: Train loss: 4.4564032554626465 Valid loss: 3.8394901752471924\n",
      "Epoch: 2158: Train loss: 4.465617656707764 Valid loss: 0.0621412917971611\n",
      "Epoch: 2159: Train loss: 4.451419353485107 Valid loss: 3.8319740295410156\n",
      "Epoch: 2160: Train loss: 4.460463047027588 Valid loss: 0.06270036846399307\n",
      "Epoch: 2161: Train loss: 4.44644021987915 Valid loss: 3.8244740962982178\n",
      "Epoch: 2162: Train loss: 4.455314636230469 Valid loss: 0.06326110661029816\n",
      "Epoch: 2163: Train loss: 4.441468715667725 Valid loss: 3.816978931427002\n",
      "Epoch: 2164: Train loss: 4.4501776695251465 Valid loss: 0.06382475048303604\n",
      "Epoch: 2165: Train loss: 4.436513423919678 Valid loss: 3.8094992637634277\n",
      "Epoch: 2166: Train loss: 4.4450578689575195 Valid loss: 0.06439138948917389\n",
      "Epoch: 2167: Train loss: 4.431570529937744 Valid loss: 3.8020455837249756\n",
      "Epoch: 2168: Train loss: 4.439950942993164 Valid loss: 0.06496050953865051\n",
      "Epoch: 2169: Train loss: 4.426633834838867 Valid loss: 3.7945973873138428\n",
      "Epoch: 2170: Train loss: 4.434856414794922 Valid loss: 0.06553243100643158\n",
      "Epoch: 2171: Train loss: 4.421712875366211 Valid loss: 3.7871694564819336\n",
      "Epoch: 2172: Train loss: 4.42977237701416 Valid loss: 0.06610657274723053\n",
      "Epoch: 2173: Train loss: 4.416800498962402 Valid loss: 3.7797625064849854\n",
      "Epoch: 2174: Train loss: 4.424696922302246 Valid loss: 0.06668241322040558\n",
      "Epoch: 2175: Train loss: 4.411893367767334 Valid loss: 3.772355318069458\n",
      "Epoch: 2176: Train loss: 4.41963005065918 Valid loss: 0.06726071238517761\n",
      "Epoch: 2177: Train loss: 4.406998157501221 Valid loss: 3.7649688720703125\n",
      "Epoch: 2178: Train loss: 4.41456937789917 Valid loss: 0.06783950328826904\n",
      "Epoch: 2179: Train loss: 4.402104377746582 Valid loss: 3.757582187652588\n",
      "Epoch: 2180: Train loss: 4.409518718719482 Valid loss: 0.0684223622083664\n",
      "Epoch: 2181: Train loss: 4.397222518920898 Valid loss: 3.750211000442505\n",
      "Epoch: 2182: Train loss: 4.404482364654541 Valid loss: 0.06900735199451447\n",
      "Epoch: 2183: Train loss: 4.392350673675537 Valid loss: 3.7428500652313232\n",
      "Epoch: 2184: Train loss: 4.399460315704346 Valid loss: 0.06959542632102966\n",
      "Epoch: 2185: Train loss: 4.387499809265137 Valid loss: 3.7355258464813232\n",
      "Epoch: 2186: Train loss: 4.394448757171631 Valid loss: 0.07018536329269409\n",
      "Epoch: 2187: Train loss: 4.382657527923584 Valid loss: 3.7282063961029053\n",
      "Epoch: 2188: Train loss: 4.38945198059082 Valid loss: 0.07077771425247192\n",
      "Epoch: 2189: Train loss: 4.377823352813721 Valid loss: 3.7209134101867676\n",
      "Epoch: 2190: Train loss: 4.384463310241699 Valid loss: 0.0713728740811348\n",
      "Epoch: 2191: Train loss: 4.37299108505249 Valid loss: 3.7136142253875732\n",
      "Epoch: 2192: Train loss: 4.379478454589844 Valid loss: 0.0719694197177887\n",
      "Epoch: 2193: Train loss: 4.368172645568848 Valid loss: 3.7063307762145996\n",
      "Epoch: 2194: Train loss: 4.374511241912842 Valid loss: 0.07256823778152466\n",
      "Epoch: 2195: Train loss: 4.363363265991211 Valid loss: 3.699068307876587\n",
      "Epoch: 2196: Train loss: 4.369550704956055 Valid loss: 0.07316958904266357\n",
      "Epoch: 2197: Train loss: 4.35856294631958 Valid loss: 3.691805601119995\n",
      "Epoch: 2198: Train loss: 4.364599227905273 Valid loss: 0.07377251982688904\n",
      "Epoch: 2199: Train loss: 4.353768348693848 Valid loss: 3.6845579147338867\n",
      "Epoch: 2200: Train loss: 4.3596577644348145 Valid loss: 0.07437853515148163\n",
      "Epoch: 2201: Train loss: 4.3489861488342285 Valid loss: 3.677320957183838\n",
      "Epoch: 2202: Train loss: 4.3547282218933105 Valid loss: 0.07498716562986374\n",
      "Epoch: 2203: Train loss: 4.344217300415039 Valid loss: 3.6701197624206543\n",
      "Epoch: 2204: Train loss: 4.349814414978027 Valid loss: 0.07559756934642792\n",
      "Epoch: 2205: Train loss: 4.3394598960876465 Valid loss: 3.662923812866211\n",
      "Epoch: 2206: Train loss: 4.344908237457275 Valid loss: 0.07621079683303833\n",
      "Epoch: 2207: Train loss: 4.334707260131836 Valid loss: 3.655742883682251\n",
      "Epoch: 2208: Train loss: 4.340009689331055 Valid loss: 0.07682613283395767\n",
      "Epoch: 2209: Train loss: 4.329963684082031 Valid loss: 3.6485676765441895\n",
      "Epoch: 2210: Train loss: 4.335124492645264 Valid loss: 0.0774434506893158\n",
      "Epoch: 2211: Train loss: 4.325230598449707 Valid loss: 3.6414127349853516\n",
      "Epoch: 2212: Train loss: 4.330244541168213 Valid loss: 0.07806302607059479\n",
      "Epoch: 2213: Train loss: 4.320504188537598 Valid loss: 3.6342575550079346\n",
      "Epoch: 2214: Train loss: 4.325381278991699 Valid loss: 0.07868527621030807\n",
      "Epoch: 2215: Train loss: 4.315793991088867 Valid loss: 3.62713360786438\n",
      "Epoch: 2216: Train loss: 4.320528984069824 Valid loss: 0.07931007444858551\n",
      "Epoch: 2217: Train loss: 4.311089515686035 Valid loss: 3.6200144290924072\n",
      "Epoch: 2218: Train loss: 4.315684795379639 Valid loss: 0.07993669807910919\n",
      "Epoch: 2219: Train loss: 4.306396961212158 Valid loss: 3.612905263900757\n",
      "Epoch: 2220: Train loss: 4.310850143432617 Valid loss: 0.08056499809026718\n",
      "Epoch: 2221: Train loss: 4.3017120361328125 Valid loss: 3.6058270931243896\n",
      "Epoch: 2222: Train loss: 4.306030750274658 Valid loss: 0.08119720220565796\n",
      "Epoch: 2223: Train loss: 4.297040939331055 Valid loss: 3.5987539291381836\n",
      "Epoch: 2224: Train loss: 4.301219463348389 Valid loss: 0.08183173835277557\n",
      "Epoch: 2225: Train loss: 4.292378902435303 Valid loss: 3.59169602394104\n",
      "Epoch: 2226: Train loss: 4.296422004699707 Valid loss: 0.08246764540672302\n",
      "Epoch: 2227: Train loss: 4.287724018096924 Valid loss: 3.584658622741699\n",
      "Epoch: 2228: Train loss: 4.291628360748291 Valid loss: 0.0831051617860794\n",
      "Epoch: 2229: Train loss: 4.28307580947876 Valid loss: 3.5776212215423584\n",
      "Epoch: 2230: Train loss: 4.286849498748779 Valid loss: 0.08374565839767456\n",
      "Epoch: 2231: Train loss: 4.278440475463867 Valid loss: 3.570604085922241\n",
      "Epoch: 2232: Train loss: 4.282077312469482 Valid loss: 0.08438801765441895\n",
      "Epoch: 2233: Train loss: 4.273813724517822 Valid loss: 3.5636024475097656\n",
      "Epoch: 2234: Train loss: 4.277315139770508 Valid loss: 0.08503277599811554\n",
      "Epoch: 2235: Train loss: 4.269189834594727 Valid loss: 3.5565953254699707\n",
      "Epoch: 2236: Train loss: 4.272562503814697 Valid loss: 0.0856795534491539\n",
      "Epoch: 2237: Train loss: 4.26458215713501 Valid loss: 3.549623966217041\n",
      "Epoch: 2238: Train loss: 4.267820358276367 Valid loss: 0.08632907271385193\n",
      "Epoch: 2239: Train loss: 4.259978294372559 Valid loss: 3.54264760017395\n",
      "Epoch: 2240: Train loss: 4.26308536529541 Valid loss: 0.08698070049285889\n",
      "Epoch: 2241: Train loss: 4.255385875701904 Valid loss: 3.535691738128662\n",
      "Epoch: 2242: Train loss: 4.258362293243408 Valid loss: 0.08763338625431061\n",
      "Epoch: 2243: Train loss: 4.250799179077148 Valid loss: 3.5287506580352783\n",
      "Epoch: 2244: Train loss: 4.253647327423096 Valid loss: 0.08828778564929962\n",
      "Epoch: 2245: Train loss: 4.246223449707031 Valid loss: 3.521820068359375\n",
      "Epoch: 2246: Train loss: 4.2489447593688965 Valid loss: 0.08894611150026321\n",
      "Epoch: 2247: Train loss: 4.24165678024292 Valid loss: 3.514894485473633\n",
      "Epoch: 2248: Train loss: 4.244251251220703 Valid loss: 0.08960568904876709\n",
      "Epoch: 2249: Train loss: 4.237100601196289 Valid loss: 3.5079941749572754\n",
      "Epoch: 2250: Train loss: 4.239569187164307 Valid loss: 0.09026819467544556\n",
      "Epoch: 2251: Train loss: 4.23255729675293 Valid loss: 3.5011143684387207\n",
      "Epoch: 2252: Train loss: 4.234900951385498 Valid loss: 0.09093427658081055\n",
      "Epoch: 2253: Train loss: 4.228024959564209 Valid loss: 3.494244337081909\n",
      "Epoch: 2254: Train loss: 4.2302422523498535 Valid loss: 0.09160220623016357\n",
      "Epoch: 2255: Train loss: 4.223503112792969 Valid loss: 3.48738956451416\n",
      "Epoch: 2256: Train loss: 4.225601673126221 Valid loss: 0.09227225929498672\n",
      "Epoch: 2257: Train loss: 4.218989849090576 Valid loss: 3.480560302734375\n",
      "Epoch: 2258: Train loss: 4.220964431762695 Valid loss: 0.09294390678405762\n",
      "Epoch: 2259: Train loss: 4.214481830596924 Valid loss: 3.4737207889556885\n",
      "Epoch: 2260: Train loss: 4.21633243560791 Valid loss: 0.09361648559570312\n",
      "Epoch: 2261: Train loss: 4.209985256195068 Valid loss: 3.4669113159179688\n",
      "Epoch: 2262: Train loss: 4.211715221405029 Valid loss: 0.09429366141557693\n",
      "Epoch: 2263: Train loss: 4.205501079559326 Valid loss: 3.4601118564605713\n",
      "Epoch: 2264: Train loss: 4.207108974456787 Valid loss: 0.09497160464525223\n",
      "Epoch: 2265: Train loss: 4.201019287109375 Valid loss: 3.4533281326293945\n",
      "Epoch: 2266: Train loss: 4.20250940322876 Valid loss: 0.09565315395593643\n",
      "Epoch: 2267: Train loss: 4.1965508460998535 Valid loss: 3.446553945541382\n",
      "Epoch: 2268: Train loss: 4.197919845581055 Valid loss: 0.09633437544107437\n",
      "Epoch: 2269: Train loss: 4.192080974578857 Valid loss: 3.439774990081787\n",
      "Epoch: 2270: Train loss: 4.193334579467773 Valid loss: 0.09701802581548691\n",
      "Epoch: 2271: Train loss: 4.187626361846924 Valid loss: 3.433021068572998\n",
      "Epoch: 2272: Train loss: 4.188757419586182 Valid loss: 0.09770447760820389\n",
      "Epoch: 2273: Train loss: 4.1831793785095215 Valid loss: 3.4262824058532715\n",
      "Epoch: 2274: Train loss: 4.184198379516602 Valid loss: 0.09839311242103577\n",
      "Epoch: 2275: Train loss: 4.178744792938232 Valid loss: 3.419558525085449\n",
      "Epoch: 2276: Train loss: 4.179647922515869 Valid loss: 0.09908433258533478\n",
      "Epoch: 2277: Train loss: 4.174322128295898 Valid loss: 3.4128499031066895\n",
      "Epoch: 2278: Train loss: 4.175110816955566 Valid loss: 0.09977903962135315\n",
      "Epoch: 2279: Train loss: 4.169905185699463 Valid loss: 3.406161308288574\n",
      "Epoch: 2280: Train loss: 4.1705780029296875 Valid loss: 0.10047384351491928\n",
      "Epoch: 2281: Train loss: 4.165494918823242 Valid loss: 3.399472713470459\n",
      "Epoch: 2282: Train loss: 4.1660566329956055 Valid loss: 0.10117147862911224\n",
      "Epoch: 2283: Train loss: 4.161096572875977 Valid loss: 3.3928089141845703\n",
      "Epoch: 2284: Train loss: 4.161543846130371 Valid loss: 0.1018713116645813\n",
      "Epoch: 2285: Train loss: 4.15670108795166 Valid loss: 3.38614559173584\n",
      "Epoch: 2286: Train loss: 4.157036304473877 Valid loss: 0.10257130861282349\n",
      "Epoch: 2287: Train loss: 4.152316570281982 Valid loss: 3.3794968128204346\n",
      "Epoch: 2288: Train loss: 4.152543544769287 Valid loss: 0.10327596962451935\n",
      "Epoch: 2289: Train loss: 4.147947311401367 Valid loss: 3.372863292694092\n",
      "Epoch: 2290: Train loss: 4.1480584144592285 Valid loss: 0.1039813831448555\n",
      "Epoch: 2291: Train loss: 4.143578052520752 Valid loss: 3.3662445545196533\n",
      "Epoch: 2292: Train loss: 4.143583297729492 Valid loss: 0.10468941926956177\n",
      "Epoch: 2293: Train loss: 4.139224529266357 Valid loss: 3.3596410751342773\n",
      "Epoch: 2294: Train loss: 4.139117240905762 Valid loss: 0.10540041327476501\n",
      "Epoch: 2295: Train loss: 4.13487434387207 Valid loss: 3.3530473709106445\n",
      "Epoch: 2296: Train loss: 4.134664058685303 Valid loss: 0.10611296445131302\n",
      "Epoch: 2297: Train loss: 4.13054084777832 Valid loss: 3.3464736938476562\n",
      "Epoch: 2298: Train loss: 4.130221843719482 Valid loss: 0.10682741552591324\n",
      "Epoch: 2299: Train loss: 4.126211166381836 Valid loss: 3.3399147987365723\n",
      "Epoch: 2300: Train loss: 4.125783920288086 Valid loss: 0.10754340887069702\n",
      "Epoch: 2301: Train loss: 4.12188720703125 Valid loss: 3.3333561420440674\n",
      "Epoch: 2302: Train loss: 4.121355056762695 Valid loss: 0.1082623153924942\n",
      "Epoch: 2303: Train loss: 4.1175737380981445 Valid loss: 3.3268074989318848\n",
      "Epoch: 2304: Train loss: 4.116935729980469 Valid loss: 0.10898247361183167\n",
      "Epoch: 2305: Train loss: 4.1132683753967285 Valid loss: 3.3202884197235107\n",
      "Epoch: 2306: Train loss: 4.11253023147583 Valid loss: 0.10970623046159744\n",
      "Epoch: 2307: Train loss: 4.108977317810059 Valid loss: 3.31377911567688\n",
      "Epoch: 2308: Train loss: 4.108133792877197 Valid loss: 0.11043109744787216\n",
      "Epoch: 2309: Train loss: 4.104694843292236 Valid loss: 3.3072850704193115\n",
      "Epoch: 2310: Train loss: 4.103747844696045 Valid loss: 0.1111585795879364\n",
      "Epoch: 2311: Train loss: 4.100419998168945 Valid loss: 3.3008005619049072\n",
      "Epoch: 2312: Train loss: 4.099370956420898 Valid loss: 0.11188767850399017\n",
      "Epoch: 2313: Train loss: 4.0961503982543945 Valid loss: 3.2943310737609863\n",
      "Epoch: 2314: Train loss: 4.094999313354492 Valid loss: 0.11261887103319168\n",
      "Epoch: 2315: Train loss: 4.091890335083008 Valid loss: 3.2878715991973877\n",
      "Epoch: 2316: Train loss: 4.090640544891357 Valid loss: 0.11335210502147675\n",
      "Epoch: 2317: Train loss: 4.087637901306152 Valid loss: 3.2814221382141113\n",
      "Epoch: 2318: Train loss: 4.086291790008545 Valid loss: 0.11408822238445282\n",
      "Epoch: 2319: Train loss: 4.083400726318359 Valid loss: 3.2750017642974854\n",
      "Epoch: 2320: Train loss: 4.0819501876831055 Valid loss: 0.11482612788677216\n",
      "Epoch: 2321: Train loss: 4.079168319702148 Valid loss: 3.2685768604278564\n",
      "Epoch: 2322: Train loss: 4.077624797821045 Valid loss: 0.115566685795784\n",
      "Epoch: 2323: Train loss: 4.074947357177734 Valid loss: 3.262181282043457\n",
      "Epoch: 2324: Train loss: 4.073304176330566 Valid loss: 0.11630786955356598\n",
      "Epoch: 2325: Train loss: 4.070730686187744 Valid loss: 3.255790948867798\n",
      "Epoch: 2326: Train loss: 4.068986892700195 Valid loss: 0.11705099791288376\n",
      "Epoch: 2327: Train loss: 4.0665202140808105 Valid loss: 3.2494051456451416\n",
      "Epoch: 2328: Train loss: 4.0646820068359375 Valid loss: 0.11779633909463882\n",
      "Epoch: 2329: Train loss: 4.062320709228516 Valid loss: 3.24303936958313\n",
      "Epoch: 2330: Train loss: 4.060384273529053 Valid loss: 0.11854375153779984\n",
      "Epoch: 2331: Train loss: 4.058125972747803 Valid loss: 3.2366783618927\n",
      "Epoch: 2332: Train loss: 4.056097030639648 Valid loss: 0.11929431557655334\n",
      "Epoch: 2333: Train loss: 4.053945064544678 Valid loss: 3.230332136154175\n",
      "Epoch: 2334: Train loss: 4.051821708679199 Valid loss: 0.12004468590021133\n",
      "Epoch: 2335: Train loss: 4.049768447875977 Valid loss: 3.224010705947876\n",
      "Epoch: 2336: Train loss: 4.047549724578857 Valid loss: 0.12079831212759018\n",
      "Epoch: 2337: Train loss: 4.045602798461914 Valid loss: 3.217689037322998\n",
      "Epoch: 2338: Train loss: 4.0432915687561035 Valid loss: 0.12155372649431229\n",
      "Epoch: 2339: Train loss: 4.041440963745117 Valid loss: 3.2113869190216064\n",
      "Epoch: 2340: Train loss: 4.039041519165039 Valid loss: 0.12231166660785675\n",
      "Epoch: 2341: Train loss: 4.037288665771484 Valid loss: 3.20509934425354\n",
      "Epoch: 2342: Train loss: 4.034796714782715 Valid loss: 0.1230693832039833\n",
      "Epoch: 2343: Train loss: 4.033149242401123 Valid loss: 3.19880747795105\n",
      "Epoch: 2344: Train loss: 4.030569076538086 Valid loss: 0.12383231520652771\n",
      "Epoch: 2345: Train loss: 4.029020309448242 Valid loss: 3.192554235458374\n",
      "Epoch: 2346: Train loss: 4.026348114013672 Valid loss: 0.12459652125835419\n",
      "Epoch: 2347: Train loss: 4.024896621704102 Valid loss: 3.1863009929656982\n",
      "Epoch: 2348: Train loss: 4.022134304046631 Valid loss: 0.12536226212978363\n",
      "Epoch: 2349: Train loss: 4.020782947540283 Valid loss: 3.180067300796509\n",
      "Epoch: 2350: Train loss: 4.017928123474121 Valid loss: 0.12612998485565186\n",
      "Epoch: 2351: Train loss: 4.016672134399414 Valid loss: 3.1738336086273193\n",
      "Epoch: 2352: Train loss: 4.013734817504883 Valid loss: 0.12689891457557678\n",
      "Epoch: 2353: Train loss: 4.012578964233398 Valid loss: 3.1676387786865234\n",
      "Epoch: 2354: Train loss: 4.009555816650391 Valid loss: 0.12767165899276733\n",
      "Epoch: 2355: Train loss: 4.008492469787598 Valid loss: 3.1614439487457275\n",
      "Epoch: 2356: Train loss: 4.0053791999816895 Valid loss: 0.12844446301460266\n",
      "Epoch: 2357: Train loss: 4.0044050216674805 Valid loss: 3.1552541255950928\n",
      "Epoch: 2358: Train loss: 4.001208305358887 Valid loss: 0.12921994924545288\n",
      "Epoch: 2359: Train loss: 4.000338554382324 Valid loss: 3.1490883827209473\n",
      "Epoch: 2360: Train loss: 3.997051239013672 Valid loss: 0.12999692559242249\n",
      "Epoch: 2361: Train loss: 3.9962704181671143 Valid loss: 3.142922878265381\n",
      "Epoch: 2362: Train loss: 3.9929003715515137 Valid loss: 0.13077646493911743\n",
      "Epoch: 2363: Train loss: 3.99221134185791 Valid loss: 3.1367719173431396\n",
      "Epoch: 2364: Train loss: 3.988760232925415 Valid loss: 0.1315591037273407\n",
      "Epoch: 2365: Train loss: 3.9881651401519775 Valid loss: 3.1306447982788086\n",
      "Epoch: 2366: Train loss: 3.984631061553955 Valid loss: 0.13234181702136993\n",
      "Epoch: 2367: Train loss: 3.9841248989105225 Valid loss: 3.1245276927948\n",
      "Epoch: 2368: Train loss: 3.9805052280426025 Valid loss: 0.13312646746635437\n",
      "Epoch: 2369: Train loss: 3.9800915718078613 Valid loss: 3.118410587310791\n",
      "Epoch: 2370: Train loss: 3.9763896465301514 Valid loss: 0.13391360640525818\n",
      "Epoch: 2371: Train loss: 3.9760661125183105 Valid loss: 3.1123223304748535\n",
      "Epoch: 2372: Train loss: 3.972278594970703 Valid loss: 0.1347009241580963\n",
      "Epoch: 2373: Train loss: 3.9720449447631836 Valid loss: 3.106234312057495\n",
      "Epoch: 2374: Train loss: 3.9681832790374756 Valid loss: 0.1354924589395523\n",
      "Epoch: 2375: Train loss: 3.9680376052856445 Valid loss: 3.1001510620117188\n",
      "Epoch: 2376: Train loss: 3.96409273147583 Valid loss: 0.13628607988357544\n",
      "Epoch: 2377: Train loss: 3.964038372039795 Valid loss: 3.0940966606140137\n",
      "Epoch: 2378: Train loss: 3.9600131511688232 Valid loss: 0.13707925379276276\n",
      "Epoch: 2379: Train loss: 3.960047483444214 Valid loss: 3.0880515575408936\n",
      "Epoch: 2380: Train loss: 3.955944061279297 Valid loss: 0.13787725567817688\n",
      "Epoch: 2381: Train loss: 3.9560635089874268 Valid loss: 3.0820260047912598\n",
      "Epoch: 2382: Train loss: 3.95188307762146 Valid loss: 0.13867463171482086\n",
      "Epoch: 2383: Train loss: 3.952086925506592 Valid loss: 3.076005220413208\n",
      "Epoch: 2384: Train loss: 3.9478323459625244 Valid loss: 0.13947544991970062\n",
      "Epoch: 2385: Train loss: 3.9481234550476074 Valid loss: 3.0700037479400635\n",
      "Epoch: 2386: Train loss: 3.9437875747680664 Valid loss: 0.1402783840894699\n",
      "Epoch: 2387: Train loss: 3.9441616535186768 Valid loss: 3.064011335372925\n",
      "Epoch: 2388: Train loss: 3.939751148223877 Valid loss: 0.14108198881149292\n",
      "Epoch: 2389: Train loss: 3.940211057662964 Valid loss: 3.0580289363861084\n",
      "Epoch: 2390: Train loss: 3.935723304748535 Valid loss: 0.14188793301582336\n",
      "Epoch: 2391: Train loss: 3.9362668991088867 Valid loss: 3.05206561088562\n",
      "Epoch: 2392: Train loss: 3.9317049980163574 Valid loss: 0.14269524812698364\n",
      "Epoch: 2393: Train loss: 3.932331085205078 Valid loss: 3.046111822128296\n",
      "Epoch: 2394: Train loss: 3.92769455909729 Valid loss: 0.1435065120458603\n",
      "Epoch: 2395: Train loss: 3.9284067153930664 Valid loss: 3.0401771068573\n",
      "Epoch: 2396: Train loss: 3.9236950874328613 Valid loss: 0.14431804418563843\n",
      "Epoch: 2397: Train loss: 3.9244868755340576 Valid loss: 3.0342328548431396\n",
      "Epoch: 2398: Train loss: 3.9197006225585938 Valid loss: 0.14513063430786133\n",
      "Epoch: 2399: Train loss: 3.9205751419067383 Valid loss: 3.0283219814300537\n",
      "Epoch: 2400: Train loss: 3.9157137870788574 Valid loss: 0.14594534039497375\n",
      "Epoch: 2401: Train loss: 3.916670083999634 Valid loss: 3.02241587638855\n",
      "Epoch: 2402: Train loss: 3.911736488342285 Valid loss: 0.1467609703540802\n",
      "Epoch: 2403: Train loss: 3.9127697944641113 Valid loss: 3.01651930809021\n",
      "Epoch: 2404: Train loss: 3.907766580581665 Valid loss: 0.14757943153381348\n",
      "Epoch: 2405: Train loss: 3.908884286880493 Valid loss: 3.010641574859619\n",
      "Epoch: 2406: Train loss: 3.903811454772949 Valid loss: 0.148401141166687\n",
      "Epoch: 2407: Train loss: 3.90500807762146 Valid loss: 3.0047733783721924\n",
      "Epoch: 2408: Train loss: 3.8998632431030273 Valid loss: 0.14922502636909485\n",
      "Epoch: 2409: Train loss: 3.9011404514312744 Valid loss: 2.9989335536956787\n",
      "Epoch: 2410: Train loss: 3.8959240913391113 Valid loss: 0.15004903078079224\n",
      "Epoch: 2411: Train loss: 3.897272825241089 Valid loss: 2.99308443069458\n",
      "Epoch: 2412: Train loss: 3.8919858932495117 Valid loss: 0.15087302029132843\n",
      "Epoch: 2413: Train loss: 3.8934109210968018 Valid loss: 2.9872496128082275\n",
      "Epoch: 2414: Train loss: 3.8880527019500732 Valid loss: 0.15170012414455414\n",
      "Epoch: 2415: Train loss: 3.8895552158355713 Valid loss: 2.981424331665039\n",
      "Epoch: 2416: Train loss: 3.884136199951172 Valid loss: 0.1525287926197052\n",
      "Epoch: 2417: Train loss: 3.885714292526245 Valid loss: 2.9756131172180176\n",
      "Epoch: 2418: Train loss: 3.8802266120910645 Valid loss: 0.15335983037948608\n",
      "Epoch: 2419: Train loss: 3.881885051727295 Valid loss: 2.969825267791748\n",
      "Epoch: 2420: Train loss: 3.876329183578491 Valid loss: 0.15419413149356842\n",
      "Epoch: 2421: Train loss: 3.8780572414398193 Valid loss: 2.9640467166900635\n",
      "Epoch: 2422: Train loss: 3.8724348545074463 Valid loss: 0.15502911806106567\n",
      "Epoch: 2423: Train loss: 3.874239683151245 Valid loss: 2.958282470703125\n",
      "Epoch: 2424: Train loss: 3.86855411529541 Valid loss: 0.15586607158184052\n",
      "Epoch: 2425: Train loss: 3.870436429977417 Valid loss: 2.9525325298309326\n",
      "Epoch: 2426: Train loss: 3.864684581756592 Valid loss: 0.1567050963640213\n",
      "Epoch: 2427: Train loss: 3.86663818359375 Valid loss: 2.94680118560791\n",
      "Epoch: 2428: Train loss: 3.86081862449646 Valid loss: 0.15754534304141998\n",
      "Epoch: 2429: Train loss: 3.862847328186035 Valid loss: 2.9410743713378906\n",
      "Epoch: 2430: Train loss: 3.856966972351074 Valid loss: 0.1583891361951828\n",
      "Epoch: 2431: Train loss: 3.8590643405914307 Valid loss: 2.935375452041626\n",
      "Epoch: 2432: Train loss: 3.853118419647217 Valid loss: 0.1592317819595337\n",
      "Epoch: 2433: Train loss: 3.8552920818328857 Valid loss: 2.9296722412109375\n",
      "Epoch: 2434: Train loss: 3.8492825031280518 Valid loss: 0.16007907688617706\n",
      "Epoch: 2435: Train loss: 3.85152268409729 Valid loss: 2.923992156982422\n",
      "Epoch: 2436: Train loss: 3.8454535007476807 Valid loss: 0.16092723608016968\n",
      "Epoch: 2437: Train loss: 3.8477630615234375 Valid loss: 2.918316602706909\n",
      "Epoch: 2438: Train loss: 3.841627359390259 Valid loss: 0.16177497804164886\n",
      "Epoch: 2439: Train loss: 3.8440065383911133 Valid loss: 2.9126553535461426\n",
      "Epoch: 2440: Train loss: 3.8378093242645264 Valid loss: 0.16262486577033997\n",
      "Epoch: 2441: Train loss: 3.8402562141418457 Valid loss: 2.906989574432373\n",
      "Epoch: 2442: Train loss: 3.8339967727661133 Valid loss: 0.16347581148147583\n",
      "Epoch: 2443: Train loss: 3.836513042449951 Valid loss: 2.9013514518737793\n",
      "Epoch: 2444: Train loss: 3.830193519592285 Valid loss: 0.164328932762146\n",
      "Epoch: 2445: Train loss: 3.832777976989746 Valid loss: 2.8957183361053467\n",
      "Epoch: 2446: Train loss: 3.8263964653015137 Valid loss: 0.16518428921699524\n",
      "Epoch: 2447: Train loss: 3.8290514945983887 Valid loss: 2.890103340148926\n",
      "Epoch: 2448: Train loss: 3.822614908218384 Valid loss: 0.1660410314798355\n",
      "Epoch: 2449: Train loss: 3.82533597946167 Valid loss: 2.884493350982666\n",
      "Epoch: 2450: Train loss: 3.8188366889953613 Valid loss: 0.16689829528331757\n",
      "Epoch: 2451: Train loss: 3.821624755859375 Valid loss: 2.878901720046997\n",
      "Epoch: 2452: Train loss: 3.815066337585449 Valid loss: 0.1677599847316742\n",
      "Epoch: 2453: Train loss: 3.8179190158843994 Valid loss: 2.873323917388916\n",
      "Epoch: 2454: Train loss: 3.811305046081543 Valid loss: 0.16862018406391144\n",
      "Epoch: 2455: Train loss: 3.8142213821411133 Valid loss: 2.8677420616149902\n",
      "Epoch: 2456: Train loss: 3.8075437545776367 Valid loss: 0.16948340833187103\n",
      "Epoch: 2457: Train loss: 3.810532569885254 Valid loss: 2.862191915512085\n",
      "Epoch: 2458: Train loss: 3.803798198699951 Valid loss: 0.17034831643104553\n",
      "Epoch: 2459: Train loss: 3.8068466186523438 Valid loss: 2.8566372394561768\n",
      "Epoch: 2460: Train loss: 3.8000564575195312 Valid loss: 0.17121294140815735\n",
      "Epoch: 2461: Train loss: 3.803168773651123 Valid loss: 2.8510968685150146\n",
      "Epoch: 2462: Train loss: 3.7963294982910156 Valid loss: 0.17208099365234375\n",
      "Epoch: 2463: Train loss: 3.7995011806488037 Valid loss: 2.845574378967285\n",
      "Epoch: 2464: Train loss: 3.792603015899658 Valid loss: 0.17294996976852417\n",
      "Epoch: 2465: Train loss: 3.795841693878174 Valid loss: 2.8400614261627197\n",
      "Epoch: 2466: Train loss: 3.788893461227417 Valid loss: 0.17382171750068665\n",
      "Epoch: 2467: Train loss: 3.7921926975250244 Valid loss: 2.834566593170166\n",
      "Epoch: 2468: Train loss: 3.785186767578125 Valid loss: 0.17469581961631775\n",
      "Epoch: 2469: Train loss: 3.78855037689209 Valid loss: 2.8290903568267822\n",
      "Epoch: 2470: Train loss: 3.78149151802063 Valid loss: 0.17557048797607422\n",
      "Epoch: 2471: Train loss: 3.7849159240722656 Valid loss: 2.823622941970825\n",
      "Epoch: 2472: Train loss: 3.7778048515319824 Valid loss: 0.1764478087425232\n",
      "Epoch: 2473: Train loss: 3.7812860012054443 Valid loss: 2.818160057067871\n",
      "Epoch: 2474: Train loss: 3.774122953414917 Valid loss: 0.1773250252008438\n",
      "Epoch: 2475: Train loss: 3.777669668197632 Valid loss: 2.812725067138672\n",
      "Epoch: 2476: Train loss: 3.770453453063965 Valid loss: 0.17820614576339722\n",
      "Epoch: 2477: Train loss: 3.7740588188171387 Valid loss: 2.8073031902313232\n",
      "Epoch: 2478: Train loss: 3.766789436340332 Valid loss: 0.17908737063407898\n",
      "Epoch: 2479: Train loss: 3.7704555988311768 Valid loss: 2.8018860816955566\n",
      "Epoch: 2480: Train loss: 3.763132333755493 Valid loss: 0.17997068166732788\n",
      "Epoch: 2481: Train loss: 3.766855001449585 Valid loss: 2.796469211578369\n",
      "Epoch: 2482: Train loss: 3.7594807147979736 Valid loss: 0.1808539628982544\n",
      "Epoch: 2483: Train loss: 3.763258934020996 Valid loss: 2.7910659313201904\n",
      "Epoch: 2484: Train loss: 3.755836009979248 Valid loss: 0.1817372739315033\n",
      "Epoch: 2485: Train loss: 3.7596757411956787 Valid loss: 2.7856853008270264\n",
      "Epoch: 2486: Train loss: 3.752202033996582 Valid loss: 0.18262584507465363\n",
      "Epoch: 2487: Train loss: 3.756099224090576 Valid loss: 2.780308961868286\n",
      "Epoch: 2488: Train loss: 3.74857497215271 Valid loss: 0.18351462483406067\n",
      "Epoch: 2489: Train loss: 3.752530813217163 Valid loss: 2.7749557495117188\n",
      "Epoch: 2490: Train loss: 3.74495792388916 Valid loss: 0.18440616130828857\n",
      "Epoch: 2491: Train loss: 3.748967409133911 Valid loss: 2.769606590270996\n",
      "Epoch: 2492: Train loss: 3.7413454055786133 Valid loss: 0.18529658019542694\n",
      "Epoch: 2493: Train loss: 3.7454092502593994 Valid loss: 2.7642712593078613\n",
      "Epoch: 2494: Train loss: 3.737741231918335 Valid loss: 0.18618932366371155\n",
      "Epoch: 2495: Train loss: 3.741861581802368 Valid loss: 2.7589447498321533\n",
      "Epoch: 2496: Train loss: 3.734145164489746 Valid loss: 0.18708471953868866\n",
      "Epoch: 2497: Train loss: 3.7383244037628174 Valid loss: 2.7536277770996094\n",
      "Epoch: 2498: Train loss: 3.730557680130005 Valid loss: 0.1879812777042389\n",
      "Epoch: 2499: Train loss: 3.7347917556762695 Valid loss: 2.7483415603637695\n",
      "Epoch: 2500: Train loss: 3.726984977722168 Valid loss: 0.18887992203235626\n",
      "Epoch: 2501: Train loss: 3.7312681674957275 Valid loss: 2.743055582046509\n",
      "Epoch: 2502: Train loss: 3.7234086990356445 Valid loss: 0.189779132604599\n",
      "Epoch: 2503: Train loss: 3.7277441024780273 Valid loss: 2.737774610519409\n",
      "Epoch: 2504: Train loss: 3.7198405265808105 Valid loss: 0.1906764805316925\n",
      "Epoch: 2505: Train loss: 3.7242279052734375 Valid loss: 2.7325069904327393\n",
      "Epoch: 2506: Train loss: 3.7162768840789795 Valid loss: 0.19158053398132324\n",
      "Epoch: 2507: Train loss: 3.7207212448120117 Valid loss: 2.727247953414917\n",
      "Epoch: 2508: Train loss: 3.7127249240875244 Valid loss: 0.1924826055765152\n",
      "Epoch: 2509: Train loss: 3.717219829559326 Valid loss: 2.7219979763031006\n",
      "Epoch: 2510: Train loss: 3.7091801166534424 Valid loss: 0.19338718056678772\n",
      "Epoch: 2511: Train loss: 3.713724374771118 Valid loss: 2.716761589050293\n",
      "Epoch: 2512: Train loss: 3.7056427001953125 Valid loss: 0.19429287314414978\n",
      "Epoch: 2513: Train loss: 3.7102415561676025 Valid loss: 2.711547613143921\n",
      "Epoch: 2514: Train loss: 3.702115058898926 Valid loss: 0.19520147144794464\n",
      "Epoch: 2515: Train loss: 3.7067642211914062 Valid loss: 2.7063379287719727\n",
      "Epoch: 2516: Train loss: 3.6985936164855957 Valid loss: 0.19611001014709473\n",
      "Epoch: 2517: Train loss: 3.7032923698425293 Valid loss: 2.701146125793457\n",
      "Epoch: 2518: Train loss: 3.6950809955596924 Valid loss: 0.19701936841011047\n",
      "Epoch: 2519: Train loss: 3.699833393096924 Valid loss: 2.695967197418213\n",
      "Epoch: 2520: Train loss: 3.691579580307007 Valid loss: 0.19793280959129333\n",
      "Epoch: 2521: Train loss: 3.6963796615600586 Valid loss: 2.6908066272735596\n",
      "Epoch: 2522: Train loss: 3.6880879402160645 Valid loss: 0.19884756207466125\n",
      "Epoch: 2523: Train loss: 3.692938804626465 Valid loss: 2.685659170150757\n",
      "Epoch: 2524: Train loss: 3.6846020221710205 Valid loss: 0.199762225151062\n",
      "Epoch: 2525: Train loss: 3.6894984245300293 Valid loss: 2.6805248260498047\n",
      "Epoch: 2526: Train loss: 3.6811206340789795 Valid loss: 0.20068007707595825\n",
      "Epoch: 2527: Train loss: 3.686063051223755 Valid loss: 2.6753814220428467\n",
      "Epoch: 2528: Train loss: 3.6776440143585205 Valid loss: 0.20159706473350525\n",
      "Epoch: 2529: Train loss: 3.6826438903808594 Valid loss: 2.670278310775757\n",
      "Epoch: 2530: Train loss: 3.6741840839385986 Valid loss: 0.20251742005348206\n",
      "Epoch: 2531: Train loss: 3.6792211532592773 Valid loss: 2.665170192718506\n",
      "Epoch: 2532: Train loss: 3.670722484588623 Valid loss: 0.2034376561641693\n",
      "Epoch: 2533: Train loss: 3.675807476043701 Valid loss: 2.660071611404419\n",
      "Epoch: 2534: Train loss: 3.667269468307495 Valid loss: 0.20435920357704163\n",
      "Epoch: 2535: Train loss: 3.6724019050598145 Valid loss: 2.6549859046936035\n",
      "Epoch: 2536: Train loss: 3.663822650909424 Valid loss: 0.20528221130371094\n",
      "Epoch: 2537: Train loss: 3.6689999103546143 Valid loss: 2.6499133110046387\n",
      "Epoch: 2538: Train loss: 3.660386323928833 Valid loss: 0.2062063068151474\n",
      "Epoch: 2539: Train loss: 3.6656084060668945 Valid loss: 2.6448497772216797\n",
      "Epoch: 2540: Train loss: 3.656951427459717 Valid loss: 0.20713019371032715\n",
      "Epoch: 2541: Train loss: 3.662219524383545 Valid loss: 2.639799118041992\n",
      "Epoch: 2542: Train loss: 3.653529644012451 Valid loss: 0.2080581784248352\n",
      "Epoch: 2543: Train loss: 3.6588380336761475 Valid loss: 2.6347572803497314\n",
      "Epoch: 2544: Train loss: 3.65010929107666 Valid loss: 0.20898635685443878\n",
      "Epoch: 2545: Train loss: 3.6554653644561768 Valid loss: 2.6297240257263184\n",
      "Epoch: 2546: Train loss: 3.646702766418457 Valid loss: 0.20991548895835876\n",
      "Epoch: 2547: Train loss: 3.652104616165161 Valid loss: 2.6247169971466064\n",
      "Epoch: 2548: Train loss: 3.6433017253875732 Valid loss: 0.21084660291671753\n",
      "Epoch: 2549: Train loss: 3.6487436294555664 Valid loss: 2.61970591545105\n",
      "Epoch: 2550: Train loss: 3.639904022216797 Valid loss: 0.2117781639099121\n",
      "Epoch: 2551: Train loss: 3.645392417907715 Valid loss: 2.614720582962036\n",
      "Epoch: 2552: Train loss: 3.636519432067871 Valid loss: 0.21271252632141113\n",
      "Epoch: 2553: Train loss: 3.6420483589172363 Valid loss: 2.6097397804260254\n",
      "Epoch: 2554: Train loss: 3.6331400871276855 Valid loss: 0.2136465311050415\n",
      "Epoch: 2555: Train loss: 3.6387133598327637 Valid loss: 2.604776620864868\n",
      "Epoch: 2556: Train loss: 3.6297683715820312 Valid loss: 0.2145828902721405\n",
      "Epoch: 2557: Train loss: 3.635383129119873 Valid loss: 2.5998215675354004\n",
      "Epoch: 2558: Train loss: 3.6264047622680664 Valid loss: 0.21552011370658875\n",
      "Epoch: 2559: Train loss: 3.6320583820343018 Valid loss: 2.5948798656463623\n",
      "Epoch: 2560: Train loss: 3.623047351837158 Valid loss: 0.21645915508270264\n",
      "Epoch: 2561: Train loss: 3.62874698638916 Valid loss: 2.5899548530578613\n",
      "Epoch: 2562: Train loss: 3.619701862335205 Valid loss: 0.21739894151687622\n",
      "Epoch: 2563: Train loss: 3.6254358291625977 Valid loss: 2.585043430328369\n",
      "Epoch: 2564: Train loss: 3.6163570880889893 Valid loss: 0.2183409184217453\n",
      "Epoch: 2565: Train loss: 3.6221346855163574 Valid loss: 2.5801234245300293\n",
      "Epoch: 2566: Train loss: 3.613023519515991 Valid loss: 0.2192835658788681\n",
      "Epoch: 2567: Train loss: 3.618837594985962 Valid loss: 2.575233221054077\n",
      "Epoch: 2568: Train loss: 3.609697103500366 Valid loss: 0.22022749483585358\n",
      "Epoch: 2569: Train loss: 3.615550994873047 Valid loss: 2.5703561305999756\n",
      "Epoch: 2570: Train loss: 3.6063718795776367 Valid loss: 0.22117117047309875\n",
      "Epoch: 2571: Train loss: 3.61226487159729 Valid loss: 2.5654749870300293\n",
      "Epoch: 2572: Train loss: 3.603055238723755 Valid loss: 0.22211626172065735\n",
      "Epoch: 2573: Train loss: 3.60898494720459 Valid loss: 2.5606110095977783\n",
      "Epoch: 2574: Train loss: 3.5997462272644043 Valid loss: 0.22306227684020996\n",
      "Epoch: 2575: Train loss: 3.6057138442993164 Valid loss: 2.5557589530944824\n",
      "Epoch: 2576: Train loss: 3.596444845199585 Valid loss: 0.22401063144207\n",
      "Epoch: 2577: Train loss: 3.602452516555786 Valid loss: 2.5509161949157715\n",
      "Epoch: 2578: Train loss: 3.593153953552246 Valid loss: 0.22495964169502258\n",
      "Epoch: 2579: Train loss: 3.599196195602417 Valid loss: 2.5460944175720215\n",
      "Epoch: 2580: Train loss: 3.5898661613464355 Valid loss: 0.22591131925582886\n",
      "Epoch: 2581: Train loss: 3.5959489345550537 Valid loss: 2.541285753250122\n",
      "Epoch: 2582: Train loss: 3.5865864753723145 Valid loss: 0.22686301171779633\n",
      "Epoch: 2583: Train loss: 3.592705726623535 Valid loss: 2.5364809036254883\n",
      "Epoch: 2584: Train loss: 3.583317995071411 Valid loss: 0.22781571745872498\n",
      "Epoch: 2585: Train loss: 3.589473247528076 Valid loss: 2.5316977500915527\n",
      "Epoch: 2586: Train loss: 3.5800561904907227 Valid loss: 0.22877107560634613\n",
      "Epoch: 2587: Train loss: 3.5862486362457275 Valid loss: 2.526918888092041\n",
      "Epoch: 2588: Train loss: 3.5767996311187744 Valid loss: 0.2297285497188568\n",
      "Epoch: 2589: Train loss: 3.583026885986328 Valid loss: 2.5221524238586426\n",
      "Epoch: 2590: Train loss: 3.573550224304199 Valid loss: 0.23068444430828094\n",
      "Epoch: 2591: Train loss: 3.5798122882843018 Valid loss: 2.5173990726470947\n",
      "Epoch: 2592: Train loss: 3.5703060626983643 Valid loss: 0.23164087533950806\n",
      "Epoch: 2593: Train loss: 3.576603889465332 Valid loss: 2.512657642364502\n",
      "Epoch: 2594: Train loss: 3.567068338394165 Valid loss: 0.23259878158569336\n",
      "Epoch: 2595: Train loss: 3.5734007358551025 Valid loss: 2.507925271987915\n",
      "Epoch: 2596: Train loss: 3.56384015083313 Valid loss: 0.23356008529663086\n",
      "Epoch: 2597: Train loss: 3.570204496383667 Valid loss: 2.5032050609588623\n",
      "Epoch: 2598: Train loss: 3.5606164932250977 Valid loss: 0.23452100157737732\n",
      "Epoch: 2599: Train loss: 3.567013740539551 Valid loss: 2.4984934329986572\n",
      "Epoch: 2600: Train loss: 3.5573976039886475 Valid loss: 0.23548319935798645\n",
      "Epoch: 2601: Train loss: 3.563828706741333 Valid loss: 2.4937903881073\n",
      "Epoch: 2602: Train loss: 3.554191827774048 Valid loss: 0.23644733428955078\n",
      "Epoch: 2603: Train loss: 3.560657501220703 Valid loss: 2.489112138748169\n",
      "Epoch: 2604: Train loss: 3.5509941577911377 Valid loss: 0.23741114139556885\n",
      "Epoch: 2605: Train loss: 3.5574917793273926 Valid loss: 2.484442949295044\n",
      "Epoch: 2606: Train loss: 3.5477960109710693 Valid loss: 0.2383766770362854\n",
      "Epoch: 2607: Train loss: 3.5543229579925537 Valid loss: 2.4797728061676025\n",
      "Epoch: 2608: Train loss: 3.5446085929870605 Valid loss: 0.23934361338615417\n",
      "Epoch: 2609: Train loss: 3.5511674880981445 Valid loss: 2.4751198291778564\n",
      "Epoch: 2610: Train loss: 3.5414278507232666 Valid loss: 0.2403104603290558\n",
      "Epoch: 2611: Train loss: 3.548022508621216 Valid loss: 2.470479726791382\n",
      "Epoch: 2612: Train loss: 3.5382564067840576 Valid loss: 0.24127966165542603\n",
      "Epoch: 2613: Train loss: 3.5448813438415527 Valid loss: 2.465859889984131\n",
      "Epoch: 2614: Train loss: 3.535087823867798 Valid loss: 0.24224969744682312\n",
      "Epoch: 2615: Train loss: 3.5417439937591553 Valid loss: 2.461236000061035\n",
      "Epoch: 2616: Train loss: 3.5319278240203857 Valid loss: 0.24321970343589783\n",
      "Epoch: 2617: Train loss: 3.5386102199554443 Valid loss: 2.456637382507324\n",
      "Epoch: 2618: Train loss: 3.528773307800293 Valid loss: 0.24419227242469788\n",
      "Epoch: 2619: Train loss: 3.5354905128479004 Valid loss: 2.4520423412323\n",
      "Epoch: 2620: Train loss: 3.5256309509277344 Valid loss: 0.24516582489013672\n",
      "Epoch: 2621: Train loss: 3.532374858856201 Valid loss: 2.4474682807922363\n",
      "Epoch: 2622: Train loss: 3.5224921703338623 Valid loss: 0.24613860249519348\n",
      "Epoch: 2623: Train loss: 3.529266834259033 Valid loss: 2.442898750305176\n",
      "Epoch: 2624: Train loss: 3.5193583965301514 Valid loss: 0.2471131980419159\n",
      "Epoch: 2625: Train loss: 3.526160717010498 Valid loss: 2.438328742980957\n",
      "Epoch: 2626: Train loss: 3.516231060028076 Valid loss: 0.24808929860591888\n",
      "Epoch: 2627: Train loss: 3.5230610370635986 Valid loss: 2.4337918758392334\n",
      "Epoch: 2628: Train loss: 3.513110399246216 Valid loss: 0.24906525015830994\n",
      "Epoch: 2629: Train loss: 3.5199711322784424 Valid loss: 2.429246187210083\n",
      "Epoch: 2630: Train loss: 3.509998083114624 Valid loss: 0.2500445544719696\n",
      "Epoch: 2631: Train loss: 3.5168871879577637 Valid loss: 2.4247171878814697\n",
      "Epoch: 2632: Train loss: 3.506889581680298 Valid loss: 0.2510216236114502\n",
      "Epoch: 2633: Train loss: 3.5138022899627686 Valid loss: 2.420196294784546\n",
      "Epoch: 2634: Train loss: 3.503787040710449 Valid loss: 0.25200051069259644\n",
      "Epoch: 2635: Train loss: 3.510727882385254 Valid loss: 2.4156877994537354\n",
      "Epoch: 2636: Train loss: 3.5006930828094482 Valid loss: 0.25298118591308594\n",
      "Epoch: 2637: Train loss: 3.507662773132324 Valid loss: 2.411191940307617\n",
      "Epoch: 2638: Train loss: 3.4976089000701904 Valid loss: 0.253962904214859\n",
      "Epoch: 2639: Train loss: 3.504605531692505 Valid loss: 2.4067206382751465\n",
      "Epoch: 2640: Train loss: 3.4945321083068848 Valid loss: 0.2549460232257843\n",
      "Epoch: 2641: Train loss: 3.5015530586242676 Valid loss: 2.4022443294525146\n",
      "Epoch: 2642: Train loss: 3.491460084915161 Valid loss: 0.2559303343296051\n",
      "Epoch: 2643: Train loss: 3.4985127449035645 Valid loss: 2.3978049755096436\n",
      "Epoch: 2644: Train loss: 3.488400459289551 Valid loss: 0.25691744685173035\n",
      "Epoch: 2645: Train loss: 3.4954824447631836 Valid loss: 2.393374443054199\n",
      "Epoch: 2646: Train loss: 3.485346555709839 Valid loss: 0.2579025626182556\n",
      "Epoch: 2647: Train loss: 3.4924471378326416 Valid loss: 2.388939142227173\n",
      "Epoch: 2648: Train loss: 3.4822933673858643 Valid loss: 0.2588903307914734\n",
      "Epoch: 2649: Train loss: 3.489420175552368 Valid loss: 2.3845160007476807\n",
      "Epoch: 2650: Train loss: 3.4792487621307373 Valid loss: 0.2598751187324524\n",
      "Epoch: 2651: Train loss: 3.486396551132202 Valid loss: 2.3801093101501465\n",
      "Epoch: 2652: Train loss: 3.4762065410614014 Valid loss: 0.2608625888824463\n",
      "Epoch: 2653: Train loss: 3.483382225036621 Valid loss: 2.3756983280181885\n",
      "Epoch: 2654: Train loss: 3.473174571990967 Valid loss: 0.2618520259857178\n",
      "Epoch: 2655: Train loss: 3.4803695678710938 Valid loss: 2.3712990283966064\n",
      "Epoch: 2656: Train loss: 3.4701452255249023 Valid loss: 0.26284050941467285\n",
      "Epoch: 2657: Train loss: 3.4773683547973633 Valid loss: 2.3669281005859375\n",
      "Epoch: 2658: Train loss: 3.467127799987793 Valid loss: 0.2638319432735443\n",
      "Epoch: 2659: Train loss: 3.4743752479553223 Valid loss: 2.362569570541382\n",
      "Epoch: 2660: Train loss: 3.4641196727752686 Valid loss: 0.2648245692253113\n",
      "Epoch: 2661: Train loss: 3.4713878631591797 Valid loss: 2.3582067489624023\n",
      "Epoch: 2662: Train loss: 3.461113929748535 Valid loss: 0.2658173143863678\n",
      "Epoch: 2663: Train loss: 3.46840763092041 Valid loss: 2.353868007659912\n",
      "Epoch: 2664: Train loss: 3.4581165313720703 Valid loss: 0.2668129801750183\n",
      "Epoch: 2665: Train loss: 3.465435266494751 Valid loss: 2.349545478820801\n",
      "Epoch: 2666: Train loss: 3.4551279544830322 Valid loss: 0.26780757308006287\n",
      "Epoch: 2667: Train loss: 3.4624667167663574 Valid loss: 2.345226287841797\n",
      "Epoch: 2668: Train loss: 3.4521431922912598 Valid loss: 0.268802285194397\n",
      "Epoch: 2669: Train loss: 3.4595069885253906 Valid loss: 2.3409197330474854\n",
      "Epoch: 2670: Train loss: 3.449169635772705 Valid loss: 0.269800066947937\n",
      "Epoch: 2671: Train loss: 3.4565558433532715 Valid loss: 2.336632490158081\n",
      "Epoch: 2672: Train loss: 3.446202516555786 Valid loss: 0.27079886198043823\n",
      "Epoch: 2673: Train loss: 3.4536080360412598 Valid loss: 2.3323535919189453\n",
      "Epoch: 2674: Train loss: 3.4432373046875 Valid loss: 0.27179741859436035\n",
      "Epoch: 2675: Train loss: 3.4506616592407227 Valid loss: 2.328078031539917\n",
      "Epoch: 2676: Train loss: 3.440279006958008 Valid loss: 0.272795170545578\n",
      "Epoch: 2677: Train loss: 3.4477248191833496 Valid loss: 2.3238155841827393\n",
      "Epoch: 2678: Train loss: 3.437325954437256 Valid loss: 0.27379584312438965\n",
      "Epoch: 2679: Train loss: 3.444791793823242 Valid loss: 2.3195641040802\n",
      "Epoch: 2680: Train loss: 3.434377908706665 Valid loss: 0.2747958302497864\n",
      "Epoch: 2681: Train loss: 3.441866636276245 Valid loss: 2.3153164386749268\n",
      "Epoch: 2682: Train loss: 3.4314374923706055 Valid loss: 0.27579575777053833\n",
      "Epoch: 2683: Train loss: 3.4389445781707764 Valid loss: 2.311084032058716\n",
      "Epoch: 2684: Train loss: 3.4285054206848145 Valid loss: 0.2767985463142395\n",
      "Epoch: 2685: Train loss: 3.436035633087158 Valid loss: 2.306879997253418\n",
      "Epoch: 2686: Train loss: 3.42557954788208 Valid loss: 0.277800977230072\n",
      "Epoch: 2687: Train loss: 3.4331257343292236 Valid loss: 2.302664041519165\n",
      "Epoch: 2688: Train loss: 3.4226596355438232 Valid loss: 0.2788028419017792\n",
      "Epoch: 2689: Train loss: 3.430225372314453 Valid loss: 2.2984673976898193\n",
      "Epoch: 2690: Train loss: 3.4197442531585693 Valid loss: 0.2798086404800415\n",
      "Epoch: 2691: Train loss: 3.42733097076416 Valid loss: 2.2942864894866943\n",
      "Epoch: 2692: Train loss: 3.416836738586426 Valid loss: 0.28081268072128296\n",
      "Epoch: 2693: Train loss: 3.424440622329712 Valid loss: 2.2901134490966797\n",
      "Epoch: 2694: Train loss: 3.413936138153076 Valid loss: 0.28181949257850647\n",
      "Epoch: 2695: Train loss: 3.4215617179870605 Valid loss: 2.2859559059143066\n",
      "Epoch: 2696: Train loss: 3.4110448360443115 Valid loss: 0.2828271985054016\n",
      "Epoch: 2697: Train loss: 3.41868257522583 Valid loss: 2.2817935943603516\n",
      "Epoch: 2698: Train loss: 3.4081506729125977 Valid loss: 0.2838320732116699\n",
      "Epoch: 2699: Train loss: 3.415809392929077 Valid loss: 2.2776520252227783\n",
      "Epoch: 2700: Train loss: 3.4052696228027344 Valid loss: 0.2848392128944397\n",
      "Epoch: 2701: Train loss: 3.4129419326782227 Valid loss: 2.273517370223999\n",
      "Epoch: 2702: Train loss: 3.402388572692871 Valid loss: 0.28584739565849304\n",
      "Epoch: 2703: Train loss: 3.4100797176361084 Valid loss: 2.2693862915039062\n",
      "Epoch: 2704: Train loss: 3.3995141983032227 Valid loss: 0.2868548631668091\n",
      "Epoch: 2705: Train loss: 3.4072265625 Valid loss: 2.265275001525879\n",
      "Epoch: 2706: Train loss: 3.3966519832611084 Valid loss: 0.28786545991897583\n",
      "Epoch: 2707: Train loss: 3.4043796062469482 Valid loss: 2.261171340942383\n",
      "Epoch: 2708: Train loss: 3.3937981128692627 Valid loss: 0.2888779044151306\n",
      "Epoch: 2709: Train loss: 3.401545524597168 Valid loss: 2.257098436355591\n",
      "Epoch: 2710: Train loss: 3.3909504413604736 Valid loss: 0.289889395236969\n",
      "Epoch: 2711: Train loss: 3.3987114429473877 Valid loss: 2.2530341148376465\n",
      "Epoch: 2712: Train loss: 3.388106107711792 Valid loss: 0.2909027338027954\n",
      "Epoch: 2713: Train loss: 3.3958845138549805 Valid loss: 2.2489569187164307\n",
      "Epoch: 2714: Train loss: 3.3852713108062744 Valid loss: 0.2919144630432129\n",
      "Epoch: 2715: Train loss: 3.393064260482788 Valid loss: 2.2449069023132324\n",
      "Epoch: 2716: Train loss: 3.3824424743652344 Valid loss: 0.2929289638996124\n",
      "Epoch: 2717: Train loss: 3.3902480602264404 Valid loss: 2.2408690452575684\n",
      "Epoch: 2718: Train loss: 3.3796133995056152 Valid loss: 0.293942928314209\n",
      "Epoch: 2719: Train loss: 3.3874404430389404 Valid loss: 2.2368459701538086\n",
      "Epoch: 2720: Train loss: 3.376798629760742 Valid loss: 0.29495900869369507\n",
      "Epoch: 2721: Train loss: 3.3846380710601807 Valid loss: 2.232830286026001\n",
      "Epoch: 2722: Train loss: 3.3739867210388184 Valid loss: 0.29597389698028564\n",
      "Epoch: 2723: Train loss: 3.3818390369415283 Valid loss: 2.2288105487823486\n",
      "Epoch: 2724: Train loss: 3.371178388595581 Valid loss: 0.29699093103408813\n",
      "Epoch: 2725: Train loss: 3.379051923751831 Valid loss: 2.2248175144195557\n",
      "Epoch: 2726: Train loss: 3.368382215499878 Valid loss: 0.2980080246925354\n",
      "Epoch: 2727: Train loss: 3.376265048980713 Valid loss: 2.220832347869873\n",
      "Epoch: 2728: Train loss: 3.3655917644500732 Valid loss: 0.29902520775794983\n",
      "Epoch: 2729: Train loss: 3.3734867572784424 Valid loss: 2.216858386993408\n",
      "Epoch: 2730: Train loss: 3.362802505493164 Valid loss: 0.3000445067882538\n",
      "Epoch: 2731: Train loss: 3.3707115650177 Valid loss: 2.2128922939300537\n",
      "Epoch: 2732: Train loss: 3.3600223064422607 Valid loss: 0.3010616600513458\n",
      "Epoch: 2733: Train loss: 3.367945671081543 Valid loss: 2.208937168121338\n",
      "Epoch: 2734: Train loss: 3.3572475910186768 Valid loss: 0.3020816743373871\n",
      "Epoch: 2735: Train loss: 3.365185499191284 Valid loss: 2.204996347427368\n",
      "Epoch: 2736: Train loss: 3.3544809818267822 Valid loss: 0.30310237407684326\n",
      "Epoch: 2737: Train loss: 3.3624331951141357 Valid loss: 2.2010719776153564\n",
      "Epoch: 2738: Train loss: 3.3517215251922607 Valid loss: 0.30412545800209045\n",
      "Epoch: 2739: Train loss: 3.359683036804199 Valid loss: 2.1971588134765625\n",
      "Epoch: 2740: Train loss: 3.348965883255005 Valid loss: 0.30514776706695557\n",
      "Epoch: 2741: Train loss: 3.3569436073303223 Valid loss: 2.193244695663452\n",
      "Epoch: 2742: Train loss: 3.346214771270752 Valid loss: 0.3061686158180237\n",
      "Epoch: 2743: Train loss: 3.354201316833496 Valid loss: 2.189337730407715\n",
      "Epoch: 2744: Train loss: 3.343470811843872 Valid loss: 0.3071918189525604\n",
      "Epoch: 2745: Train loss: 3.351470947265625 Valid loss: 2.1854569911956787\n",
      "Epoch: 2746: Train loss: 3.340731382369995 Valid loss: 0.3082129955291748\n",
      "Epoch: 2747: Train loss: 3.3487446308135986 Valid loss: 2.18157696723938\n",
      "Epoch: 2748: Train loss: 3.3380002975463867 Valid loss: 0.3092363476753235\n",
      "Epoch: 2749: Train loss: 3.3460261821746826 Valid loss: 2.177715301513672\n",
      "Epoch: 2750: Train loss: 3.3352766036987305 Valid loss: 0.3102632164955139\n",
      "Epoch: 2751: Train loss: 3.3433127403259277 Valid loss: 2.1738522052764893\n",
      "Epoch: 2752: Train loss: 3.3325588703155518 Valid loss: 0.31128597259521484\n",
      "Epoch: 2753: Train loss: 3.340604782104492 Valid loss: 2.1700057983398438\n",
      "Epoch: 2754: Train loss: 3.329845905303955 Valid loss: 0.3123120367527008\n",
      "Epoch: 2755: Train loss: 3.3379087448120117 Valid loss: 2.166180372238159\n",
      "Epoch: 2756: Train loss: 3.327143430709839 Valid loss: 0.31333982944488525\n",
      "Epoch: 2757: Train loss: 3.3352155685424805 Valid loss: 2.162363052368164\n",
      "Epoch: 2758: Train loss: 3.324444055557251 Valid loss: 0.3143662214279175\n",
      "Epoch: 2759: Train loss: 3.332526206970215 Valid loss: 2.158552885055542\n",
      "Epoch: 2760: Train loss: 3.3217525482177734 Valid loss: 0.315393328666687\n",
      "Epoch: 2761: Train loss: 3.329843521118164 Valid loss: 2.154757499694824\n",
      "Epoch: 2762: Train loss: 3.3190677165985107 Valid loss: 0.31642329692840576\n",
      "Epoch: 2763: Train loss: 3.327169895172119 Valid loss: 2.15097713470459\n",
      "Epoch: 2764: Train loss: 3.316387176513672 Valid loss: 0.317449688911438\n",
      "Epoch: 2765: Train loss: 3.3244967460632324 Valid loss: 2.1472039222717285\n",
      "Epoch: 2766: Train loss: 3.3137099742889404 Valid loss: 0.3184794783592224\n",
      "Epoch: 2767: Train loss: 3.3218302726745605 Valid loss: 2.143425226211548\n",
      "Epoch: 2768: Train loss: 3.311041831970215 Valid loss: 0.3195078670978546\n",
      "Epoch: 2769: Train loss: 3.319171667098999 Valid loss: 2.1396734714508057\n",
      "Epoch: 2770: Train loss: 3.308377265930176 Valid loss: 0.3205386996269226\n",
      "Epoch: 2771: Train loss: 3.3165156841278076 Valid loss: 2.135921001434326\n",
      "Epoch: 2772: Train loss: 3.305720090866089 Valid loss: 0.32156795263290405\n",
      "Epoch: 2773: Train loss: 3.31386661529541 Valid loss: 2.132187604904175\n",
      "Epoch: 2774: Train loss: 3.303065776824951 Valid loss: 0.3225972652435303\n",
      "Epoch: 2775: Train loss: 3.3112258911132812 Valid loss: 2.128469228744507\n",
      "Epoch: 2776: Train loss: 3.3004202842712402 Valid loss: 0.32363009452819824\n",
      "Epoch: 2777: Train loss: 3.3085861206054688 Valid loss: 2.1247494220733643\n",
      "Epoch: 2778: Train loss: 3.297781467437744 Valid loss: 0.32466158270835876\n",
      "Epoch: 2779: Train loss: 3.3059587478637695 Valid loss: 2.1210477352142334\n",
      "Epoch: 2780: Train loss: 3.295149564743042 Valid loss: 0.325692743062973\n",
      "Epoch: 2781: Train loss: 3.303335666656494 Valid loss: 2.117361307144165\n",
      "Epoch: 2782: Train loss: 3.2925264835357666 Valid loss: 0.3267277181148529\n",
      "Epoch: 2783: Train loss: 3.3007185459136963 Valid loss: 2.113682270050049\n",
      "Epoch: 2784: Train loss: 3.289905548095703 Valid loss: 0.327761173248291\n",
      "Epoch: 2785: Train loss: 3.298105478286743 Valid loss: 2.110013246536255\n",
      "Epoch: 2786: Train loss: 3.287292957305908 Valid loss: 0.3287960886955261\n",
      "Epoch: 2787: Train loss: 3.2955007553100586 Valid loss: 2.106358528137207\n",
      "Epoch: 2788: Train loss: 3.284682273864746 Valid loss: 0.32982969284057617\n",
      "Epoch: 2789: Train loss: 3.292896032333374 Valid loss: 2.102707624435425\n",
      "Epoch: 2790: Train loss: 3.282076835632324 Valid loss: 0.33086252212524414\n",
      "Epoch: 2791: Train loss: 3.2902963161468506 Valid loss: 2.0990593433380127\n",
      "Epoch: 2792: Train loss: 3.279479503631592 Valid loss: 0.33189713954925537\n",
      "Epoch: 2793: Train loss: 3.2877085208892822 Valid loss: 2.0954370498657227\n",
      "Epoch: 2794: Train loss: 3.2768869400024414 Valid loss: 0.332931786775589\n",
      "Epoch: 2795: Train loss: 3.285120964050293 Valid loss: 2.0918147563934326\n",
      "Epoch: 2796: Train loss: 3.2742996215820312 Valid loss: 0.3339664936065674\n",
      "Epoch: 2797: Train loss: 3.2825422286987305 Valid loss: 2.0882089138031006\n",
      "Epoch: 2798: Train loss: 3.2717206478118896 Valid loss: 0.33500364422798157\n",
      "Epoch: 2799: Train loss: 3.2799673080444336 Valid loss: 2.084611654281616\n",
      "Epoch: 2800: Train loss: 3.2691450119018555 Valid loss: 0.336040198802948\n",
      "Epoch: 2801: Train loss: 3.277397632598877 Valid loss: 2.081016778945923\n",
      "Epoch: 2802: Train loss: 3.266575336456299 Valid loss: 0.33707505464553833\n",
      "Epoch: 2803: Train loss: 3.2748379707336426 Valid loss: 2.0774474143981934\n",
      "Epoch: 2804: Train loss: 3.2640151977539062 Valid loss: 0.33811426162719727\n",
      "Epoch: 2805: Train loss: 3.272282361984253 Valid loss: 2.0738844871520996\n",
      "Epoch: 2806: Train loss: 3.261458158493042 Valid loss: 0.33915144205093384\n",
      "Epoch: 2807: Train loss: 3.269730806350708 Valid loss: 2.0703282356262207\n",
      "Epoch: 2808: Train loss: 3.2589099407196045 Valid loss: 0.3401910364627838\n",
      "Epoch: 2809: Train loss: 3.2671892642974854 Valid loss: 2.0667834281921387\n",
      "Epoch: 2810: Train loss: 3.25636625289917 Valid loss: 0.34122979640960693\n",
      "Epoch: 2811: Train loss: 3.264650583267212 Valid loss: 2.0632519721984863\n",
      "Epoch: 2812: Train loss: 3.2538328170776367 Valid loss: 0.34226977825164795\n",
      "Epoch: 2813: Train loss: 3.26212215423584 Valid loss: 2.059739112854004\n",
      "Epoch: 2814: Train loss: 3.25130033493042 Valid loss: 0.3433108329772949\n",
      "Epoch: 2815: Train loss: 3.259594202041626 Valid loss: 2.0562291145324707\n",
      "Epoch: 2816: Train loss: 3.2487754821777344 Valid loss: 0.3443494439125061\n",
      "Epoch: 2817: Train loss: 3.2570736408233643 Valid loss: 2.0527257919311523\n",
      "Epoch: 2818: Train loss: 3.246253490447998 Valid loss: 0.3453879654407501\n",
      "Epoch: 2819: Train loss: 3.254556894302368 Valid loss: 2.049236536026001\n",
      "Epoch: 2820: Train loss: 3.2437381744384766 Valid loss: 0.3464295268058777\n",
      "Epoch: 2821: Train loss: 3.252046585083008 Valid loss: 2.045758008956909\n",
      "Epoch: 2822: Train loss: 3.24122953414917 Valid loss: 0.3474716246128082\n",
      "Epoch: 2823: Train loss: 3.2495431900024414 Valid loss: 2.0422887802124023\n",
      "Epoch: 2824: Train loss: 3.238729238510132 Valid loss: 0.34851306676864624\n",
      "Epoch: 2825: Train loss: 3.247044563293457 Valid loss: 2.0388340950012207\n",
      "Epoch: 2826: Train loss: 3.236234188079834 Valid loss: 0.3495555818080902\n",
      "Epoch: 2827: Train loss: 3.244556427001953 Valid loss: 2.0353901386260986\n",
      "Epoch: 2828: Train loss: 3.2337443828582764 Valid loss: 0.3505966067314148\n",
      "Epoch: 2829: Train loss: 3.242067575454712 Valid loss: 2.0319528579711914\n",
      "Epoch: 2830: Train loss: 3.2312560081481934 Valid loss: 0.35164016485214233\n",
      "Epoch: 2831: Train loss: 3.239584445953369 Valid loss: 2.0285251140594482\n",
      "Epoch: 2832: Train loss: 3.2287795543670654 Valid loss: 0.35268160700798035\n",
      "Epoch: 2833: Train loss: 3.2371091842651367 Valid loss: 2.025103807449341\n",
      "Epoch: 2834: Train loss: 3.2263057231903076 Valid loss: 0.3537258207798004\n",
      "Epoch: 2835: Train loss: 3.234642505645752 Valid loss: 2.0217082500457764\n",
      "Epoch: 2836: Train loss: 3.223841905593872 Valid loss: 0.35476917028427124\n",
      "Epoch: 2837: Train loss: 3.2321760654449463 Valid loss: 2.0183186531066895\n",
      "Epoch: 2838: Train loss: 3.2213761806488037 Valid loss: 0.35581284761428833\n",
      "Epoch: 2839: Train loss: 3.2297143936157227 Valid loss: 2.0149285793304443\n",
      "Epoch: 2840: Train loss: 3.2189176082611084 Valid loss: 0.3568549156188965\n",
      "Epoch: 2841: Train loss: 3.2272579669952393 Valid loss: 2.0115485191345215\n",
      "Epoch: 2842: Train loss: 3.2164628505706787 Valid loss: 0.3578983545303345\n",
      "Epoch: 2843: Train loss: 3.224808692932129 Valid loss: 2.0081822872161865\n",
      "Epoch: 2844: Train loss: 3.2140188217163086 Valid loss: 0.35894322395324707\n",
      "Epoch: 2845: Train loss: 3.2223691940307617 Valid loss: 2.0048253536224365\n",
      "Epoch: 2846: Train loss: 3.2115819454193115 Valid loss: 0.35998713970184326\n",
      "Epoch: 2847: Train loss: 3.21993088722229 Valid loss: 2.0014874935150146\n",
      "Epoch: 2848: Train loss: 3.2091453075408936 Valid loss: 0.3610328137874603\n",
      "Epoch: 2849: Train loss: 3.2174971103668213 Valid loss: 1.998146891593933\n",
      "Epoch: 2850: Train loss: 3.2067172527313232 Valid loss: 0.3620757460594177\n",
      "Epoch: 2851: Train loss: 3.215070962905884 Valid loss: 1.9948172569274902\n",
      "Epoch: 2852: Train loss: 3.204294204711914 Valid loss: 0.363122820854187\n",
      "Epoch: 2853: Train loss: 3.2126500606536865 Valid loss: 1.9915083646774292\n",
      "Epoch: 2854: Train loss: 3.20188045501709 Valid loss: 0.36416906118392944\n",
      "Epoch: 2855: Train loss: 3.210239887237549 Valid loss: 1.988209843635559\n",
      "Epoch: 2856: Train loss: 3.1994731426239014 Valid loss: 0.365215927362442\n",
      "Epoch: 2857: Train loss: 3.207831621170044 Valid loss: 1.9849252700805664\n",
      "Epoch: 2858: Train loss: 3.1970677375793457 Valid loss: 0.36626043915748596\n",
      "Epoch: 2859: Train loss: 3.2054288387298584 Valid loss: 1.9816426038742065\n",
      "Epoch: 2860: Train loss: 3.194672107696533 Valid loss: 0.3673110008239746\n",
      "Epoch: 2861: Train loss: 3.2030327320098877 Valid loss: 1.978380799293518\n",
      "Epoch: 2862: Train loss: 3.1922807693481445 Valid loss: 0.36835598945617676\n",
      "Epoch: 2863: Train loss: 3.2006397247314453 Valid loss: 1.9751149415969849\n",
      "Epoch: 2864: Train loss: 3.1898884773254395 Valid loss: 0.36940130591392517\n",
      "Epoch: 2865: Train loss: 3.1982550621032715 Valid loss: 1.971866250038147\n",
      "Epoch: 2866: Train loss: 3.1875107288360596 Valid loss: 0.3704521656036377\n",
      "Epoch: 2867: Train loss: 3.1958765983581543 Valid loss: 1.9686343669891357\n",
      "Epoch: 2868: Train loss: 3.1851370334625244 Valid loss: 0.3715006113052368\n",
      "Epoch: 2869: Train loss: 3.193500518798828 Valid loss: 1.965404748916626\n",
      "Epoch: 2870: Train loss: 3.182765245437622 Valid loss: 0.3725472688674927\n",
      "Epoch: 2871: Train loss: 3.1911306381225586 Valid loss: 1.9621899127960205\n",
      "Epoch: 2872: Train loss: 3.180401086807251 Valid loss: 0.3735957741737366\n",
      "Epoch: 2873: Train loss: 3.188765525817871 Valid loss: 1.9589769840240479\n",
      "Epoch: 2874: Train loss: 3.1780383586883545 Valid loss: 0.37464457750320435\n",
      "Epoch: 2875: Train loss: 3.186403751373291 Valid loss: 1.955772876739502\n",
      "Epoch: 2876: Train loss: 3.1756858825683594 Valid loss: 0.37569233775138855\n",
      "Epoch: 2877: Train loss: 3.1840498447418213 Valid loss: 1.952589988708496\n",
      "Epoch: 2878: Train loss: 3.1733336448669434 Valid loss: 0.3767395317554474\n",
      "Epoch: 2879: Train loss: 3.181696891784668 Valid loss: 1.9493987560272217\n",
      "Epoch: 2880: Train loss: 3.170988082885742 Valid loss: 0.3777848482131958\n",
      "Epoch: 2881: Train loss: 3.1793527603149414 Valid loss: 1.9462285041809082\n",
      "Epoch: 2882: Train loss: 3.1686513423919678 Valid loss: 0.3788352906703949\n",
      "Epoch: 2883: Train loss: 3.1770167350769043 Valid loss: 1.9430817365646362\n",
      "Epoch: 2884: Train loss: 3.1663217544555664 Valid loss: 0.37988540530204773\n",
      "Epoch: 2885: Train loss: 3.1746842861175537 Valid loss: 1.9399316310882568\n",
      "Epoch: 2886: Train loss: 3.163991689682007 Valid loss: 0.3809345066547394\n",
      "Epoch: 2887: Train loss: 3.1723546981811523 Valid loss: 1.9367895126342773\n",
      "Epoch: 2888: Train loss: 3.1616718769073486 Valid loss: 0.3819831609725952\n",
      "Epoch: 2889: Train loss: 3.1700313091278076 Valid loss: 1.9336576461791992\n",
      "Epoch: 2890: Train loss: 3.1593539714813232 Valid loss: 0.3830324411392212\n",
      "Epoch: 2891: Train loss: 3.167712450027466 Valid loss: 1.9305288791656494\n",
      "Epoch: 2892: Train loss: 3.1570401191711426 Valid loss: 0.3840814530849457\n",
      "Epoch: 2893: Train loss: 3.165400505065918 Valid loss: 1.9274202585220337\n",
      "Epoch: 2894: Train loss: 3.1547365188598633 Valid loss: 0.38513168692588806\n",
      "Epoch: 2895: Train loss: 3.1630945205688477 Valid loss: 1.924320101737976\n",
      "Epoch: 2896: Train loss: 3.152437686920166 Valid loss: 0.38618138432502747\n",
      "Epoch: 2897: Train loss: 3.160799741744995 Valid loss: 1.9212441444396973\n",
      "Epoch: 2898: Train loss: 3.150148391723633 Valid loss: 0.3872327506542206\n",
      "Epoch: 2899: Train loss: 3.158503532409668 Valid loss: 1.9181640148162842\n",
      "Epoch: 2900: Train loss: 3.1478617191314697 Valid loss: 0.38828256726264954\n",
      "Epoch: 2901: Train loss: 3.1562161445617676 Valid loss: 1.9151002168655396\n",
      "Epoch: 2902: Train loss: 3.1455793380737305 Valid loss: 0.38933366537094116\n",
      "Epoch: 2903: Train loss: 3.1539316177368164 Valid loss: 1.9120419025421143\n",
      "Epoch: 2904: Train loss: 3.1433048248291016 Valid loss: 0.3903837203979492\n",
      "Epoch: 2905: Train loss: 3.151658773422241 Valid loss: 1.9090015888214111\n",
      "Epoch: 2906: Train loss: 3.141040086746216 Valid loss: 0.39143896102905273\n",
      "Epoch: 2907: Train loss: 3.149388313293457 Valid loss: 1.9059697389602661\n",
      "Epoch: 2908: Train loss: 3.13877534866333 Valid loss: 0.3924907445907593\n",
      "Epoch: 2909: Train loss: 3.1471240520477295 Valid loss: 1.9029465913772583\n",
      "Epoch: 2910: Train loss: 3.136519432067871 Valid loss: 0.3935413658618927\n",
      "Epoch: 2911: Train loss: 3.1448659896850586 Valid loss: 1.899936556816101\n",
      "Epoch: 2912: Train loss: 3.1342694759368896 Valid loss: 0.394593209028244\n",
      "Epoch: 2913: Train loss: 3.142608880996704 Valid loss: 1.8969295024871826\n",
      "Epoch: 2914: Train loss: 3.1320183277130127 Valid loss: 0.39564457535743713\n",
      "Epoch: 2915: Train loss: 3.140355110168457 Valid loss: 1.893934965133667\n",
      "Epoch: 2916: Train loss: 3.1297738552093506 Valid loss: 0.39669501781463623\n",
      "Epoch: 2917: Train loss: 3.138108968734741 Valid loss: 1.890950322151184\n",
      "Epoch: 2918: Train loss: 3.1275370121002197 Valid loss: 0.3977474272251129\n",
      "Epoch: 2919: Train loss: 3.135871410369873 Valid loss: 1.8879749774932861\n",
      "Epoch: 2920: Train loss: 3.125305414199829 Valid loss: 0.39879870414733887\n",
      "Epoch: 2921: Train loss: 3.1336379051208496 Valid loss: 1.8850078582763672\n",
      "Epoch: 2922: Train loss: 3.123077154159546 Valid loss: 0.39985042810440063\n",
      "Epoch: 2923: Train loss: 3.1314070224761963 Valid loss: 1.8820558786392212\n",
      "Epoch: 2924: Train loss: 3.1208572387695312 Valid loss: 0.4009033739566803\n",
      "Epoch: 2925: Train loss: 3.129180431365967 Valid loss: 1.8791110515594482\n",
      "Epoch: 2926: Train loss: 3.1186370849609375 Valid loss: 0.40195420384407043\n",
      "Epoch: 2927: Train loss: 3.1269588470458984 Valid loss: 1.8761653900146484\n",
      "Epoch: 2928: Train loss: 3.116424560546875 Valid loss: 0.4030052721500397\n",
      "Epoch: 2929: Train loss: 3.124741315841675 Valid loss: 1.873236060142517\n",
      "Epoch: 2930: Train loss: 3.1142148971557617 Valid loss: 0.4040563106536865\n",
      "Epoch: 2931: Train loss: 3.1225295066833496 Valid loss: 1.8703083992004395\n",
      "Epoch: 2932: Train loss: 3.112014055252075 Valid loss: 0.40510714054107666\n",
      "Epoch: 2933: Train loss: 3.1203272342681885 Valid loss: 1.8673999309539795\n",
      "Epoch: 2934: Train loss: 3.1098194122314453 Valid loss: 0.40615755319595337\n",
      "Epoch: 2935: Train loss: 3.1181247234344482 Valid loss: 1.864498496055603\n",
      "Epoch: 2936: Train loss: 3.1076295375823975 Valid loss: 0.40720993280410767\n",
      "Epoch: 2937: Train loss: 3.115931510925293 Valid loss: 1.861613154411316\n",
      "Epoch: 2938: Train loss: 3.105442762374878 Valid loss: 0.4082622826099396\n",
      "Epoch: 2939: Train loss: 3.113739013671875 Valid loss: 1.8587218523025513\n",
      "Epoch: 2940: Train loss: 3.1032583713531494 Valid loss: 0.40931037068367004\n",
      "Epoch: 2941: Train loss: 3.11155366897583 Valid loss: 1.8558542728424072\n",
      "Epoch: 2942: Train loss: 3.1010847091674805 Valid loss: 0.4103635847568512\n",
      "Epoch: 2943: Train loss: 3.109375476837158 Valid loss: 1.8529893159866333\n",
      "Epoch: 2944: Train loss: 3.098911762237549 Valid loss: 0.4114155173301697\n",
      "Epoch: 2945: Train loss: 3.1071977615356445 Valid loss: 1.850132703781128\n",
      "Epoch: 2946: Train loss: 3.096745491027832 Valid loss: 0.4124653935432434\n",
      "Epoch: 2947: Train loss: 3.1050291061401367 Valid loss: 1.8472880125045776\n",
      "Epoch: 2948: Train loss: 3.094586133956909 Valid loss: 0.4135172963142395\n",
      "Epoch: 2949: Train loss: 3.102865219116211 Valid loss: 1.8444533348083496\n",
      "Epoch: 2950: Train loss: 3.092434883117676 Valid loss: 0.4145698845386505\n",
      "Epoch: 2951: Train loss: 3.100707530975342 Valid loss: 1.8416380882263184\n",
      "Epoch: 2952: Train loss: 3.090285301208496 Valid loss: 0.41562050580978394\n",
      "Epoch: 2953: Train loss: 3.0985546112060547 Valid loss: 1.8388240337371826\n",
      "Epoch: 2954: Train loss: 3.0881431102752686 Valid loss: 0.4166746139526367\n",
      "Epoch: 2955: Train loss: 3.096407413482666 Valid loss: 1.8360230922698975\n",
      "Epoch: 2956: Train loss: 3.0860049724578857 Valid loss: 0.4177267253398895\n",
      "Epoch: 2957: Train loss: 3.0942635536193848 Valid loss: 1.8332239389419556\n",
      "Epoch: 2958: Train loss: 3.0838723182678223 Valid loss: 0.4187783896923065\n",
      "Epoch: 2959: Train loss: 3.092125415802002 Valid loss: 1.8304436206817627\n",
      "Epoch: 2960: Train loss: 3.0817408561706543 Valid loss: 0.41983023285865784\n",
      "Epoch: 2961: Train loss: 3.0899901390075684 Valid loss: 1.827659010887146\n",
      "Epoch: 2962: Train loss: 3.0796194076538086 Valid loss: 0.4208804965019226\n",
      "Epoch: 2963: Train loss: 3.0878615379333496 Valid loss: 1.8248932361602783\n",
      "Epoch: 2964: Train loss: 3.077500581741333 Valid loss: 0.42193225026130676\n",
      "Epoch: 2965: Train loss: 3.0857369899749756 Valid loss: 1.8221368789672852\n",
      "Epoch: 2966: Train loss: 3.0753891468048096 Valid loss: 0.4229855537414551\n",
      "Epoch: 2967: Train loss: 3.0836246013641357 Valid loss: 1.8193992376327515\n",
      "Epoch: 2968: Train loss: 3.0732827186584473 Valid loss: 0.4240376353263855\n",
      "Epoch: 2969: Train loss: 3.0815112590789795 Valid loss: 1.8166594505310059\n",
      "Epoch: 2970: Train loss: 3.071180820465088 Valid loss: 0.42508798837661743\n",
      "Epoch: 2971: Train loss: 3.079402446746826 Valid loss: 1.8139325380325317\n",
      "Epoch: 2972: Train loss: 3.0690858364105225 Valid loss: 0.4261404871940613\n",
      "Epoch: 2973: Train loss: 3.0773019790649414 Valid loss: 1.811218023300171\n",
      "Epoch: 2974: Train loss: 3.0669941902160645 Valid loss: 0.4271937608718872\n",
      "Epoch: 2975: Train loss: 3.075202465057373 Valid loss: 1.8085052967071533\n",
      "Epoch: 2976: Train loss: 3.064908266067505 Valid loss: 0.42824456095695496\n",
      "Epoch: 2977: Train loss: 3.0731117725372314 Valid loss: 1.8058034181594849\n",
      "Epoch: 2978: Train loss: 3.062824249267578 Valid loss: 0.42929506301879883\n",
      "Epoch: 2979: Train loss: 3.0710248947143555 Valid loss: 1.8031198978424072\n",
      "Epoch: 2980: Train loss: 3.0607521533966064 Valid loss: 0.43035027384757996\n",
      "Epoch: 2981: Train loss: 3.068946361541748 Valid loss: 1.8004448413848877\n",
      "Epoch: 2982: Train loss: 3.0586838722229004 Valid loss: 0.43140077590942383\n",
      "Epoch: 2983: Train loss: 3.0668716430664062 Valid loss: 1.7977780103683472\n",
      "Epoch: 2984: Train loss: 3.0566210746765137 Valid loss: 0.4324520230293274\n",
      "Epoch: 2985: Train loss: 3.064802408218384 Valid loss: 1.7951252460479736\n",
      "Epoch: 2986: Train loss: 3.054560661315918 Valid loss: 0.4335038959980011\n",
      "Epoch: 2987: Train loss: 3.0627338886260986 Valid loss: 1.792467713356018\n",
      "Epoch: 2988: Train loss: 3.0525026321411133 Valid loss: 0.43455570936203003\n",
      "Epoch: 2989: Train loss: 3.060668468475342 Valid loss: 1.789828896522522\n",
      "Epoch: 2990: Train loss: 3.050450086593628 Valid loss: 0.43560540676116943\n",
      "Epoch: 2991: Train loss: 3.0586133003234863 Valid loss: 1.7871910333633423\n",
      "Epoch: 2992: Train loss: 3.0484073162078857 Valid loss: 0.4366576373577118\n",
      "Epoch: 2993: Train loss: 3.056563138961792 Valid loss: 1.7845665216445923\n",
      "Epoch: 2994: Train loss: 3.0463685989379883 Valid loss: 0.43771111965179443\n",
      "Epoch: 2995: Train loss: 3.0545170307159424 Valid loss: 1.7819570302963257\n",
      "Epoch: 2996: Train loss: 3.0443310737609863 Valid loss: 0.4387616217136383\n",
      "Epoch: 2997: Train loss: 3.0524730682373047 Valid loss: 1.779350757598877\n",
      "Epoch: 2998: Train loss: 3.0423028469085693 Valid loss: 0.43981167674064636\n",
      "Epoch: 2999: Train loss: 3.0504345893859863 Valid loss: 1.7767481803894043\n",
      "Epoch: 3000: Train loss: 3.0402774810791016 Valid loss: 0.44086289405822754\n",
      "Epoch: 3001: Train loss: 3.048403739929199 Valid loss: 1.7741702795028687\n",
      "Epoch: 3002: Train loss: 3.0382587909698486 Valid loss: 0.44191408157348633\n",
      "Epoch: 3003: Train loss: 3.0463764667510986 Valid loss: 1.7715907096862793\n",
      "Epoch: 3004: Train loss: 3.036243438720703 Valid loss: 0.4429650604724884\n",
      "Epoch: 3005: Train loss: 3.0443570613861084 Valid loss: 1.7690263986587524\n",
      "Epoch: 3006: Train loss: 3.034233808517456 Valid loss: 0.4440152943134308\n",
      "Epoch: 3007: Train loss: 3.0423390865325928 Valid loss: 1.7664673328399658\n",
      "Epoch: 3008: Train loss: 3.032229423522949 Valid loss: 0.4450663626194\n",
      "Epoch: 3009: Train loss: 3.0403268337249756 Valid loss: 1.7639186382293701\n",
      "Epoch: 3010: Train loss: 3.0302278995513916 Valid loss: 0.4461168646812439\n",
      "Epoch: 3011: Train loss: 3.038320302963257 Valid loss: 1.7613801956176758\n",
      "Epoch: 3012: Train loss: 3.028233528137207 Valid loss: 0.4471656382083893\n",
      "Epoch: 3013: Train loss: 3.0363152027130127 Valid loss: 1.7588423490524292\n",
      "Epoch: 3014: Train loss: 3.0262444019317627 Valid loss: 0.4482158422470093\n",
      "Epoch: 3015: Train loss: 3.0343210697174072 Valid loss: 1.7563261985778809\n",
      "Epoch: 3016: Train loss: 3.0242624282836914 Valid loss: 0.4492666721343994\n",
      "Epoch: 3017: Train loss: 3.0323305130004883 Valid loss: 1.7538198232650757\n",
      "Epoch: 3018: Train loss: 3.022282361984253 Valid loss: 0.4503181278705597\n",
      "Epoch: 3019: Train loss: 3.0303428173065186 Valid loss: 1.7513161897659302\n",
      "Epoch: 3020: Train loss: 3.0203053951263428 Valid loss: 0.4513689875602722\n",
      "Epoch: 3021: Train loss: 3.02836012840271 Valid loss: 1.7488254308700562\n",
      "Epoch: 3022: Train loss: 3.018340826034546 Valid loss: 0.4524184763431549\n",
      "Epoch: 3023: Train loss: 3.0263874530792236 Valid loss: 1.7463490962982178\n",
      "Epoch: 3024: Train loss: 3.016375780105591 Valid loss: 0.4534689486026764\n",
      "Epoch: 3025: Train loss: 3.0244154930114746 Valid loss: 1.743873953819275\n",
      "Epoch: 3026: Train loss: 3.014416456222534 Valid loss: 0.4545188546180725\n",
      "Epoch: 3027: Train loss: 3.022444486618042 Valid loss: 1.7414015531539917\n",
      "Epoch: 3028: Train loss: 3.0124576091766357 Valid loss: 0.45556485652923584\n",
      "Epoch: 3029: Train loss: 3.0204761028289795 Valid loss: 1.7389428615570068\n",
      "Epoch: 3030: Train loss: 3.010503053665161 Valid loss: 0.4566138982772827\n",
      "Epoch: 3031: Train loss: 3.0185165405273438 Valid loss: 1.7364857196807861\n",
      "Epoch: 3032: Train loss: 3.008557081222534 Valid loss: 0.4576619267463684\n",
      "Epoch: 3033: Train loss: 3.016559600830078 Valid loss: 1.7340466976165771\n",
      "Epoch: 3034: Train loss: 3.006610870361328 Valid loss: 0.45870810747146606\n",
      "Epoch: 3035: Train loss: 3.0146076679229736 Valid loss: 1.7316036224365234\n",
      "Epoch: 3036: Train loss: 3.004671812057495 Valid loss: 0.4597579538822174\n",
      "Epoch: 3037: Train loss: 3.0126609802246094 Valid loss: 1.7291816473007202\n",
      "Epoch: 3038: Train loss: 3.002739906311035 Valid loss: 0.46080487966537476\n",
      "Epoch: 3039: Train loss: 3.0107226371765137 Valid loss: 1.7267694473266602\n",
      "Epoch: 3040: Train loss: 3.0008139610290527 Valid loss: 0.4618528485298157\n",
      "Epoch: 3041: Train loss: 3.008788824081421 Valid loss: 1.7243626117706299\n",
      "Epoch: 3042: Train loss: 2.998893976211548 Valid loss: 0.46290159225463867\n",
      "Epoch: 3043: Train loss: 3.0068583488464355 Valid loss: 1.7219679355621338\n",
      "Epoch: 3044: Train loss: 2.996976375579834 Valid loss: 0.4639502763748169\n",
      "Epoch: 3045: Train loss: 3.0049309730529785 Valid loss: 1.71958327293396\n",
      "Epoch: 3046: Train loss: 2.995063066482544 Valid loss: 0.4649975299835205\n",
      "Epoch: 3047: Train loss: 3.003009557723999 Valid loss: 1.7171940803527832\n",
      "Epoch: 3048: Train loss: 2.9931581020355225 Valid loss: 0.46604424715042114\n",
      "Epoch: 3049: Train loss: 3.001095771789551 Valid loss: 1.7148292064666748\n",
      "Epoch: 3050: Train loss: 2.99125599861145 Valid loss: 0.46709179878234863\n",
      "Epoch: 3051: Train loss: 2.999187469482422 Valid loss: 1.712476134300232\n",
      "Epoch: 3052: Train loss: 2.9893593788146973 Valid loss: 0.4681411683559418\n",
      "Epoch: 3053: Train loss: 2.997281551361084 Valid loss: 1.7101236581802368\n",
      "Epoch: 3054: Train loss: 2.9874699115753174 Valid loss: 0.46918752789497375\n",
      "Epoch: 3055: Train loss: 2.995384693145752 Valid loss: 1.7077891826629639\n",
      "Epoch: 3056: Train loss: 2.9855823516845703 Valid loss: 0.47023510932922363\n",
      "Epoch: 3057: Train loss: 2.9934849739074707 Valid loss: 1.705450177192688\n",
      "Epoch: 3058: Train loss: 2.9836971759796143 Valid loss: 0.47128117084503174\n",
      "Epoch: 3059: Train loss: 2.9915919303894043 Valid loss: 1.7031323909759521\n",
      "Epoch: 3060: Train loss: 2.981818675994873 Valid loss: 0.4723273515701294\n",
      "Epoch: 3061: Train loss: 2.989701986312866 Valid loss: 1.7008037567138672\n",
      "Epoch: 3062: Train loss: 2.9799447059631348 Valid loss: 0.47337284684181213\n",
      "Epoch: 3063: Train loss: 2.9878203868865967 Valid loss: 1.6985011100769043\n",
      "Epoch: 3064: Train loss: 2.9780759811401367 Valid loss: 0.47441741824150085\n",
      "Epoch: 3065: Train loss: 2.9859418869018555 Valid loss: 1.696202039718628\n",
      "Epoch: 3066: Train loss: 2.9762086868286133 Valid loss: 0.4754635691642761\n",
      "Epoch: 3067: Train loss: 2.984064817428589 Valid loss: 1.6939060688018799\n",
      "Epoch: 3068: Train loss: 2.974346399307251 Valid loss: 0.47650736570358276\n",
      "Epoch: 3069: Train loss: 2.982191801071167 Valid loss: 1.691619873046875\n",
      "Epoch: 3070: Train loss: 2.9724881649017334 Valid loss: 0.4775533080101013\n",
      "Epoch: 3071: Train loss: 2.980325222015381 Valid loss: 1.6893447637557983\n",
      "Epoch: 3072: Train loss: 2.970634937286377 Valid loss: 0.4785943925380707\n",
      "Epoch: 3073: Train loss: 2.9784655570983887 Valid loss: 1.6870776414871216\n",
      "Epoch: 3074: Train loss: 2.9687891006469727 Valid loss: 0.47963953018188477\n",
      "Epoch: 3075: Train loss: 2.9766087532043457 Valid loss: 1.6848193407058716\n",
      "Epoch: 3076: Train loss: 2.966949224472046 Valid loss: 0.48068371415138245\n",
      "Epoch: 3077: Train loss: 2.974759340286255 Valid loss: 1.6825788021087646\n",
      "Epoch: 3078: Train loss: 2.9651150703430176 Valid loss: 0.48172885179519653\n",
      "Epoch: 3079: Train loss: 2.972913980484009 Valid loss: 1.680334210395813\n",
      "Epoch: 3080: Train loss: 2.963282823562622 Valid loss: 0.48277366161346436\n",
      "Epoch: 3081: Train loss: 2.9710724353790283 Valid loss: 1.678106427192688\n",
      "Epoch: 3082: Train loss: 2.9614529609680176 Valid loss: 0.4838160276412964\n",
      "Epoch: 3083: Train loss: 2.9692323207855225 Valid loss: 1.6758760213851929\n",
      "Epoch: 3084: Train loss: 2.959629535675049 Valid loss: 0.4848597049713135\n",
      "Epoch: 3085: Train loss: 2.9674007892608643 Valid loss: 1.6736681461334229\n",
      "Epoch: 3086: Train loss: 2.9578123092651367 Valid loss: 0.4859018325805664\n",
      "Epoch: 3087: Train loss: 2.9655749797821045 Valid loss: 1.671460509300232\n",
      "Epoch: 3088: Train loss: 2.956001043319702 Valid loss: 0.48694756627082825\n",
      "Epoch: 3089: Train loss: 2.963751792907715 Valid loss: 1.6692692041397095\n",
      "Epoch: 3090: Train loss: 2.9541923999786377 Valid loss: 0.48799026012420654\n",
      "Epoch: 3091: Train loss: 2.9619317054748535 Valid loss: 1.667077660560608\n",
      "Epoch: 3092: Train loss: 2.9523887634277344 Valid loss: 0.4890311062335968\n",
      "Epoch: 3093: Train loss: 2.960120677947998 Valid loss: 1.6649011373519897\n",
      "Epoch: 3094: Train loss: 2.9505906105041504 Valid loss: 0.4900742769241333\n",
      "Epoch: 3095: Train loss: 2.9583115577697754 Valid loss: 1.6627349853515625\n",
      "Epoch: 3096: Train loss: 2.9487979412078857 Valid loss: 0.4911182224750519\n",
      "Epoch: 3097: Train loss: 2.956509828567505 Valid loss: 1.6605780124664307\n",
      "Epoch: 3098: Train loss: 2.947009325027466 Valid loss: 0.49215859174728394\n",
      "Epoch: 3099: Train loss: 2.9547104835510254 Valid loss: 1.6584285497665405\n",
      "Epoch: 3100: Train loss: 2.945223331451416 Valid loss: 0.4932017922401428\n",
      "Epoch: 3101: Train loss: 2.952914237976074 Valid loss: 1.6562762260437012\n",
      "Epoch: 3102: Train loss: 2.9434428215026855 Valid loss: 0.4942416548728943\n",
      "Epoch: 3103: Train loss: 2.951122760772705 Valid loss: 1.654138207435608\n",
      "Epoch: 3104: Train loss: 2.9416630268096924 Valid loss: 0.4952827990055084\n",
      "Epoch: 3105: Train loss: 2.9493348598480225 Valid loss: 1.6520113945007324\n",
      "Epoch: 3106: Train loss: 2.939891815185547 Valid loss: 0.496321439743042\n",
      "Epoch: 3107: Train loss: 2.9475512504577637 Valid loss: 1.649888038635254\n",
      "Epoch: 3108: Train loss: 2.938124656677246 Valid loss: 0.4973613917827606\n",
      "Epoch: 3109: Train loss: 2.9457759857177734 Valid loss: 1.6477806568145752\n",
      "Epoch: 3110: Train loss: 2.9363632202148438 Valid loss: 0.4984039068222046\n",
      "Epoch: 3111: Train loss: 2.944004535675049 Valid loss: 1.6456780433654785\n",
      "Epoch: 3112: Train loss: 2.9346048831939697 Valid loss: 0.49944135546684265\n",
      "Epoch: 3113: Train loss: 2.9422295093536377 Valid loss: 1.6435773372650146\n",
      "Epoch: 3114: Train loss: 2.932851552963257 Valid loss: 0.5004822015762329\n",
      "Epoch: 3115: Train loss: 2.94046950340271 Valid loss: 1.641493320465088\n",
      "Epoch: 3116: Train loss: 2.931102991104126 Valid loss: 0.5015207529067993\n",
      "Epoch: 3117: Train loss: 2.938714027404785 Valid loss: 1.6394214630126953\n",
      "Epoch: 3118: Train loss: 2.929361581802368 Valid loss: 0.5025603771209717\n",
      "Epoch: 3119: Train loss: 2.936955213546753 Valid loss: 1.6373536586761475\n",
      "Epoch: 3120: Train loss: 2.9276185035705566 Valid loss: 0.5035985112190247\n",
      "Epoch: 3121: Train loss: 2.9352049827575684 Valid loss: 1.6352872848510742\n",
      "Epoch: 3122: Train loss: 2.9258835315704346 Valid loss: 0.5046353936195374\n",
      "Epoch: 3123: Train loss: 2.933457136154175 Valid loss: 1.6332354545593262\n",
      "Epoch: 3124: Train loss: 2.924149751663208 Valid loss: 0.5056748390197754\n",
      "Epoch: 3125: Train loss: 2.9317142963409424 Valid loss: 1.6311873197555542\n",
      "Epoch: 3126: Train loss: 2.9224212169647217 Valid loss: 0.5067112445831299\n",
      "Epoch: 3127: Train loss: 2.929975986480713 Valid loss: 1.6291449069976807\n",
      "Epoch: 3128: Train loss: 2.9207003116607666 Valid loss: 0.5077476501464844\n",
      "Epoch: 3129: Train loss: 2.9282431602478027 Valid loss: 1.6271144151687622\n",
      "Epoch: 3130: Train loss: 2.9189789295196533 Valid loss: 0.5087827444076538\n",
      "Epoch: 3131: Train loss: 2.926510810852051 Valid loss: 1.6250954866409302\n",
      "Epoch: 3132: Train loss: 2.9172656536102295 Valid loss: 0.5098206996917725\n",
      "Epoch: 3133: Train loss: 2.924785614013672 Valid loss: 1.6230785846710205\n",
      "Epoch: 3134: Train loss: 2.9155542850494385 Valid loss: 0.5108557343482971\n",
      "Epoch: 3135: Train loss: 2.923062562942505 Valid loss: 1.62107253074646\n",
      "Epoch: 3136: Train loss: 2.9138455390930176 Valid loss: 0.5118910670280457\n",
      "Epoch: 3137: Train loss: 2.92134165763855 Valid loss: 1.6190671920776367\n",
      "Epoch: 3138: Train loss: 2.9121408462524414 Valid loss: 0.5129221081733704\n",
      "Epoch: 3139: Train loss: 2.919623374938965 Valid loss: 1.6170662641525269\n",
      "Epoch: 3140: Train loss: 2.910438060760498 Valid loss: 0.513954758644104\n",
      "Epoch: 3141: Train loss: 2.9179134368896484 Valid loss: 1.615072250366211\n",
      "Epoch: 3142: Train loss: 2.908743381500244 Valid loss: 0.5149886608123779\n",
      "Epoch: 3143: Train loss: 2.9162075519561768 Valid loss: 1.6130965948104858\n",
      "Epoch: 3144: Train loss: 2.907053232192993 Valid loss: 0.5160218477249146\n",
      "Epoch: 3145: Train loss: 2.91450572013855 Valid loss: 1.611130714416504\n",
      "Epoch: 3146: Train loss: 2.9053688049316406 Valid loss: 0.5170553922653198\n",
      "Epoch: 3147: Train loss: 2.912808656692505 Valid loss: 1.6091654300689697\n",
      "Epoch: 3148: Train loss: 2.9036877155303955 Valid loss: 0.5180878043174744\n",
      "Epoch: 3149: Train loss: 2.911118745803833 Valid loss: 1.607215166091919\n",
      "Epoch: 3150: Train loss: 2.9020118713378906 Valid loss: 0.5191212892532349\n",
      "Epoch: 3151: Train loss: 2.9094340801239014 Valid loss: 1.6052719354629517\n",
      "Epoch: 3152: Train loss: 2.900346279144287 Valid loss: 0.5201579928398132\n",
      "Epoch: 3153: Train loss: 2.9077577590942383 Valid loss: 1.6033436059951782\n",
      "Epoch: 3154: Train loss: 2.8986830711364746 Valid loss: 0.5211904048919678\n",
      "Epoch: 3155: Train loss: 2.9060823917388916 Valid loss: 1.6014232635498047\n",
      "Epoch: 3156: Train loss: 2.8970224857330322 Valid loss: 0.5222227573394775\n",
      "Epoch: 3157: Train loss: 2.904407501220703 Valid loss: 1.5995064973831177\n",
      "Epoch: 3158: Train loss: 2.8953657150268555 Valid loss: 0.5232551097869873\n",
      "Epoch: 3159: Train loss: 2.902740001678467 Valid loss: 1.5975971221923828\n",
      "Epoch: 3160: Train loss: 2.893714427947998 Valid loss: 0.5242875218391418\n",
      "Epoch: 3161: Train loss: 2.9010801315307617 Valid loss: 1.5956966876983643\n",
      "Epoch: 3162: Train loss: 2.8920695781707764 Valid loss: 0.5253190398216248\n",
      "Epoch: 3163: Train loss: 2.8994245529174805 Valid loss: 1.5938160419464111\n",
      "Epoch: 3164: Train loss: 2.890427589416504 Valid loss: 0.5263528227806091\n",
      "Epoch: 3165: Train loss: 2.897768020629883 Valid loss: 1.5919299125671387\n",
      "Epoch: 3166: Train loss: 2.8887884616851807 Valid loss: 0.5273832082748413\n",
      "Epoch: 3167: Train loss: 2.8961198329925537 Valid loss: 1.5900540351867676\n",
      "Epoch: 3168: Train loss: 2.8871524333953857 Valid loss: 0.5284136533737183\n",
      "Epoch: 3169: Train loss: 2.8944711685180664 Valid loss: 1.5881837606430054\n",
      "Epoch: 3170: Train loss: 2.885521650314331 Valid loss: 0.5294436812400818\n",
      "Epoch: 3171: Train loss: 2.8928298950195312 Valid loss: 1.5863287448883057\n",
      "Epoch: 3172: Train loss: 2.883894681930542 Valid loss: 0.5304745435714722\n",
      "Epoch: 3173: Train loss: 2.891186475753784 Valid loss: 1.5844658613204956\n",
      "Epoch: 3174: Train loss: 2.8822691440582275 Valid loss: 0.5315012335777283\n",
      "Epoch: 3175: Train loss: 2.889554500579834 Valid loss: 1.5826261043548584\n",
      "Epoch: 3176: Train loss: 2.880650758743286 Valid loss: 0.5325292348861694\n",
      "Epoch: 3177: Train loss: 2.8879241943359375 Valid loss: 1.5807844400405884\n",
      "Epoch: 3178: Train loss: 2.879038095474243 Valid loss: 0.5335593223571777\n",
      "Epoch: 3179: Train loss: 2.886296510696411 Valid loss: 1.5789556503295898\n",
      "Epoch: 3180: Train loss: 2.8774261474609375 Valid loss: 0.5345854759216309\n",
      "Epoch: 3181: Train loss: 2.8846731185913086 Valid loss: 1.5771254301071167\n",
      "Epoch: 3182: Train loss: 2.875816822052002 Valid loss: 0.5356124639511108\n",
      "Epoch: 3183: Train loss: 2.8830554485321045 Valid loss: 1.5753071308135986\n",
      "Epoch: 3184: Train loss: 2.8742151260375977 Valid loss: 0.5366376638412476\n",
      "Epoch: 3185: Train loss: 2.881439685821533 Valid loss: 1.5735020637512207\n",
      "Epoch: 3186: Train loss: 2.8726165294647217 Valid loss: 0.5376639366149902\n",
      "Epoch: 3187: Train loss: 2.879826068878174 Valid loss: 1.5716986656188965\n",
      "Epoch: 3188: Train loss: 2.8710179328918457 Valid loss: 0.5386902093887329\n",
      "Epoch: 3189: Train loss: 2.878218650817871 Valid loss: 1.569894552230835\n",
      "Epoch: 3190: Train loss: 2.869426727294922 Valid loss: 0.5397144556045532\n",
      "Epoch: 3191: Train loss: 2.876614809036255 Valid loss: 1.5681073665618896\n",
      "Epoch: 3192: Train loss: 2.8678407669067383 Valid loss: 0.540738582611084\n",
      "Epoch: 3193: Train loss: 2.8750202655792236 Valid loss: 1.5663325786590576\n",
      "Epoch: 3194: Train loss: 2.8662614822387695 Valid loss: 0.5417645573616028\n",
      "Epoch: 3195: Train loss: 2.8734281063079834 Valid loss: 1.5645583868026733\n",
      "Epoch: 3196: Train loss: 2.864683151245117 Valid loss: 0.5427865386009216\n",
      "Epoch: 3197: Train loss: 2.871835470199585 Valid loss: 1.5627914667129517\n",
      "Epoch: 3198: Train loss: 2.863105297088623 Valid loss: 0.543809175491333\n",
      "Epoch: 3199: Train loss: 2.870245933532715 Valid loss: 1.5610244274139404\n",
      "Epoch: 3200: Train loss: 2.8615365028381348 Valid loss: 0.5448311567306519\n",
      "Epoch: 3201: Train loss: 2.868666648864746 Valid loss: 1.5592803955078125\n",
      "Epoch: 3202: Train loss: 2.8599696159362793 Valid loss: 0.5458558797836304\n",
      "Epoch: 3203: Train loss: 2.8670876026153564 Valid loss: 1.55753493309021\n",
      "Epoch: 3204: Train loss: 2.8584065437316895 Valid loss: 0.546877384185791\n",
      "Epoch: 3205: Train loss: 2.8655083179473877 Valid loss: 1.5557911396026611\n",
      "Epoch: 3206: Train loss: 2.856846332550049 Valid loss: 0.5478982925415039\n",
      "Epoch: 3207: Train loss: 2.863940477371216 Valid loss: 1.554071307182312\n",
      "Epoch: 3208: Train loss: 2.8552918434143066 Valid loss: 0.5489192008972168\n",
      "Epoch: 3209: Train loss: 2.862372398376465 Valid loss: 1.5523433685302734\n",
      "Epoch: 3210: Train loss: 2.8537418842315674 Valid loss: 0.5499399900436401\n",
      "Epoch: 3211: Train loss: 2.8608102798461914 Valid loss: 1.5506309270858765\n",
      "Epoch: 3212: Train loss: 2.852195978164673 Valid loss: 0.5509588718414307\n",
      "Epoch: 3213: Train loss: 2.8592545986175537 Valid loss: 1.5489274263381958\n",
      "Epoch: 3214: Train loss: 2.850656747817993 Valid loss: 0.5519839525222778\n",
      "Epoch: 3215: Train loss: 2.8577003479003906 Valid loss: 1.5472278594970703\n",
      "Epoch: 3216: Train loss: 2.849119186401367 Valid loss: 0.5530030131340027\n",
      "Epoch: 3217: Train loss: 2.8561527729034424 Valid loss: 1.5455427169799805\n",
      "Epoch: 3218: Train loss: 2.8475868701934814 Valid loss: 0.554020881652832\n",
      "Epoch: 3219: Train loss: 2.8546059131622314 Valid loss: 1.5438580513000488\n",
      "Epoch: 3220: Train loss: 2.846055030822754 Valid loss: 0.555039644241333\n",
      "Epoch: 3221: Train loss: 2.8530654907226562 Valid loss: 1.5421833992004395\n",
      "Epoch: 3222: Train loss: 2.8445305824279785 Valid loss: 0.5560581088066101\n",
      "Epoch: 3223: Train loss: 2.8515243530273438 Valid loss: 1.540510892868042\n",
      "Epoch: 3224: Train loss: 2.843007802963257 Valid loss: 0.557070791721344\n",
      "Epoch: 3225: Train loss: 2.849990129470825 Valid loss: 1.5388457775115967\n",
      "Epoch: 3226: Train loss: 2.8414902687072754 Valid loss: 0.558091402053833\n",
      "Epoch: 3227: Train loss: 2.848463296890259 Valid loss: 1.5371954441070557\n",
      "Epoch: 3228: Train loss: 2.8399763107299805 Valid loss: 0.5591088533401489\n",
      "Epoch: 3229: Train loss: 2.8469369411468506 Valid loss: 1.5355463027954102\n",
      "Epoch: 3230: Train loss: 2.838466167449951 Valid loss: 0.5601251125335693\n",
      "Epoch: 3231: Train loss: 2.845412492752075 Valid loss: 1.5338990688323975\n",
      "Epoch: 3232: Train loss: 2.836958885192871 Valid loss: 0.5611401796340942\n",
      "Epoch: 3233: Train loss: 2.843891143798828 Valid loss: 1.5322620868682861\n",
      "Epoch: 3234: Train loss: 2.8354551792144775 Valid loss: 0.5621542930603027\n",
      "Epoch: 3235: Train loss: 2.8423776626586914 Valid loss: 1.5306392908096313\n",
      "Epoch: 3236: Train loss: 2.8339569568634033 Valid loss: 0.5631707906723022\n",
      "Epoch: 3237: Train loss: 2.8408687114715576 Valid loss: 1.5290241241455078\n",
      "Epoch: 3238: Train loss: 2.8324649333953857 Valid loss: 0.5641855597496033\n",
      "Epoch: 3239: Train loss: 2.8393640518188477 Valid loss: 1.5274136066436768\n",
      "Epoch: 3240: Train loss: 2.8309738636016846 Valid loss: 0.5651982426643372\n",
      "Epoch: 3241: Train loss: 2.837857723236084 Valid loss: 1.5258071422576904\n",
      "Epoch: 3242: Train loss: 2.829488754272461 Valid loss: 0.5662131309509277\n",
      "Epoch: 3243: Train loss: 2.836359739303589 Valid loss: 1.5242078304290771\n",
      "Epoch: 3244: Train loss: 2.8280038833618164 Valid loss: 0.5672203302383423\n",
      "Epoch: 3245: Train loss: 2.8348634243011475 Valid loss: 1.5226143598556519\n",
      "Epoch: 3246: Train loss: 2.8265249729156494 Valid loss: 0.568235456943512\n",
      "Epoch: 3247: Train loss: 2.833372116088867 Valid loss: 1.5210322141647339\n",
      "Epoch: 3248: Train loss: 2.825047731399536 Valid loss: 0.5692459344863892\n",
      "Epoch: 3249: Train loss: 2.8318793773651123 Valid loss: 1.5194449424743652\n",
      "Epoch: 3250: Train loss: 2.8235745429992676 Valid loss: 0.5702559947967529\n",
      "Epoch: 3251: Train loss: 2.830395460128784 Valid loss: 1.5178741216659546\n",
      "Epoch: 3252: Train loss: 2.8221049308776855 Valid loss: 0.5712658166885376\n",
      "Epoch: 3253: Train loss: 2.828911781311035 Valid loss: 1.5163037776947021\n",
      "Epoch: 3254: Train loss: 2.820636510848999 Valid loss: 0.5722774267196655\n",
      "Epoch: 3255: Train loss: 2.827435255050659 Valid loss: 1.5147488117218018\n",
      "Epoch: 3256: Train loss: 2.819176435470581 Valid loss: 0.5732851028442383\n",
      "Epoch: 3257: Train loss: 2.825963020324707 Valid loss: 1.513196349143982\n",
      "Epoch: 3258: Train loss: 2.8177218437194824 Valid loss: 0.5742945671081543\n",
      "Epoch: 3259: Train loss: 2.8244922161102295 Valid loss: 1.511650800704956\n",
      "Epoch: 3260: Train loss: 2.816270589828491 Valid loss: 0.5753039121627808\n",
      "Epoch: 3261: Train loss: 2.8230278491973877 Valid loss: 1.510116696357727\n",
      "Epoch: 3262: Train loss: 2.814819812774658 Valid loss: 0.5763115882873535\n",
      "Epoch: 3263: Train loss: 2.8215689659118652 Valid loss: 1.5085893869400024\n",
      "Epoch: 3264: Train loss: 2.813375473022461 Valid loss: 0.5773200392723083\n",
      "Epoch: 3265: Train loss: 2.8201115131378174 Valid loss: 1.507063865661621\n",
      "Epoch: 3266: Train loss: 2.811936378479004 Valid loss: 0.5783283710479736\n",
      "Epoch: 3267: Train loss: 2.818657875061035 Valid loss: 1.5055499076843262\n",
      "Epoch: 3268: Train loss: 2.8104987144470215 Valid loss: 0.5793358683586121\n",
      "Epoch: 3269: Train loss: 2.8172080516815186 Valid loss: 1.5040433406829834\n",
      "Epoch: 3270: Train loss: 2.8090641498565674 Valid loss: 0.5803408026695251\n",
      "Epoch: 3271: Train loss: 2.815762758255005 Valid loss: 1.5025403499603271\n",
      "Epoch: 3272: Train loss: 2.807633876800537 Valid loss: 0.5813484787940979\n",
      "Epoch: 3273: Train loss: 2.8143184185028076 Valid loss: 1.5010457038879395\n",
      "Epoch: 3274: Train loss: 2.806206464767456 Valid loss: 0.5823493599891663\n",
      "Epoch: 3275: Train loss: 2.8128774166107178 Valid loss: 1.4995503425598145\n",
      "Epoch: 3276: Train loss: 2.804785966873169 Valid loss: 0.5833560228347778\n",
      "Epoch: 3277: Train loss: 2.811441421508789 Valid loss: 1.4980721473693848\n",
      "Epoch: 3278: Train loss: 2.8033647537231445 Valid loss: 0.5843597054481506\n",
      "Epoch: 3279: Train loss: 2.8100106716156006 Valid loss: 1.49660062789917\n",
      "Epoch: 3280: Train loss: 2.801950693130493 Valid loss: 0.5853619575500488\n",
      "Epoch: 3281: Train loss: 2.8085837364196777 Valid loss: 1.4951260089874268\n",
      "Epoch: 3282: Train loss: 2.800537109375 Valid loss: 0.5863637328147888\n",
      "Epoch: 3283: Train loss: 2.807159185409546 Valid loss: 1.4936686754226685\n",
      "Epoch: 3284: Train loss: 2.799131393432617 Valid loss: 0.5873702764511108\n",
      "Epoch: 3285: Train loss: 2.805741310119629 Valid loss: 1.4922168254852295\n",
      "Epoch: 3286: Train loss: 2.7977285385131836 Valid loss: 0.5883703827857971\n",
      "Epoch: 3287: Train loss: 2.8043227195739746 Valid loss: 1.4907625913619995\n",
      "Epoch: 3288: Train loss: 2.7963268756866455 Valid loss: 0.5893714427947998\n",
      "Epoch: 3289: Train loss: 2.802910566329956 Valid loss: 1.4893217086791992\n",
      "Epoch: 3290: Train loss: 2.794933319091797 Valid loss: 0.590372622013092\n",
      "Epoch: 3291: Train loss: 2.8015012741088867 Valid loss: 1.4878933429718018\n",
      "Epoch: 3292: Train loss: 2.79354190826416 Valid loss: 0.5913721919059753\n",
      "Epoch: 3293: Train loss: 2.800095796585083 Valid loss: 1.4864633083343506\n",
      "Epoch: 3294: Train loss: 2.7921504974365234 Valid loss: 0.5923739671707153\n",
      "Epoch: 3295: Train loss: 2.7986950874328613 Valid loss: 1.4850413799285889\n",
      "Epoch: 3296: Train loss: 2.7907652854919434 Valid loss: 0.5933730006217957\n",
      "Epoch: 3297: Train loss: 2.7972941398620605 Valid loss: 1.4836232662200928\n",
      "Epoch: 3298: Train loss: 2.789381980895996 Valid loss: 0.5943705439567566\n",
      "Epoch: 3299: Train loss: 2.7959001064300537 Valid loss: 1.4822152853012085\n",
      "Epoch: 3300: Train loss: 2.788003444671631 Valid loss: 0.5953693389892578\n",
      "Epoch: 3301: Train loss: 2.7945103645324707 Valid loss: 1.4808202981948853\n",
      "Epoch: 3302: Train loss: 2.786630392074585 Valid loss: 0.5963672995567322\n",
      "Epoch: 3303: Train loss: 2.7931244373321533 Valid loss: 1.4794245958328247\n",
      "Epoch: 3304: Train loss: 2.785261392593384 Valid loss: 0.5973661541938782\n",
      "Epoch: 3305: Train loss: 2.7917425632476807 Valid loss: 1.4780359268188477\n",
      "Epoch: 3306: Train loss: 2.7838950157165527 Valid loss: 0.5983644127845764\n",
      "Epoch: 3307: Train loss: 2.7903642654418945 Valid loss: 1.476660132408142\n",
      "Epoch: 3308: Train loss: 2.78253173828125 Valid loss: 0.599359929561615\n",
      "Epoch: 3309: Train loss: 2.788987159729004 Valid loss: 1.4752804040908813\n",
      "Epoch: 3310: Train loss: 2.781172513961792 Valid loss: 0.6003568172454834\n",
      "Epoch: 3311: Train loss: 2.78761625289917 Valid loss: 1.4739168882369995\n",
      "Epoch: 3312: Train loss: 2.779818058013916 Valid loss: 0.6013543605804443\n",
      "Epoch: 3313: Train loss: 2.786247968673706 Valid loss: 1.4725552797317505\n",
      "Epoch: 3314: Train loss: 2.7784671783447266 Valid loss: 0.6023468971252441\n",
      "Epoch: 3315: Train loss: 2.7848825454711914 Valid loss: 1.4712028503417969\n",
      "Epoch: 3316: Train loss: 2.7771170139312744 Valid loss: 0.6033408045768738\n",
      "Epoch: 3317: Train loss: 2.783519983291626 Valid loss: 1.4698495864868164\n",
      "Epoch: 3318: Train loss: 2.7757692337036133 Valid loss: 0.6043344736099243\n",
      "Epoch: 3319: Train loss: 2.7821595668792725 Valid loss: 1.4685041904449463\n",
      "Epoch: 3320: Train loss: 2.7744243144989014 Valid loss: 0.6053256392478943\n",
      "Epoch: 3321: Train loss: 2.7808003425598145 Valid loss: 1.4671642780303955\n",
      "Epoch: 3322: Train loss: 2.7730817794799805 Valid loss: 0.6063135862350464\n",
      "Epoch: 3323: Train loss: 2.7794458866119385 Valid loss: 1.4658284187316895\n",
      "Epoch: 3324: Train loss: 2.7717444896698 Valid loss: 0.607306718826294\n",
      "Epoch: 3325: Train loss: 2.7780981063842773 Valid loss: 1.4645025730133057\n",
      "Epoch: 3326: Train loss: 2.770411491394043 Valid loss: 0.6082959175109863\n",
      "Epoch: 3327: Train loss: 2.7767529487609863 Valid loss: 1.4631834030151367\n",
      "Epoch: 3328: Train loss: 2.7690839767456055 Valid loss: 0.6092881560325623\n",
      "Epoch: 3329: Train loss: 2.7754108905792236 Valid loss: 1.461869716644287\n",
      "Epoch: 3330: Train loss: 2.767759084701538 Valid loss: 0.6102762222290039\n",
      "Epoch: 3331: Train loss: 2.774071455001831 Valid loss: 1.4605605602264404\n",
      "Epoch: 3332: Train loss: 2.766434907913208 Valid loss: 0.6112651228904724\n",
      "Epoch: 3333: Train loss: 2.7727346420288086 Valid loss: 1.4592596292495728\n",
      "Epoch: 3334: Train loss: 2.7651169300079346 Valid loss: 0.6122531890869141\n",
      "Epoch: 3335: Train loss: 2.771404981613159 Valid loss: 1.457963228225708\n",
      "Epoch: 3336: Train loss: 2.763803243637085 Valid loss: 0.6132422685623169\n",
      "Epoch: 3337: Train loss: 2.77007794380188 Valid loss: 1.456676721572876\n",
      "Epoch: 3338: Train loss: 2.762493371963501 Valid loss: 0.6142293810844421\n",
      "Epoch: 3339: Train loss: 2.7687559127807617 Valid loss: 1.455397367477417\n",
      "Epoch: 3340: Train loss: 2.7611842155456543 Valid loss: 0.6152184009552002\n",
      "Epoch: 3341: Train loss: 2.7674360275268555 Valid loss: 1.4541245698928833\n",
      "Epoch: 3342: Train loss: 2.759883165359497 Valid loss: 0.616205096244812\n",
      "Epoch: 3343: Train loss: 2.7661192417144775 Valid loss: 1.4528557062149048\n",
      "Epoch: 3344: Train loss: 2.758582353591919 Valid loss: 0.6171894669532776\n",
      "Epoch: 3345: Train loss: 2.76480770111084 Valid loss: 1.4515959024429321\n",
      "Epoch: 3346: Train loss: 2.7572858333587646 Valid loss: 0.6181761622428894\n",
      "Epoch: 3347: Train loss: 2.7634963989257812 Valid loss: 1.4503414630889893\n",
      "Epoch: 3348: Train loss: 2.7559919357299805 Valid loss: 0.6191608905792236\n",
      "Epoch: 3349: Train loss: 2.7621870040893555 Valid loss: 1.4490869045257568\n",
      "Epoch: 3350: Train loss: 2.7546987533569336 Valid loss: 0.6201424598693848\n",
      "Epoch: 3351: Train loss: 2.7608816623687744 Valid loss: 1.4478398561477661\n",
      "Epoch: 3352: Train loss: 2.753411054611206 Valid loss: 0.6211237907409668\n",
      "Epoch: 3353: Train loss: 2.759582996368408 Valid loss: 1.4466058015823364\n",
      "Epoch: 3354: Train loss: 2.752126932144165 Valid loss: 0.6221060156822205\n",
      "Epoch: 3355: Train loss: 2.7582881450653076 Valid loss: 1.4453763961791992\n",
      "Epoch: 3356: Train loss: 2.750849723815918 Valid loss: 0.623090386390686\n",
      "Epoch: 3357: Train loss: 2.7569949626922607 Valid loss: 1.4441505670547485\n",
      "Epoch: 3358: Train loss: 2.7495718002319336 Valid loss: 0.6240731477737427\n",
      "Epoch: 3359: Train loss: 2.755704641342163 Valid loss: 1.4429302215576172\n",
      "Epoch: 3360: Train loss: 2.748297691345215 Valid loss: 0.6250529289245605\n",
      "Epoch: 3361: Train loss: 2.7544162273406982 Valid loss: 1.441717267036438\n",
      "Epoch: 3362: Train loss: 2.747023820877075 Valid loss: 0.626031219959259\n",
      "Epoch: 3363: Train loss: 2.7531309127807617 Valid loss: 1.4405019283294678\n",
      "Epoch: 3364: Train loss: 2.7457573413848877 Valid loss: 0.627011239528656\n",
      "Epoch: 3365: Train loss: 2.7518510818481445 Valid loss: 1.439302682876587\n",
      "Epoch: 3366: Train loss: 2.7444918155670166 Valid loss: 0.6279886960983276\n",
      "Epoch: 3367: Train loss: 2.7505717277526855 Valid loss: 1.4381030797958374\n",
      "Epoch: 3368: Train loss: 2.743227005004883 Valid loss: 0.6289648413658142\n",
      "Epoch: 3369: Train loss: 2.7492947578430176 Valid loss: 1.4369089603424072\n",
      "Epoch: 3370: Train loss: 2.741966962814331 Valid loss: 0.6299430131912231\n",
      "Epoch: 3371: Train loss: 2.7480216026306152 Valid loss: 1.4357165098190308\n",
      "Epoch: 3372: Train loss: 2.740706443786621 Valid loss: 0.630915641784668\n",
      "Epoch: 3373: Train loss: 2.7467498779296875 Valid loss: 1.4345279932022095\n",
      "Epoch: 3374: Train loss: 2.739457607269287 Valid loss: 0.6318908333778381\n",
      "Epoch: 3375: Train loss: 2.745488166809082 Valid loss: 1.433353066444397\n",
      "Epoch: 3376: Train loss: 2.7382075786590576 Valid loss: 0.6328670382499695\n",
      "Epoch: 3377: Train loss: 2.7442235946655273 Valid loss: 1.4321852922439575\n",
      "Epoch: 3378: Train loss: 2.736964702606201 Valid loss: 0.6338412761688232\n",
      "Epoch: 3379: Train loss: 2.742969036102295 Valid loss: 1.4310230016708374\n",
      "Epoch: 3380: Train loss: 2.735724449157715 Valid loss: 0.6348156332969666\n",
      "Epoch: 3381: Train loss: 2.741716146469116 Valid loss: 1.4298691749572754\n",
      "Epoch: 3382: Train loss: 2.7344889640808105 Valid loss: 0.6357919573783875\n",
      "Epoch: 3383: Train loss: 2.7404651641845703 Valid loss: 1.4287185668945312\n",
      "Epoch: 3384: Train loss: 2.7332520484924316 Valid loss: 0.6367658376693726\n",
      "Epoch: 3385: Train loss: 2.7392208576202393 Valid loss: 1.4275810718536377\n",
      "Epoch: 3386: Train loss: 2.7320234775543213 Valid loss: 0.6377423405647278\n",
      "Epoch: 3387: Train loss: 2.7379775047302246 Valid loss: 1.4264432191848755\n",
      "Epoch: 3388: Train loss: 2.73079776763916 Valid loss: 0.6387134790420532\n",
      "Epoch: 3389: Train loss: 2.73673415184021 Valid loss: 1.4253085851669312\n",
      "Epoch: 3390: Train loss: 2.729569673538208 Valid loss: 0.6396836042404175\n",
      "Epoch: 3391: Train loss: 2.7354965209960938 Valid loss: 1.4241845607757568\n",
      "Epoch: 3392: Train loss: 2.7283453941345215 Valid loss: 0.6406537294387817\n",
      "Epoch: 3393: Train loss: 2.7342591285705566 Valid loss: 1.4230555295944214\n",
      "Epoch: 3394: Train loss: 2.7271273136138916 Valid loss: 0.6416221857070923\n",
      "Epoch: 3395: Train loss: 2.733027696609497 Valid loss: 1.421943187713623\n",
      "Epoch: 3396: Train loss: 2.7259116172790527 Valid loss: 0.6425937414169312\n",
      "Epoch: 3397: Train loss: 2.731799602508545 Valid loss: 1.4208335876464844\n",
      "Epoch: 3398: Train loss: 2.724699020385742 Valid loss: 0.6435601711273193\n",
      "Epoch: 3399: Train loss: 2.730574369430542 Valid loss: 1.4197279214859009\n",
      "Epoch: 3400: Train loss: 2.7234902381896973 Valid loss: 0.6445303559303284\n",
      "Epoch: 3401: Train loss: 2.729353666305542 Valid loss: 1.4186279773712158\n",
      "Epoch: 3402: Train loss: 2.722285509109497 Valid loss: 0.6454986929893494\n",
      "Epoch: 3403: Train loss: 2.7281322479248047 Valid loss: 1.4175364971160889\n",
      "Epoch: 3404: Train loss: 2.7210781574249268 Valid loss: 0.6464613676071167\n",
      "Epoch: 3405: Train loss: 2.7269127368927 Valid loss: 1.4164402484893799\n",
      "Epoch: 3406: Train loss: 2.719876289367676 Valid loss: 0.6474283933639526\n",
      "Epoch: 3407: Train loss: 2.725703716278076 Valid loss: 1.4153633117675781\n",
      "Epoch: 3408: Train loss: 2.7186827659606934 Valid loss: 0.648393452167511\n",
      "Epoch: 3409: Train loss: 2.7244927883148193 Valid loss: 1.4142788648605347\n",
      "Epoch: 3410: Train loss: 2.717487335205078 Valid loss: 0.6493560075759888\n",
      "Epoch: 3411: Train loss: 2.723283529281616 Valid loss: 1.4132120609283447\n",
      "Epoch: 3412: Train loss: 2.716295003890991 Valid loss: 0.6503216028213501\n",
      "Epoch: 3413: Train loss: 2.722081184387207 Valid loss: 1.4121431112289429\n",
      "Epoch: 3414: Train loss: 2.7151076793670654 Valid loss: 0.6512823104858398\n",
      "Epoch: 3415: Train loss: 2.720876932144165 Valid loss: 1.4110764265060425\n",
      "Epoch: 3416: Train loss: 2.713918924331665 Valid loss: 0.6522437334060669\n",
      "Epoch: 3417: Train loss: 2.7196760177612305 Valid loss: 1.4100220203399658\n",
      "Epoch: 3418: Train loss: 2.7127387523651123 Valid loss: 0.6532054543495178\n",
      "Epoch: 3419: Train loss: 2.71848464012146 Valid loss: 1.4089736938476562\n",
      "Epoch: 3420: Train loss: 2.711561679840088 Valid loss: 0.6541675329208374\n",
      "Epoch: 3421: Train loss: 2.717294454574585 Valid loss: 1.407923936843872\n",
      "Epoch: 3422: Train loss: 2.7103846073150635 Valid loss: 0.6551308035850525\n",
      "Epoch: 3423: Train loss: 2.716104507446289 Valid loss: 1.406888723373413\n",
      "Epoch: 3424: Train loss: 2.7092134952545166 Valid loss: 0.656090497970581\n",
      "Epoch: 3425: Train loss: 2.7149226665496826 Valid loss: 1.4058585166931152\n",
      "Epoch: 3426: Train loss: 2.7080438137054443 Valid loss: 0.6570506691932678\n",
      "Epoch: 3427: Train loss: 2.713743209838867 Valid loss: 1.4048312902450562\n",
      "Epoch: 3428: Train loss: 2.706881523132324 Valid loss: 0.6580107808113098\n",
      "Epoch: 3429: Train loss: 2.712564706802368 Valid loss: 1.4038118124008179\n",
      "Epoch: 3430: Train loss: 2.7057206630706787 Valid loss: 0.6589686274528503\n",
      "Epoch: 3431: Train loss: 2.7113876342773438 Valid loss: 1.4027982950210571\n",
      "Epoch: 3432: Train loss: 2.7045578956604004 Valid loss: 0.6599265336990356\n",
      "Epoch: 3433: Train loss: 2.7102155685424805 Valid loss: 1.4017888307571411\n",
      "Epoch: 3434: Train loss: 2.703403949737549 Valid loss: 0.6608846187591553\n",
      "Epoch: 3435: Train loss: 2.709047317504883 Valid loss: 1.4007841348648071\n",
      "Epoch: 3436: Train loss: 2.7022504806518555 Valid loss: 0.6618413329124451\n",
      "Epoch: 3437: Train loss: 2.70788311958313 Valid loss: 1.399785041809082\n",
      "Epoch: 3438: Train loss: 2.7011003494262695 Valid loss: 0.6627975106239319\n",
      "Epoch: 3439: Train loss: 2.7067160606384277 Valid loss: 1.3987853527069092\n",
      "Epoch: 3440: Train loss: 2.6999502182006836 Valid loss: 0.6637510657310486\n",
      "Epoch: 3441: Train loss: 2.705554962158203 Valid loss: 1.3977928161621094\n",
      "Epoch: 3442: Train loss: 2.6988024711608887 Valid loss: 0.6647003889083862\n",
      "Epoch: 3443: Train loss: 2.7043962478637695 Valid loss: 1.3968054056167603\n",
      "Epoch: 3444: Train loss: 2.6976613998413086 Valid loss: 0.665656328201294\n",
      "Epoch: 3445: Train loss: 2.7032418251037598 Valid loss: 1.395829200744629\n",
      "Epoch: 3446: Train loss: 2.6965246200561523 Valid loss: 0.6666092872619629\n",
      "Epoch: 3447: Train loss: 2.7020907402038574 Valid loss: 1.3948558568954468\n",
      "Epoch: 3448: Train loss: 2.695387363433838 Valid loss: 0.6675612330436707\n",
      "Epoch: 3449: Train loss: 2.700944423675537 Valid loss: 1.3938891887664795\n",
      "Epoch: 3450: Train loss: 2.694256067276001 Valid loss: 0.6685149669647217\n",
      "Epoch: 3451: Train loss: 2.6997976303100586 Valid loss: 1.3929256200790405\n",
      "Epoch: 3452: Train loss: 2.6931252479553223 Valid loss: 0.6694650053977966\n",
      "Epoch: 3453: Train loss: 2.6986515522003174 Valid loss: 1.3919605016708374\n",
      "Epoch: 3454: Train loss: 2.69199538230896 Valid loss: 0.6704142689704895\n",
      "Epoch: 3455: Train loss: 2.6975131034851074 Valid loss: 1.391008734703064\n",
      "Epoch: 3456: Train loss: 2.6908750534057617 Valid loss: 0.6713650226593018\n",
      "Epoch: 3457: Train loss: 2.696380376815796 Valid loss: 1.390059471130371\n",
      "Epoch: 3458: Train loss: 2.6897547245025635 Valid loss: 0.6723126769065857\n",
      "Epoch: 3459: Train loss: 2.6952438354492188 Valid loss: 1.3891165256500244\n",
      "Epoch: 3460: Train loss: 2.688636541366577 Valid loss: 0.6732622385025024\n",
      "Epoch: 3461: Train loss: 2.694114923477173 Valid loss: 1.388183832168579\n",
      "Epoch: 3462: Train loss: 2.6875200271606445 Valid loss: 0.6742095351219177\n",
      "Epoch: 3463: Train loss: 2.6929852962493896 Valid loss: 1.38724684715271\n",
      "Epoch: 3464: Train loss: 2.6864101886749268 Valid loss: 0.6751563549041748\n",
      "Epoch: 3465: Train loss: 2.691861152648926 Valid loss: 1.3863235712051392\n",
      "Epoch: 3466: Train loss: 2.685299873352051 Valid loss: 0.6761028170585632\n",
      "Epoch: 3467: Train loss: 2.6907384395599365 Valid loss: 1.385401964187622\n",
      "Epoch: 3468: Train loss: 2.6841936111450195 Valid loss: 0.6770493388175964\n",
      "Epoch: 3469: Train loss: 2.6896212100982666 Valid loss: 1.3844850063323975\n",
      "Epoch: 3470: Train loss: 2.6830894947052 Valid loss: 0.6779930591583252\n",
      "Epoch: 3471: Train loss: 2.6885032653808594 Valid loss: 1.383574366569519\n",
      "Epoch: 3472: Train loss: 2.6819894313812256 Valid loss: 0.6789373755455017\n",
      "Epoch: 3473: Train loss: 2.687392234802246 Valid loss: 1.382664680480957\n",
      "Epoch: 3474: Train loss: 2.6808910369873047 Valid loss: 0.6798816919326782\n",
      "Epoch: 3475: Train loss: 2.6862776279449463 Valid loss: 1.3817625045776367\n",
      "Epoch: 3476: Train loss: 2.679793357849121 Valid loss: 0.6808225512504578\n",
      "Epoch: 3477: Train loss: 2.685170888900757 Valid loss: 1.380866527557373\n",
      "Epoch: 3478: Train loss: 2.6787025928497314 Valid loss: 0.6817659735679626\n",
      "Epoch: 3479: Train loss: 2.6840672492980957 Valid loss: 1.3799705505371094\n",
      "Epoch: 3480: Train loss: 2.6776113510131836 Valid loss: 0.6827061176300049\n",
      "Epoch: 3481: Train loss: 2.6829640865325928 Valid loss: 1.379083275794983\n",
      "Epoch: 3482: Train loss: 2.676529884338379 Valid loss: 0.6836491823196411\n",
      "Epoch: 3483: Train loss: 2.6818695068359375 Valid loss: 1.378206491470337\n",
      "Epoch: 3484: Train loss: 2.675446033477783 Valid loss: 0.6845894455909729\n",
      "Epoch: 3485: Train loss: 2.6807754039764404 Valid loss: 1.3773393630981445\n",
      "Epoch: 3486: Train loss: 2.674365520477295 Valid loss: 0.6855295300483704\n",
      "Epoch: 3487: Train loss: 2.679680824279785 Valid loss: 1.3764622211456299\n",
      "Epoch: 3488: Train loss: 2.6732888221740723 Valid loss: 0.6864669322967529\n",
      "Epoch: 3489: Train loss: 2.6785922050476074 Valid loss: 1.3755983114242554\n",
      "Epoch: 3490: Train loss: 2.6722142696380615 Valid loss: 0.6874047517776489\n",
      "Epoch: 3491: Train loss: 2.677504777908325 Valid loss: 1.3747386932373047\n",
      "Epoch: 3492: Train loss: 2.671142816543579 Valid loss: 0.688343346118927\n",
      "Epoch: 3493: Train loss: 2.6764183044433594 Valid loss: 1.37388014793396\n",
      "Epoch: 3494: Train loss: 2.670072317123413 Valid loss: 0.6892799139022827\n",
      "Epoch: 3495: Train loss: 2.675334930419922 Valid loss: 1.3730223178863525\n",
      "Epoch: 3496: Train loss: 2.669003486633301 Valid loss: 0.6902154684066772\n",
      "Epoch: 3497: Train loss: 2.6742539405822754 Valid loss: 1.3721739053726196\n",
      "Epoch: 3498: Train loss: 2.667940855026245 Valid loss: 0.6911478638648987\n",
      "Epoch: 3499: Train loss: 2.6731793880462646 Valid loss: 1.3713321685791016\n",
      "Epoch: 3500: Train loss: 2.6668782234191895 Valid loss: 0.6920832395553589\n",
      "Epoch: 3501: Train loss: 2.6721086502075195 Valid loss: 1.3704965114593506\n",
      "Epoch: 3502: Train loss: 2.665822982788086 Valid loss: 0.693014919757843\n",
      "Epoch: 3503: Train loss: 2.671034574508667 Valid loss: 1.369663953781128\n",
      "Epoch: 3504: Train loss: 2.664764642715454 Valid loss: 0.6939495801925659\n",
      "Epoch: 3505: Train loss: 2.66996431350708 Valid loss: 1.3688323497772217\n",
      "Epoch: 3506: Train loss: 2.663712501525879 Valid loss: 0.6948802471160889\n",
      "Epoch: 3507: Train loss: 2.668900489807129 Valid loss: 1.368011236190796\n",
      "Epoch: 3508: Train loss: 2.6626627445220947 Valid loss: 0.6958123445510864\n",
      "Epoch: 3509: Train loss: 2.6678378582000732 Valid loss: 1.3671923875808716\n",
      "Epoch: 3510: Train loss: 2.661613702774048 Valid loss: 0.696744441986084\n",
      "Epoch: 3511: Train loss: 2.666778802871704 Valid loss: 1.366381049156189\n",
      "Epoch: 3512: Train loss: 2.660569190979004 Valid loss: 0.6976723074913025\n",
      "Epoch: 3513: Train loss: 2.6657187938690186 Valid loss: 1.3655719757080078\n",
      "Epoch: 3514: Train loss: 2.659525156021118 Valid loss: 0.698600709438324\n",
      "Epoch: 3515: Train loss: 2.6646618843078613 Valid loss: 1.3647606372833252\n",
      "Epoch: 3516: Train loss: 2.658482313156128 Valid loss: 0.69952791929245\n",
      "Epoch: 3517: Train loss: 2.663607597351074 Valid loss: 1.3639588356018066\n",
      "Epoch: 3518: Train loss: 2.657444953918457 Valid loss: 0.7004542350769043\n",
      "Epoch: 3519: Train loss: 2.662557363510132 Valid loss: 1.3631618022918701\n",
      "Epoch: 3520: Train loss: 2.6564080715179443 Valid loss: 0.7013816833496094\n",
      "Epoch: 3521: Train loss: 2.6615097522735596 Valid loss: 1.3623677492141724\n",
      "Epoch: 3522: Train loss: 2.6553757190704346 Valid loss: 0.7023069858551025\n",
      "Epoch: 3523: Train loss: 2.6604669094085693 Valid loss: 1.3615792989730835\n",
      "Epoch: 3524: Train loss: 2.6543490886688232 Valid loss: 0.7032320499420166\n",
      "Epoch: 3525: Train loss: 2.6594271659851074 Valid loss: 1.36080002784729\n",
      "Epoch: 3526: Train loss: 2.6533238887786865 Valid loss: 0.7041606307029724\n",
      "Epoch: 3527: Train loss: 2.658388614654541 Valid loss: 1.3600239753723145\n",
      "Epoch: 3528: Train loss: 2.6523046493530273 Valid loss: 0.7050856947898865\n",
      "Epoch: 3529: Train loss: 2.6573567390441895 Valid loss: 1.3592603206634521\n",
      "Epoch: 3530: Train loss: 2.6512844562530518 Valid loss: 0.7060102224349976\n",
      "Epoch: 3531: Train loss: 2.6563258171081543 Valid loss: 1.3584985733032227\n",
      "Epoch: 3532: Train loss: 2.650268793106079 Valid loss: 0.7069340348243713\n",
      "Epoch: 3533: Train loss: 2.6552937030792236 Valid loss: 1.3577299118041992\n",
      "Epoch: 3534: Train loss: 2.649254322052002 Valid loss: 0.7078567147254944\n",
      "Epoch: 3535: Train loss: 2.6542727947235107 Valid loss: 1.3569865226745605\n",
      "Epoch: 3536: Train loss: 2.648245334625244 Valid loss: 0.7087814807891846\n",
      "Epoch: 3537: Train loss: 2.6532466411590576 Valid loss: 1.3562328815460205\n",
      "Epoch: 3538: Train loss: 2.6472349166870117 Valid loss: 0.7097012996673584\n",
      "Epoch: 3539: Train loss: 2.6522254943847656 Valid loss: 1.355483889579773\n",
      "Epoch: 3540: Train loss: 2.6462299823760986 Valid loss: 0.7106221318244934\n",
      "Epoch: 3541: Train loss: 2.6512057781219482 Valid loss: 1.3547419309616089\n",
      "Epoch: 3542: Train loss: 2.645221710205078 Valid loss: 0.7115399241447449\n",
      "Epoch: 3543: Train loss: 2.6501874923706055 Valid loss: 1.3539985418319702\n",
      "Epoch: 3544: Train loss: 2.6442155838012695 Valid loss: 0.7124559283256531\n",
      "Epoch: 3545: Train loss: 2.649169445037842 Valid loss: 1.353252649307251\n",
      "Epoch: 3546: Train loss: 2.6432173252105713 Valid loss: 0.7133753895759583\n",
      "Epoch: 3547: Train loss: 2.6481568813323975 Valid loss: 1.352526307106018\n",
      "Epoch: 3548: Train loss: 2.6422171592712402 Valid loss: 0.7142902612686157\n",
      "Epoch: 3549: Train loss: 2.6471447944641113 Valid loss: 1.3517965078353882\n",
      "Epoch: 3550: Train loss: 2.641221761703491 Valid loss: 0.7152062058448792\n",
      "Epoch: 3551: Train loss: 2.646138906478882 Valid loss: 1.3510748147964478\n",
      "Epoch: 3552: Train loss: 2.640230894088745 Valid loss: 0.7161237597465515\n",
      "Epoch: 3553: Train loss: 2.6451354026794434 Valid loss: 1.3503516912460327\n",
      "Epoch: 3554: Train loss: 2.63924241065979 Valid loss: 0.7170372009277344\n",
      "Epoch: 3555: Train loss: 2.6441354751586914 Valid loss: 1.3496437072753906\n",
      "Epoch: 3556: Train loss: 2.6382577419281006 Valid loss: 0.7179557681083679\n",
      "Epoch: 3557: Train loss: 2.6431384086608887 Valid loss: 1.3489413261413574\n",
      "Epoch: 3558: Train loss: 2.6372745037078857 Valid loss: 0.7188684344291687\n",
      "Epoch: 3559: Train loss: 2.6421401500701904 Valid loss: 1.3482370376586914\n",
      "Epoch: 3560: Train loss: 2.636291742324829 Valid loss: 0.7197809815406799\n",
      "Epoch: 3561: Train loss: 2.6411492824554443 Valid loss: 1.3475364446640015\n",
      "Epoch: 3562: Train loss: 2.63531494140625 Valid loss: 0.72069251537323\n",
      "Epoch: 3563: Train loss: 2.640155076980591 Valid loss: 1.3468475341796875\n",
      "Epoch: 3564: Train loss: 2.634335994720459 Valid loss: 0.7216047644615173\n",
      "Epoch: 3565: Train loss: 2.6391682624816895 Valid loss: 1.3461554050445557\n",
      "Epoch: 3566: Train loss: 2.63336181640625 Valid loss: 0.722513735294342\n",
      "Epoch: 3567: Train loss: 2.638178825378418 Valid loss: 1.345467209815979\n",
      "Epoch: 3568: Train loss: 2.63238787651062 Valid loss: 0.7234221696853638\n",
      "Epoch: 3569: Train loss: 2.637197494506836 Valid loss: 1.344787359237671\n",
      "Epoch: 3570: Train loss: 2.6314191818237305 Valid loss: 0.7243335843086243\n",
      "Epoch: 3571: Train loss: 2.6362123489379883 Valid loss: 1.3441076278686523\n",
      "Epoch: 3572: Train loss: 2.63045072555542 Valid loss: 0.7252405881881714\n",
      "Epoch: 3573: Train loss: 2.6352343559265137 Valid loss: 1.343430519104004\n",
      "Epoch: 3574: Train loss: 2.629484176635742 Valid loss: 0.7261477708816528\n",
      "Epoch: 3575: Train loss: 2.6342551708221436 Valid loss: 1.3427565097808838\n",
      "Epoch: 3576: Train loss: 2.6285240650177 Valid loss: 0.7270536422729492\n",
      "Epoch: 3577: Train loss: 2.6332809925079346 Valid loss: 1.3420913219451904\n",
      "Epoch: 3578: Train loss: 2.627562999725342 Valid loss: 0.7279594540596008\n",
      "Epoch: 3579: Train loss: 2.6323091983795166 Valid loss: 1.3414324522018433\n",
      "Epoch: 3580: Train loss: 2.62660551071167 Valid loss: 0.7288635969161987\n",
      "Epoch: 3581: Train loss: 2.6313395500183105 Valid loss: 1.3407721519470215\n",
      "Epoch: 3582: Train loss: 2.6256532669067383 Valid loss: 0.729769229888916\n",
      "Epoch: 3583: Train loss: 2.6303751468658447 Valid loss: 1.3401222229003906\n",
      "Epoch: 3584: Train loss: 2.6247003078460693 Valid loss: 0.7306748628616333\n",
      "Epoch: 3585: Train loss: 2.629411458969116 Valid loss: 1.3394749164581299\n",
      "Epoch: 3586: Train loss: 2.6237525939941406 Valid loss: 0.7315769195556641\n",
      "Epoch: 3587: Train loss: 2.628450870513916 Valid loss: 1.3388334512710571\n",
      "Epoch: 3588: Train loss: 2.62280535697937 Valid loss: 0.732479453086853\n",
      "Epoch: 3589: Train loss: 2.6274919509887695 Valid loss: 1.338194489479065\n",
      "Epoch: 3590: Train loss: 2.6218607425689697 Valid loss: 0.7333789467811584\n",
      "Epoch: 3591: Train loss: 2.626535177230835 Valid loss: 1.3375595808029175\n",
      "Epoch: 3592: Train loss: 2.620917797088623 Valid loss: 0.7342819571495056\n",
      "Epoch: 3593: Train loss: 2.6255788803100586 Valid loss: 1.3369228839874268\n",
      "Epoch: 3594: Train loss: 2.6199750900268555 Valid loss: 0.7351802587509155\n",
      "Epoch: 3595: Train loss: 2.624624490737915 Valid loss: 1.3362921476364136\n",
      "Epoch: 3596: Train loss: 2.6190342903137207 Valid loss: 0.7360760569572449\n",
      "Epoch: 3597: Train loss: 2.623673915863037 Valid loss: 1.3356633186340332\n",
      "Epoch: 3598: Train loss: 2.6180992126464844 Valid loss: 0.7369741797447205\n",
      "Epoch: 3599: Train loss: 2.6227242946624756 Valid loss: 1.3350458145141602\n",
      "Epoch: 3600: Train loss: 2.6171650886535645 Valid loss: 0.737873375415802\n",
      "Epoch: 3601: Train loss: 2.6217775344848633 Valid loss: 1.3344227075576782\n",
      "Epoch: 3602: Train loss: 2.6162362098693848 Valid loss: 0.7387700080871582\n",
      "Epoch: 3603: Train loss: 2.6208395957946777 Valid loss: 1.333817720413208\n",
      "Epoch: 3604: Train loss: 2.6153101921081543 Valid loss: 0.7396687269210815\n",
      "Epoch: 3605: Train loss: 2.6199002265930176 Valid loss: 1.3332079648971558\n",
      "Epoch: 3606: Train loss: 2.614384174346924 Valid loss: 0.7405630350112915\n",
      "Epoch: 3607: Train loss: 2.6189610958099365 Valid loss: 1.3326034545898438\n",
      "Epoch: 3608: Train loss: 2.613459587097168 Valid loss: 0.7414584159851074\n",
      "Epoch: 3609: Train loss: 2.618027687072754 Valid loss: 1.332004427909851\n",
      "Epoch: 3610: Train loss: 2.6125404834747314 Valid loss: 0.7423517107963562\n",
      "Epoch: 3611: Train loss: 2.617095470428467 Valid loss: 1.3314149379730225\n",
      "Epoch: 3612: Train loss: 2.6116225719451904 Valid loss: 0.7432465553283691\n",
      "Epoch: 3613: Train loss: 2.616164445877075 Valid loss: 1.3308236598968506\n",
      "Epoch: 3614: Train loss: 2.610706090927124 Valid loss: 0.7441392540931702\n",
      "Epoch: 3615: Train loss: 2.615239143371582 Valid loss: 1.3302404880523682\n",
      "Epoch: 3616: Train loss: 2.6097936630249023 Valid loss: 0.7450301647186279\n",
      "Epoch: 3617: Train loss: 2.6143155097961426 Valid loss: 1.3296620845794678\n",
      "Epoch: 3618: Train loss: 2.608884572982788 Valid loss: 0.7459239363670349\n",
      "Epoch: 3619: Train loss: 2.6133947372436523 Valid loss: 1.3290847539901733\n",
      "Epoch: 3620: Train loss: 2.607980251312256 Valid loss: 0.7468167543411255\n",
      "Epoch: 3621: Train loss: 2.6124775409698486 Valid loss: 1.3285187482833862\n",
      "Epoch: 3622: Train loss: 2.6070733070373535 Valid loss: 0.7477079629898071\n",
      "Epoch: 3623: Train loss: 2.6115572452545166 Valid loss: 1.3279426097869873\n",
      "Epoch: 3624: Train loss: 2.606168746948242 Valid loss: 0.7485941648483276\n",
      "Epoch: 3625: Train loss: 2.610642433166504 Valid loss: 1.3273788690567017\n",
      "Epoch: 3626: Train loss: 2.605266571044922 Valid loss: 0.7494834661483765\n",
      "Epoch: 3627: Train loss: 2.609726905822754 Valid loss: 1.326817512512207\n",
      "Epoch: 3628: Train loss: 2.6043646335601807 Valid loss: 0.7503678798675537\n",
      "Epoch: 3629: Train loss: 2.608813762664795 Valid loss: 1.326256275177002\n",
      "Epoch: 3630: Train loss: 2.603468656539917 Valid loss: 0.7512562274932861\n",
      "Epoch: 3631: Train loss: 2.6079070568084717 Valid loss: 1.3257062435150146\n",
      "Epoch: 3632: Train loss: 2.6025755405426025 Valid loss: 0.7521454691886902\n",
      "Epoch: 3633: Train loss: 2.607003927230835 Valid loss: 1.3251590728759766\n",
      "Epoch: 3634: Train loss: 2.6016855239868164 Valid loss: 0.7530310153961182\n",
      "Epoch: 3635: Train loss: 2.606100559234619 Valid loss: 1.324610710144043\n",
      "Epoch: 3636: Train loss: 2.600795269012451 Valid loss: 0.7539147138595581\n",
      "Epoch: 3637: Train loss: 2.6051998138427734 Valid loss: 1.3240708112716675\n",
      "Epoch: 3638: Train loss: 2.599909782409668 Valid loss: 0.7548018097877502\n",
      "Epoch: 3639: Train loss: 2.604301929473877 Valid loss: 1.3235418796539307\n",
      "Epoch: 3640: Train loss: 2.5990240573883057 Valid loss: 0.7556835412979126\n",
      "Epoch: 3641: Train loss: 2.6034038066864014 Valid loss: 1.3230056762695312\n",
      "Epoch: 3642: Train loss: 2.598140239715576 Valid loss: 0.756567120552063\n",
      "Epoch: 3643: Train loss: 2.6025071144104004 Valid loss: 1.3224685192108154\n",
      "Epoch: 3644: Train loss: 2.597257614135742 Valid loss: 0.7574476003646851\n",
      "Epoch: 3645: Train loss: 2.601613998413086 Valid loss: 1.321939468383789\n",
      "Epoch: 3646: Train loss: 2.596379041671753 Valid loss: 0.7583292722702026\n",
      "Epoch: 3647: Train loss: 2.6007237434387207 Valid loss: 1.3214112520217896\n",
      "Epoch: 3648: Train loss: 2.595503091812134 Valid loss: 0.7592072486877441\n",
      "Epoch: 3649: Train loss: 2.5998361110687256 Valid loss: 1.3209012746810913\n",
      "Epoch: 3650: Train loss: 2.5946297645568848 Valid loss: 0.7600884437561035\n",
      "Epoch: 3651: Train loss: 2.598952293395996 Valid loss: 1.3203884363174438\n",
      "Epoch: 3652: Train loss: 2.5937581062316895 Valid loss: 0.7609670162200928\n",
      "Epoch: 3653: Train loss: 2.5980682373046875 Valid loss: 1.319878101348877\n",
      "Epoch: 3654: Train loss: 2.5928893089294434 Valid loss: 0.7618445754051208\n",
      "Epoch: 3655: Train loss: 2.597186326980591 Valid loss: 1.3193645477294922\n",
      "Epoch: 3656: Train loss: 2.5920207500457764 Valid loss: 0.7627221345901489\n",
      "Epoch: 3657: Train loss: 2.596306800842285 Valid loss: 1.318861722946167\n",
      "Epoch: 3658: Train loss: 2.5911545753479004 Valid loss: 0.7635989785194397\n",
      "Epoch: 3659: Train loss: 2.5954294204711914 Valid loss: 1.3183624744415283\n",
      "Epoch: 3660: Train loss: 2.590290069580078 Valid loss: 0.7644740343093872\n",
      "Epoch: 3661: Train loss: 2.594557046890259 Valid loss: 1.3178675174713135\n",
      "Epoch: 3662: Train loss: 2.5894341468811035 Valid loss: 0.7653507590293884\n",
      "Epoch: 3663: Train loss: 2.59368634223938 Valid loss: 1.3173812627792358\n",
      "Epoch: 3664: Train loss: 2.5885777473449707 Valid loss: 0.7662287950515747\n",
      "Epoch: 3665: Train loss: 2.592818260192871 Valid loss: 1.3168938159942627\n",
      "Epoch: 3666: Train loss: 2.587716579437256 Valid loss: 0.7670982480049133\n",
      "Epoch: 3667: Train loss: 2.5919482707977295 Valid loss: 1.3164067268371582\n",
      "Epoch: 3668: Train loss: 2.586862325668335 Valid loss: 0.7679710984230042\n",
      "Epoch: 3669: Train loss: 2.591081380844116 Valid loss: 1.3159295320510864\n",
      "Epoch: 3670: Train loss: 2.586010456085205 Valid loss: 0.7688438892364502\n",
      "Epoch: 3671: Train loss: 2.5902175903320312 Valid loss: 1.3154464960098267\n",
      "Epoch: 3672: Train loss: 2.5851590633392334 Valid loss: 0.7697128653526306\n",
      "Epoch: 3673: Train loss: 2.589355230331421 Valid loss: 1.3149707317352295\n",
      "Epoch: 3674: Train loss: 2.584312915802002 Valid loss: 0.7705850005149841\n",
      "Epoch: 3675: Train loss: 2.588495969772339 Valid loss: 1.3145017623901367\n",
      "Epoch: 3676: Train loss: 2.583465337753296 Valid loss: 0.7714536786079407\n",
      "Epoch: 3677: Train loss: 2.587636947631836 Valid loss: 1.3140321969985962\n",
      "Epoch: 3678: Train loss: 2.5826234817504883 Valid loss: 0.7723230719566345\n",
      "Epoch: 3679: Train loss: 2.586785078048706 Valid loss: 1.3135721683502197\n",
      "Epoch: 3680: Train loss: 2.581784248352051 Valid loss: 0.7731932997703552\n",
      "Epoch: 3681: Train loss: 2.585935354232788 Valid loss: 1.313117265701294\n",
      "Epoch: 3682: Train loss: 2.5809450149536133 Valid loss: 0.7740597724914551\n",
      "Epoch: 3683: Train loss: 2.5850841999053955 Valid loss: 1.3126628398895264\n",
      "Epoch: 3684: Train loss: 2.5801095962524414 Valid loss: 0.7749258875846863\n",
      "Epoch: 3685: Train loss: 2.584238052368164 Valid loss: 1.3122116327285767\n",
      "Epoch: 3686: Train loss: 2.579275131225586 Valid loss: 0.7757936120033264\n",
      "Epoch: 3687: Train loss: 2.583393096923828 Valid loss: 1.3117637634277344\n",
      "Epoch: 3688: Train loss: 2.578444004058838 Valid loss: 0.7766596078872681\n",
      "Epoch: 3689: Train loss: 2.582550048828125 Valid loss: 1.3113234043121338\n",
      "Epoch: 3690: Train loss: 2.5776145458221436 Valid loss: 0.7775249481201172\n",
      "Epoch: 3691: Train loss: 2.581709861755371 Valid loss: 1.310880184173584\n",
      "Epoch: 3692: Train loss: 2.5767900943756104 Valid loss: 0.7783890962600708\n",
      "Epoch: 3693: Train loss: 2.580871343612671 Valid loss: 1.3104467391967773\n",
      "Epoch: 3694: Train loss: 2.575963020324707 Valid loss: 0.7792510390281677\n",
      "Epoch: 3695: Train loss: 2.5800352096557617 Valid loss: 1.310015320777893\n",
      "Epoch: 3696: Train loss: 2.5751378536224365 Valid loss: 0.7801126837730408\n",
      "Epoch: 3697: Train loss: 2.5791990756988525 Valid loss: 1.3095831871032715\n",
      "Epoch: 3698: Train loss: 2.574317693710327 Valid loss: 0.7809745669364929\n",
      "Epoch: 3699: Train loss: 2.5783679485321045 Valid loss: 1.3091583251953125\n",
      "Epoch: 3700: Train loss: 2.573497772216797 Valid loss: 0.7818360924720764\n",
      "Epoch: 3701: Train loss: 2.577536106109619 Valid loss: 1.308737874031067\n",
      "Epoch: 3702: Train loss: 2.5726804733276367 Valid loss: 0.7826976776123047\n",
      "Epoch: 3703: Train loss: 2.5767083168029785 Valid loss: 1.3083175420761108\n",
      "Epoch: 3704: Train loss: 2.571864366531372 Valid loss: 0.7835564613342285\n",
      "Epoch: 3705: Train loss: 2.5758798122406006 Valid loss: 1.3078994750976562\n",
      "Epoch: 3706: Train loss: 2.5710489749908447 Valid loss: 0.78441321849823\n",
      "Epoch: 3707: Train loss: 2.575054883956909 Valid loss: 1.3074907064437866\n",
      "Epoch: 3708: Train loss: 2.5702388286590576 Valid loss: 0.7852687239646912\n",
      "Epoch: 3709: Train loss: 2.574233055114746 Valid loss: 1.3070785999298096\n",
      "Epoch: 3710: Train loss: 2.569430351257324 Valid loss: 0.7861271500587463\n",
      "Epoch: 3711: Train loss: 2.5734105110168457 Valid loss: 1.3066685199737549\n",
      "Epoch: 3712: Train loss: 2.5686190128326416 Valid loss: 0.7869824171066284\n",
      "Epoch: 3713: Train loss: 2.5725903511047363 Valid loss: 1.3062664270401\n",
      "Epoch: 3714: Train loss: 2.5678133964538574 Valid loss: 0.7878371477127075\n",
      "Epoch: 3715: Train loss: 2.571774482727051 Valid loss: 1.3058629035949707\n",
      "Epoch: 3716: Train loss: 2.5670104026794434 Valid loss: 0.7886908650398254\n",
      "Epoch: 3717: Train loss: 2.5709590911865234 Valid loss: 1.3054661750793457\n",
      "Epoch: 3718: Train loss: 2.5662081241607666 Valid loss: 0.7895451188087463\n",
      "Epoch: 3719: Train loss: 2.570146083831787 Valid loss: 1.3050742149353027\n",
      "Epoch: 3720: Train loss: 2.5654075145721436 Valid loss: 0.7903952598571777\n",
      "Epoch: 3721: Train loss: 2.569335699081421 Valid loss: 1.304685354232788\n",
      "Epoch: 3722: Train loss: 2.5646090507507324 Valid loss: 0.7912507653236389\n",
      "Epoch: 3723: Train loss: 2.568526268005371 Valid loss: 1.3043009042739868\n",
      "Epoch: 3724: Train loss: 2.563814640045166 Valid loss: 0.7921010255813599\n",
      "Epoch: 3725: Train loss: 2.567718982696533 Valid loss: 1.3039109706878662\n",
      "Epoch: 3726: Train loss: 2.5630204677581787 Valid loss: 0.7929534316062927\n",
      "Epoch: 3727: Train loss: 2.5669143199920654 Valid loss: 1.3035372495651245\n",
      "Epoch: 3728: Train loss: 2.562227964401245 Valid loss: 0.7937999367713928\n",
      "Epoch: 3729: Train loss: 2.566110372543335 Valid loss: 1.303161859512329\n",
      "Epoch: 3730: Train loss: 2.5614383220672607 Valid loss: 0.7946479320526123\n",
      "Epoch: 3731: Train loss: 2.5653111934661865 Valid loss: 1.3027887344360352\n",
      "Epoch: 3732: Train loss: 2.5606489181518555 Valid loss: 0.7954999208450317\n",
      "Epoch: 3733: Train loss: 2.5645127296447754 Valid loss: 1.3024256229400635\n",
      "Epoch: 3734: Train loss: 2.559863567352295 Valid loss: 0.7963475584983826\n",
      "Epoch: 3735: Train loss: 2.5637147426605225 Valid loss: 1.3020520210266113\n",
      "Epoch: 3736: Train loss: 2.559081792831421 Valid loss: 0.797192394733429\n",
      "Epoch: 3737: Train loss: 2.5629231929779053 Valid loss: 1.3016877174377441\n",
      "Epoch: 3738: Train loss: 2.5582995414733887 Valid loss: 0.7980400323867798\n",
      "Epoch: 3739: Train loss: 2.562126874923706 Valid loss: 1.301327109336853\n",
      "Epoch: 3740: Train loss: 2.557518720626831 Valid loss: 0.798882246017456\n",
      "Epoch: 3741: Train loss: 2.561338186264038 Valid loss: 1.3009748458862305\n",
      "Epoch: 3742: Train loss: 2.5567409992218018 Valid loss: 0.7997298240661621\n",
      "Epoch: 3743: Train loss: 2.5605485439300537 Valid loss: 1.300620675086975\n",
      "Epoch: 3744: Train loss: 2.55596661567688 Valid loss: 0.8005743026733398\n",
      "Epoch: 3745: Train loss: 2.5597634315490723 Valid loss: 1.3002710342407227\n",
      "Epoch: 3746: Train loss: 2.5551929473876953 Valid loss: 0.8014180660247803\n",
      "Epoch: 3747: Train loss: 2.558978319168091 Valid loss: 1.2999255657196045\n",
      "Epoch: 3748: Train loss: 2.5544207096099854 Valid loss: 0.8022602796554565\n",
      "Epoch: 3749: Train loss: 2.558196544647217 Valid loss: 1.2995851039886475\n",
      "Epoch: 3750: Train loss: 2.5536515712738037 Valid loss: 0.8031028509140015\n",
      "Epoch: 3751: Train loss: 2.557417869567871 Valid loss: 1.299249291419983\n",
      "Epoch: 3752: Train loss: 2.552884817123413 Valid loss: 0.8039449453353882\n",
      "Epoch: 3753: Train loss: 2.556640863418579 Valid loss: 1.2989156246185303\n",
      "Epoch: 3754: Train loss: 2.5521206855773926 Valid loss: 0.8047857284545898\n",
      "Epoch: 3755: Train loss: 2.555864095687866 Valid loss: 1.298576831817627\n",
      "Epoch: 3756: Train loss: 2.551358461380005 Valid loss: 0.8056270480155945\n",
      "Epoch: 3757: Train loss: 2.555093288421631 Valid loss: 1.2982606887817383\n",
      "Epoch: 3758: Train loss: 2.55059814453125 Valid loss: 0.806465744972229\n",
      "Epoch: 3759: Train loss: 2.5543222427368164 Valid loss: 1.2979366779327393\n",
      "Epoch: 3760: Train loss: 2.5498411655426025 Valid loss: 0.8073052763938904\n",
      "Epoch: 3761: Train loss: 2.553555488586426 Valid loss: 1.297615647315979\n",
      "Epoch: 3762: Train loss: 2.549086570739746 Valid loss: 0.8081459403038025\n",
      "Epoch: 3763: Train loss: 2.5527889728546143 Valid loss: 1.297299861907959\n",
      "Epoch: 3764: Train loss: 2.5483317375183105 Valid loss: 0.8089829087257385\n",
      "Epoch: 3765: Train loss: 2.552023410797119 Valid loss: 1.2969887256622314\n",
      "Epoch: 3766: Train loss: 2.5475761890411377 Valid loss: 0.8098184466362\n",
      "Epoch: 3767: Train loss: 2.5512592792510986 Valid loss: 1.2966735363006592\n",
      "Epoch: 3768: Train loss: 2.5468273162841797 Valid loss: 0.8106546401977539\n",
      "Epoch: 3769: Train loss: 2.5504977703094482 Valid loss: 1.2963664531707764\n",
      "Epoch: 3770: Train loss: 2.5460777282714844 Valid loss: 0.8114875555038452\n",
      "Epoch: 3771: Train loss: 2.5497400760650635 Valid loss: 1.296065092086792\n",
      "Epoch: 3772: Train loss: 2.54533052444458 Valid loss: 0.812323808670044\n",
      "Epoch: 3773: Train loss: 2.548978328704834 Valid loss: 1.2957563400268555\n",
      "Epoch: 3774: Train loss: 2.5445823669433594 Valid loss: 0.8131539225578308\n",
      "Epoch: 3775: Train loss: 2.5482208728790283 Valid loss: 1.2954587936401367\n",
      "Epoch: 3776: Train loss: 2.543837785720825 Valid loss: 0.8139874935150146\n",
      "Epoch: 3777: Train loss: 2.54746413230896 Valid loss: 1.2951548099517822\n",
      "Epoch: 3778: Train loss: 2.5430939197540283 Valid loss: 0.8148174285888672\n",
      "Epoch: 3779: Train loss: 2.5467100143432617 Valid loss: 1.2948625087738037\n",
      "Epoch: 3780: Train loss: 2.5423526763916016 Valid loss: 0.8156458139419556\n",
      "Epoch: 3781: Train loss: 2.5459585189819336 Valid loss: 1.294572353363037\n",
      "Epoch: 3782: Train loss: 2.5416128635406494 Valid loss: 0.8164753317832947\n",
      "Epoch: 3783: Train loss: 2.545207977294922 Valid loss: 1.2942774295806885\n",
      "Epoch: 3784: Train loss: 2.5408735275268555 Valid loss: 0.8173034191131592\n",
      "Epoch: 3785: Train loss: 2.5444583892822266 Valid loss: 1.293992280960083\n",
      "Epoch: 3786: Train loss: 2.5401387214660645 Valid loss: 0.8181307911872864\n",
      "Epoch: 3787: Train loss: 2.543715238571167 Valid loss: 1.293715000152588\n",
      "Epoch: 3788: Train loss: 2.5394067764282227 Valid loss: 0.8189602494239807\n",
      "Epoch: 3789: Train loss: 2.5429704189300537 Valid loss: 1.2934367656707764\n",
      "Epoch: 3790: Train loss: 2.538674831390381 Valid loss: 0.8197858333587646\n",
      "Epoch: 3791: Train loss: 2.542229652404785 Valid loss: 1.29315984249115\n",
      "Epoch: 3792: Train loss: 2.537946939468384 Valid loss: 0.8206158876419067\n",
      "Epoch: 3793: Train loss: 2.5414938926696777 Valid loss: 1.2928932905197144\n",
      "Epoch: 3794: Train loss: 2.537222385406494 Valid loss: 0.8214433789253235\n",
      "Epoch: 3795: Train loss: 2.540757179260254 Valid loss: 1.2926230430603027\n",
      "Epoch: 3796: Train loss: 2.5364952087402344 Valid loss: 0.8222664594650269\n",
      "Epoch: 3797: Train loss: 2.5400190353393555 Valid loss: 1.292357087135315\n",
      "Epoch: 3798: Train loss: 2.5357673168182373 Valid loss: 0.8230910897254944\n",
      "Epoch: 3799: Train loss: 2.539283275604248 Valid loss: 1.2920875549316406\n",
      "Epoch: 3800: Train loss: 2.5350465774536133 Valid loss: 0.8239094018936157\n",
      "Epoch: 3801: Train loss: 2.53855037689209 Valid loss: 1.2918286323547363\n",
      "Epoch: 3802: Train loss: 2.5343267917633057 Valid loss: 0.8247354030609131\n",
      "Epoch: 3803: Train loss: 2.5378193855285645 Valid loss: 1.2915726900100708\n",
      "Epoch: 3804: Train loss: 2.533608913421631 Valid loss: 0.8255569934844971\n",
      "Epoch: 3805: Train loss: 2.537093162536621 Valid loss: 1.291319489479065\n",
      "Epoch: 3806: Train loss: 2.532895803451538 Valid loss: 0.8263799548149109\n",
      "Epoch: 3807: Train loss: 2.5363683700561523 Valid loss: 1.2910709381103516\n",
      "Epoch: 3808: Train loss: 2.5321812629699707 Valid loss: 0.8272010684013367\n",
      "Epoch: 3809: Train loss: 2.535645008087158 Valid loss: 1.2908241748809814\n",
      "Epoch: 3810: Train loss: 2.5314674377441406 Valid loss: 0.8280203342437744\n",
      "Epoch: 3811: Train loss: 2.5349223613739014 Valid loss: 1.2905796766281128\n",
      "Epoch: 3812: Train loss: 2.5307600498199463 Valid loss: 0.8288386464118958\n",
      "Epoch: 3813: Train loss: 2.534203290939331 Valid loss: 1.29033625125885\n",
      "Epoch: 3814: Train loss: 2.530050277709961 Valid loss: 0.8296564817428589\n",
      "Epoch: 3815: Train loss: 2.533482074737549 Valid loss: 1.2900993824005127\n",
      "Epoch: 3816: Train loss: 2.529343366622925 Valid loss: 0.8304769992828369\n",
      "Epoch: 3817: Train loss: 2.5327646732330322 Valid loss: 1.2898666858673096\n",
      "Epoch: 3818: Train loss: 2.528637647628784 Valid loss: 0.8312902450561523\n",
      "Epoch: 3819: Train loss: 2.532048463821411 Valid loss: 1.2896292209625244\n",
      "Epoch: 3820: Train loss: 2.527933120727539 Valid loss: 0.8321093916893005\n",
      "Epoch: 3821: Train loss: 2.53133487701416 Valid loss: 1.2894026041030884\n",
      "Epoch: 3822: Train loss: 2.5272300243377686 Valid loss: 0.8329196572303772\n",
      "Epoch: 3823: Train loss: 2.53062105178833 Valid loss: 1.289171576499939\n",
      "Epoch: 3824: Train loss: 2.526527166366577 Valid loss: 0.8337329626083374\n",
      "Epoch: 3825: Train loss: 2.5299088954925537 Valid loss: 1.2889442443847656\n",
      "Epoch: 3826: Train loss: 2.5258278846740723 Valid loss: 0.8345460891723633\n",
      "Epoch: 3827: Train loss: 2.5291998386383057 Valid loss: 1.288724422454834\n",
      "Epoch: 3828: Train loss: 2.525131940841675 Valid loss: 0.8353583216667175\n",
      "Epoch: 3829: Train loss: 2.5284910202026367 Valid loss: 1.2884998321533203\n",
      "Epoch: 3830: Train loss: 2.5244321823120117 Valid loss: 0.8361673355102539\n",
      "Epoch: 3831: Train loss: 2.527782440185547 Valid loss: 1.288283109664917\n",
      "Epoch: 3832: Train loss: 2.5237374305725098 Valid loss: 0.8369789123535156\n",
      "Epoch: 3833: Train loss: 2.5270800590515137 Valid loss: 1.288069248199463\n",
      "Epoch: 3834: Train loss: 2.5230467319488525 Valid loss: 0.8377920985221863\n",
      "Epoch: 3835: Train loss: 2.526376485824585 Valid loss: 1.2878539562225342\n",
      "Epoch: 3836: Train loss: 2.5223538875579834 Valid loss: 0.8385990858078003\n",
      "Epoch: 3837: Train loss: 2.525674819946289 Valid loss: 1.287642478942871\n",
      "Epoch: 3838: Train loss: 2.5216639041900635 Valid loss: 0.839407742023468\n",
      "Epoch: 3839: Train loss: 2.5249745845794678 Valid loss: 1.2874362468719482\n",
      "Epoch: 3840: Train loss: 2.5209782123565674 Valid loss: 0.840214192867279\n",
      "Epoch: 3841: Train loss: 2.5242791175842285 Valid loss: 1.2872388362884521\n",
      "Epoch: 3842: Train loss: 2.520293951034546 Valid loss: 0.8410234451293945\n",
      "Epoch: 3843: Train loss: 2.5235862731933594 Valid loss: 1.287034034729004\n",
      "Epoch: 3844: Train loss: 2.519613027572632 Valid loss: 0.8418319225311279\n",
      "Epoch: 3845: Train loss: 2.5228941440582275 Valid loss: 1.286841869354248\n",
      "Epoch: 3846: Train loss: 2.518932819366455 Valid loss: 0.8426395654678345\n",
      "Epoch: 3847: Train loss: 2.5222041606903076 Valid loss: 1.2866497039794922\n",
      "Epoch: 3848: Train loss: 2.5182535648345947 Valid loss: 0.8434423804283142\n",
      "Epoch: 3849: Train loss: 2.521514892578125 Valid loss: 1.286458969116211\n",
      "Epoch: 3850: Train loss: 2.517573833465576 Valid loss: 0.84424889087677\n",
      "Epoch: 3851: Train loss: 2.5208258628845215 Valid loss: 1.2862714529037476\n",
      "Epoch: 3852: Train loss: 2.5168981552124023 Valid loss: 0.8450527191162109\n",
      "Epoch: 3853: Train loss: 2.520138740539551 Valid loss: 1.286085605621338\n",
      "Epoch: 3854: Train loss: 2.516223192214966 Valid loss: 0.8458552360534668\n",
      "Epoch: 3855: Train loss: 2.5194571018218994 Valid loss: 1.2859032154083252\n",
      "Epoch: 3856: Train loss: 2.515549659729004 Valid loss: 0.8466570377349854\n",
      "Epoch: 3857: Train loss: 2.5187723636627197 Valid loss: 1.2857246398925781\n",
      "Epoch: 3858: Train loss: 2.514878034591675 Valid loss: 0.8474574089050293\n",
      "Epoch: 3859: Train loss: 2.51809024810791 Valid loss: 1.28554368019104\n",
      "Epoch: 3860: Train loss: 2.5142064094543457 Valid loss: 0.8482577204704285\n",
      "Epoch: 3861: Train loss: 2.5174076557159424 Valid loss: 1.285368800163269\n",
      "Epoch: 3862: Train loss: 2.513535499572754 Valid loss: 0.8490559458732605\n",
      "Epoch: 3863: Train loss: 2.5167288780212402 Valid loss: 1.2851965427398682\n",
      "Epoch: 3864: Train loss: 2.512866973876953 Valid loss: 0.8498526215553284\n",
      "Epoch: 3865: Train loss: 2.516050338745117 Valid loss: 1.2850207090377808\n",
      "Epoch: 3866: Train loss: 2.512199640274048 Valid loss: 0.8506511449813843\n",
      "Epoch: 3867: Train loss: 2.515373945236206 Valid loss: 1.2848520278930664\n",
      "Epoch: 3868: Train loss: 2.5115346908569336 Valid loss: 0.8514490723609924\n",
      "Epoch: 3869: Train loss: 2.514697551727295 Valid loss: 1.2846789360046387\n",
      "Epoch: 3870: Train loss: 2.5108704566955566 Valid loss: 0.8522448539733887\n",
      "Epoch: 3871: Train loss: 2.514024496078491 Valid loss: 1.2845141887664795\n",
      "Epoch: 3872: Train loss: 2.51020884513855 Valid loss: 0.8530364036560059\n",
      "Epoch: 3873: Train loss: 2.5133535861968994 Valid loss: 1.2843538522720337\n",
      "Epoch: 3874: Train loss: 2.5095536708831787 Valid loss: 0.8538328409194946\n",
      "Epoch: 3875: Train loss: 2.5126869678497314 Valid loss: 1.28419828414917\n",
      "Epoch: 3876: Train loss: 2.508894681930542 Valid loss: 0.8546319007873535\n",
      "Epoch: 3877: Train loss: 2.5120203495025635 Valid loss: 1.2840425968170166\n",
      "Epoch: 3878: Train loss: 2.5082404613494873 Valid loss: 0.8554251194000244\n",
      "Epoch: 3879: Train loss: 2.5113563537597656 Valid loss: 1.2838921546936035\n",
      "Epoch: 3880: Train loss: 2.507586717605591 Valid loss: 0.8562179803848267\n",
      "Epoch: 3881: Train loss: 2.510692834854126 Valid loss: 1.2837402820587158\n",
      "Epoch: 3882: Train loss: 2.5069360733032227 Valid loss: 0.8570120334625244\n",
      "Epoch: 3883: Train loss: 2.5100319385528564 Valid loss: 1.283598780632019\n",
      "Epoch: 3884: Train loss: 2.506288766860962 Valid loss: 0.857804536819458\n",
      "Epoch: 3885: Train loss: 2.5093750953674316 Valid loss: 1.2834560871124268\n",
      "Epoch: 3886: Train loss: 2.505638360977173 Valid loss: 0.8585953116416931\n",
      "Epoch: 3887: Train loss: 2.5087170600891113 Valid loss: 1.2833192348480225\n",
      "Epoch: 3888: Train loss: 2.5049924850463867 Valid loss: 0.8593882322311401\n",
      "Epoch: 3889: Train loss: 2.5080597400665283 Valid loss: 1.28318452835083\n",
      "Epoch: 3890: Train loss: 2.504348039627075 Valid loss: 0.8601758480072021\n",
      "Epoch: 3891: Train loss: 2.5074071884155273 Valid loss: 1.2830512523651123\n",
      "Epoch: 3892: Train loss: 2.503702402114868 Valid loss: 0.8609669804573059\n",
      "Epoch: 3893: Train loss: 2.5067520141601562 Valid loss: 1.2829174995422363\n",
      "Epoch: 3894: Train loss: 2.503061056137085 Valid loss: 0.8617538809776306\n",
      "Epoch: 3895: Train loss: 2.50610089302063 Valid loss: 1.2827868461608887\n",
      "Epoch: 3896: Train loss: 2.5024235248565674 Valid loss: 0.8625431656837463\n",
      "Epoch: 3897: Train loss: 2.505453586578369 Valid loss: 1.2826615571975708\n",
      "Epoch: 3898: Train loss: 2.5017855167388916 Valid loss: 0.8633309006690979\n",
      "Epoch: 3899: Train loss: 2.50480318069458 Valid loss: 1.2825367450714111\n",
      "Epoch: 3900: Train loss: 2.5011448860168457 Valid loss: 0.8641149997711182\n",
      "Epoch: 3901: Train loss: 2.5041563510894775 Valid loss: 1.28240966796875\n",
      "Epoch: 3902: Train loss: 2.5005111694335938 Valid loss: 0.8649023175239563\n",
      "Epoch: 3903: Train loss: 2.5035133361816406 Valid loss: 1.282292127609253\n",
      "Epoch: 3904: Train loss: 2.499877452850342 Valid loss: 0.8656876683235168\n",
      "Epoch: 3905: Train loss: 2.502870559692383 Valid loss: 1.2821784019470215\n",
      "Epoch: 3906: Train loss: 2.4992456436157227 Valid loss: 0.8664708137512207\n",
      "Epoch: 3907: Train loss: 2.5022292137145996 Valid loss: 1.2820640802383423\n",
      "Epoch: 3908: Train loss: 2.4986143112182617 Valid loss: 0.8672529458999634\n",
      "Epoch: 3909: Train loss: 2.501589775085449 Valid loss: 1.2819533348083496\n",
      "Epoch: 3910: Train loss: 2.4979851245880127 Valid loss: 0.8680368065834045\n",
      "Epoch: 3911: Train loss: 2.500950813293457 Valid loss: 1.2818481922149658\n",
      "Epoch: 3912: Train loss: 2.497358560562134 Valid loss: 0.8688187599182129\n",
      "Epoch: 3913: Train loss: 2.5003151893615723 Valid loss: 1.2817425727844238\n",
      "Epoch: 3914: Train loss: 2.496731758117676 Valid loss: 0.8696019649505615\n",
      "Epoch: 3915: Train loss: 2.499678134918213 Valid loss: 1.2816367149353027\n",
      "Epoch: 3916: Train loss: 2.496107816696167 Valid loss: 0.8703804016113281\n",
      "Epoch: 3917: Train loss: 2.4990434646606445 Valid loss: 1.2815332412719727\n",
      "Epoch: 3918: Train loss: 2.4954822063446045 Valid loss: 0.8711604475975037\n",
      "Epoch: 3919: Train loss: 2.4984092712402344 Valid loss: 1.2814334630966187\n",
      "Epoch: 3920: Train loss: 2.4948623180389404 Valid loss: 0.8719401955604553\n",
      "Epoch: 3921: Train loss: 2.4977827072143555 Valid loss: 1.2813348770141602\n",
      "Epoch: 3922: Train loss: 2.4942448139190674 Valid loss: 0.8727197051048279\n",
      "Epoch: 3923: Train loss: 2.497157096862793 Valid loss: 1.281252145767212\n",
      "Epoch: 3924: Train loss: 2.4936304092407227 Valid loss: 0.8735001087188721\n",
      "Epoch: 3925: Train loss: 2.496530771255493 Valid loss: 1.281158685684204\n",
      "Epoch: 3926: Train loss: 2.4930148124694824 Valid loss: 0.8742783069610596\n",
      "Epoch: 3927: Train loss: 2.495907783508301 Valid loss: 1.281074047088623\n",
      "Epoch: 3928: Train loss: 2.492401361465454 Valid loss: 0.8750560879707336\n",
      "Epoch: 3929: Train loss: 2.495285987854004 Valid loss: 1.2809948921203613\n",
      "Epoch: 3930: Train loss: 2.4917898178100586 Valid loss: 0.8758319616317749\n",
      "Epoch: 3931: Train loss: 2.4946632385253906 Valid loss: 1.2809123992919922\n",
      "Epoch: 3932: Train loss: 2.4911813735961914 Valid loss: 0.8766058683395386\n",
      "Epoch: 3933: Train loss: 2.4940452575683594 Valid loss: 1.2808377742767334\n",
      "Epoch: 3934: Train loss: 2.4905717372894287 Valid loss: 0.8773834705352783\n",
      "Epoch: 3935: Train loss: 2.493424892425537 Valid loss: 1.2807562351226807\n",
      "Epoch: 3936: Train loss: 2.489962577819824 Valid loss: 0.8781549334526062\n",
      "Epoch: 3937: Train loss: 2.4928064346313477 Valid loss: 1.28068208694458\n",
      "Epoch: 3938: Train loss: 2.489352226257324 Valid loss: 0.8789280652999878\n",
      "Epoch: 3939: Train loss: 2.4921889305114746 Valid loss: 1.2806000709533691\n",
      "Epoch: 3940: Train loss: 2.488746166229248 Valid loss: 0.8797000050544739\n",
      "Epoch: 3941: Train loss: 2.4915754795074463 Valid loss: 1.280531644821167\n",
      "Epoch: 3942: Train loss: 2.488144636154175 Valid loss: 0.8804709315299988\n",
      "Epoch: 3943: Train loss: 2.4909603595733643 Valid loss: 1.2804620265960693\n",
      "Epoch: 3944: Train loss: 2.4875402450561523 Valid loss: 0.8812398910522461\n",
      "Epoch: 3945: Train loss: 2.490349292755127 Valid loss: 1.2803988456726074\n",
      "Epoch: 3946: Train loss: 2.4869375228881836 Valid loss: 0.8820121884346008\n",
      "Epoch: 3947: Train loss: 2.489736795425415 Valid loss: 1.280329942703247\n",
      "Epoch: 3948: Train loss: 2.4863367080688477 Valid loss: 0.8827793598175049\n",
      "Epoch: 3949: Train loss: 2.489126682281494 Valid loss: 1.2802679538726807\n",
      "Epoch: 3950: Train loss: 2.4857370853424072 Valid loss: 0.8835451602935791\n",
      "Epoch: 3951: Train loss: 2.4885189533233643 Valid loss: 1.2802081108093262\n",
      "Epoch: 3952: Train loss: 2.4851412773132324 Valid loss: 0.884314775466919\n",
      "Epoch: 3953: Train loss: 2.4879136085510254 Valid loss: 1.280146837234497\n",
      "Epoch: 3954: Train loss: 2.4845447540283203 Valid loss: 0.8850831389427185\n",
      "Epoch: 3955: Train loss: 2.4873099327087402 Valid loss: 1.2800977230072021\n",
      "Epoch: 3956: Train loss: 2.48395037651062 Valid loss: 0.8858479261398315\n",
      "Epoch: 3957: Train loss: 2.486705780029297 Valid loss: 1.2800405025482178\n",
      "Epoch: 3958: Train loss: 2.48335862159729 Valid loss: 0.8866138458251953\n",
      "Epoch: 3959: Train loss: 2.4861059188842773 Valid loss: 1.2799909114837646\n",
      "Epoch: 3960: Train loss: 2.482766628265381 Valid loss: 0.8873814344406128\n",
      "Epoch: 3961: Train loss: 2.485504150390625 Valid loss: 1.2799456119537354\n",
      "Epoch: 3962: Train loss: 2.4821767807006836 Valid loss: 0.8881439566612244\n",
      "Epoch: 3963: Train loss: 2.4849045276641846 Valid loss: 1.279900312423706\n",
      "Epoch: 3964: Train loss: 2.4815895557403564 Valid loss: 0.8889071941375732\n",
      "Epoch: 3965: Train loss: 2.484308958053589 Valid loss: 1.2798529863357544\n",
      "Epoch: 3966: Train loss: 2.4810006618499756 Valid loss: 0.8896673917770386\n",
      "Epoch: 3967: Train loss: 2.4837114810943604 Valid loss: 1.2798120975494385\n",
      "Epoch: 3968: Train loss: 2.4804129600524902 Valid loss: 0.8904308676719666\n",
      "Epoch: 3969: Train loss: 2.483116388320923 Valid loss: 1.2797715663909912\n",
      "Epoch: 3970: Train loss: 2.4798295497894287 Valid loss: 0.8911914229393005\n",
      "Epoch: 3971: Train loss: 2.482522964477539 Valid loss: 1.2797341346740723\n",
      "Epoch: 3972: Train loss: 2.4792468547821045 Valid loss: 0.8919521570205688\n",
      "Epoch: 3973: Train loss: 2.481928825378418 Valid loss: 1.2796977758407593\n",
      "Epoch: 3974: Train loss: 2.478663921356201 Valid loss: 0.892711341381073\n",
      "Epoch: 3975: Train loss: 2.4813406467437744 Valid loss: 1.2796612977981567\n",
      "Epoch: 3976: Train loss: 2.478084087371826 Valid loss: 0.8934710025787354\n",
      "Epoch: 3977: Train loss: 2.48075008392334 Valid loss: 1.279632568359375\n",
      "Epoch: 3978: Train loss: 2.477503776550293 Valid loss: 0.894224226474762\n",
      "Epoch: 3979: Train loss: 2.48016095161438 Valid loss: 1.2796015739440918\n",
      "Epoch: 3980: Train loss: 2.476926326751709 Valid loss: 0.8949853777885437\n",
      "Epoch: 3981: Train loss: 2.479576349258423 Valid loss: 1.2795798778533936\n",
      "Epoch: 3982: Train loss: 2.4763519763946533 Valid loss: 0.8957420587539673\n",
      "Epoch: 3983: Train loss: 2.4789934158325195 Valid loss: 1.2795567512512207\n",
      "Epoch: 3984: Train loss: 2.4757778644561768 Valid loss: 0.8964976072311401\n",
      "Epoch: 3985: Train loss: 2.478409767150879 Valid loss: 1.2795318365097046\n",
      "Epoch: 3986: Train loss: 2.475205659866333 Valid loss: 0.897254467010498\n",
      "Epoch: 3987: Train loss: 2.4778292179107666 Valid loss: 1.2795143127441406\n",
      "Epoch: 3988: Train loss: 2.4746346473693848 Valid loss: 0.8980096578598022\n",
      "Epoch: 3989: Train loss: 2.477248430252075 Valid loss: 1.2794914245605469\n",
      "Epoch: 3990: Train loss: 2.4740636348724365 Valid loss: 0.8987665772438049\n",
      "Epoch: 3991: Train loss: 2.476667881011963 Valid loss: 1.2794771194458008\n",
      "Epoch: 3992: Train loss: 2.473493814468384 Valid loss: 0.8995171189308167\n",
      "Epoch: 3993: Train loss: 2.4760916233062744 Valid loss: 1.279464840888977\n",
      "Epoch: 3994: Train loss: 2.472928524017334 Valid loss: 0.9002687931060791\n",
      "Epoch: 3995: Train loss: 2.4755163192749023 Valid loss: 1.2794557809829712\n",
      "Epoch: 3996: Train loss: 2.4723622798919678 Valid loss: 0.9010242223739624\n",
      "Epoch: 3997: Train loss: 2.4749441146850586 Valid loss: 1.2794525623321533\n",
      "Epoch: 3998: Train loss: 2.4718000888824463 Valid loss: 0.9017758369445801\n",
      "Epoch: 3999: Train loss: 2.474371910095215 Valid loss: 1.279451608657837\n",
      "Epoch: 4000: Train loss: 2.471238136291504 Valid loss: 0.9025272130966187\n",
      "Epoch: 4001: Train loss: 2.473802328109741 Valid loss: 1.2794456481933594\n",
      "Epoch: 4002: Train loss: 2.470677375793457 Valid loss: 0.9032782912254333\n",
      "Epoch: 4003: Train loss: 2.473233461380005 Valid loss: 1.2794467210769653\n",
      "Epoch: 4004: Train loss: 2.4701154232025146 Valid loss: 0.9040294289588928\n",
      "Epoch: 4005: Train loss: 2.472662925720215 Valid loss: 1.2794485092163086\n",
      "Epoch: 4006: Train loss: 2.4695589542388916 Valid loss: 0.9047773480415344\n",
      "Epoch: 4007: Train loss: 2.4720957279205322 Valid loss: 1.2794482707977295\n",
      "Epoch: 4008: Train loss: 2.468999147415161 Valid loss: 0.905523955821991\n",
      "Epoch: 4009: Train loss: 2.471529722213745 Valid loss: 1.2794523239135742\n",
      "Epoch: 4010: Train loss: 2.4684436321258545 Valid loss: 0.9062715172767639\n",
      "Epoch: 4011: Train loss: 2.4709653854370117 Valid loss: 1.2794594764709473\n",
      "Epoch: 4012: Train loss: 2.467890739440918 Valid loss: 0.9070172905921936\n",
      "Epoch: 4013: Train loss: 2.470404624938965 Valid loss: 1.2794718742370605\n",
      "Epoch: 4014: Train loss: 2.4673376083374023 Valid loss: 0.9077641367912292\n",
      "Epoch: 4015: Train loss: 2.469839572906494 Valid loss: 1.2794835567474365\n",
      "Epoch: 4016: Train loss: 2.4667842388153076 Valid loss: 0.9085085988044739\n",
      "Epoch: 4017: Train loss: 2.4692797660827637 Valid loss: 1.2794945240020752\n",
      "Epoch: 4018: Train loss: 2.466233015060425 Valid loss: 0.909252405166626\n",
      "Epoch: 4019: Train loss: 2.4687182903289795 Valid loss: 1.279505729675293\n",
      "Epoch: 4020: Train loss: 2.4656832218170166 Valid loss: 0.9099984169006348\n",
      "Epoch: 4021: Train loss: 2.468163251876831 Valid loss: 1.279529333114624\n",
      "Epoch: 4022: Train loss: 2.465134859085083 Valid loss: 0.9107407927513123\n",
      "Epoch: 4023: Train loss: 2.467606782913208 Valid loss: 1.2795474529266357\n",
      "Epoch: 4024: Train loss: 2.4645895957946777 Valid loss: 0.9114833474159241\n",
      "Epoch: 4025: Train loss: 2.4670510292053223 Valid loss: 1.279564619064331\n",
      "Epoch: 4026: Train loss: 2.464043140411377 Valid loss: 0.9122228026390076\n",
      "Epoch: 4027: Train loss: 2.466498374938965 Valid loss: 1.2795908451080322\n",
      "Epoch: 4028: Train loss: 2.463501453399658 Valid loss: 0.9129683375358582\n",
      "Epoch: 4029: Train loss: 2.465946912765503 Valid loss: 1.27962064743042\n",
      "Epoch: 4030: Train loss: 2.4629595279693604 Valid loss: 0.9137085676193237\n",
      "Epoch: 4031: Train loss: 2.4653983116149902 Valid loss: 1.2796517610549927\n",
      "Epoch: 4032: Train loss: 2.4624204635620117 Valid loss: 0.9144474267959595\n",
      "Epoch: 4033: Train loss: 2.464848518371582 Valid loss: 1.2796852588653564\n",
      "Epoch: 4034: Train loss: 2.4618818759918213 Valid loss: 0.915187418460846\n",
      "Epoch: 4035: Train loss: 2.4643008708953857 Valid loss: 1.2797160148620605\n",
      "Epoch: 4036: Train loss: 2.461343288421631 Valid loss: 0.9159257411956787\n",
      "Epoch: 4037: Train loss: 2.4637556076049805 Valid loss: 1.2797516584396362\n",
      "Epoch: 4038: Train loss: 2.460806369781494 Valid loss: 0.9166673421859741\n",
      "Epoch: 4039: Train loss: 2.463212013244629 Valid loss: 1.2797900438308716\n",
      "Epoch: 4040: Train loss: 2.4602701663970947 Valid loss: 0.9174038767814636\n",
      "Epoch: 4041: Train loss: 2.4626684188842773 Valid loss: 1.2798314094543457\n",
      "Epoch: 4042: Train loss: 2.4597387313842773 Valid loss: 0.918140172958374\n",
      "Epoch: 4043: Train loss: 2.4621264934539795 Valid loss: 1.2798787355422974\n",
      "Epoch: 4044: Train loss: 2.4592061042785645 Valid loss: 0.9188804626464844\n",
      "Epoch: 4045: Train loss: 2.4615864753723145 Valid loss: 1.2799203395843506\n",
      "Epoch: 4046: Train loss: 2.458674907684326 Valid loss: 0.9196146130561829\n",
      "Epoch: 4047: Train loss: 2.461045980453491 Valid loss: 1.2799665927886963\n",
      "Epoch: 4048: Train loss: 2.4581446647644043 Valid loss: 0.9203490614891052\n",
      "Epoch: 4049: Train loss: 2.4605071544647217 Valid loss: 1.2800170183181763\n",
      "Epoch: 4050: Train loss: 2.457615375518799 Valid loss: 0.9210842847824097\n",
      "Epoch: 4051: Train loss: 2.4599690437316895 Valid loss: 1.2800642251968384\n",
      "Epoch: 4052: Train loss: 2.457084894180298 Valid loss: 0.9218173623085022\n",
      "Epoch: 4053: Train loss: 2.4594335556030273 Valid loss: 1.2801109552383423\n",
      "Epoch: 4054: Train loss: 2.456561326980591 Valid loss: 0.9225510954856873\n",
      "Epoch: 4055: Train loss: 2.458897590637207 Valid loss: 1.2801692485809326\n",
      "Epoch: 4056: Train loss: 2.4560346603393555 Valid loss: 0.9232812523841858\n",
      "Epoch: 4057: Train loss: 2.4583675861358643 Valid loss: 1.2802248001098633\n",
      "Epoch: 4058: Train loss: 2.455512762069702 Valid loss: 0.9240158200263977\n",
      "Epoch: 4059: Train loss: 2.457836866378784 Valid loss: 1.2802865505218506\n",
      "Epoch: 4060: Train loss: 2.4549882411956787 Valid loss: 0.9247474670410156\n",
      "Epoch: 4061: Train loss: 2.4573051929473877 Valid loss: 1.2803425788879395\n",
      "Epoch: 4062: Train loss: 2.4544692039489746 Valid loss: 0.9254759550094604\n",
      "Epoch: 4063: Train loss: 2.4567766189575195 Valid loss: 1.2804080247879028\n",
      "Epoch: 4064: Train loss: 2.453949213027954 Valid loss: 0.9262044429779053\n",
      "Epoch: 4065: Train loss: 2.4562504291534424 Valid loss: 1.2804741859436035\n",
      "Epoch: 4066: Train loss: 2.4534313678741455 Valid loss: 0.9269344806671143\n",
      "Epoch: 4067: Train loss: 2.4557230472564697 Valid loss: 1.2805383205413818\n",
      "Epoch: 4068: Train loss: 2.4529106616973877 Valid loss: 0.92766273021698\n",
      "Epoch: 4069: Train loss: 2.455195426940918 Valid loss: 1.2806057929992676\n",
      "Epoch: 4070: Train loss: 2.452394723892212 Valid loss: 0.9283905029296875\n",
      "Epoch: 4071: Train loss: 2.4546706676483154 Valid loss: 1.2806698083877563\n",
      "Epoch: 4072: Train loss: 2.4518790245056152 Valid loss: 0.9291148781776428\n",
      "Epoch: 4073: Train loss: 2.454146385192871 Valid loss: 1.2807416915893555\n",
      "Epoch: 4074: Train loss: 2.451364755630493 Valid loss: 0.9298426508903503\n",
      "Epoch: 4075: Train loss: 2.4536244869232178 Valid loss: 1.280815839767456\n",
      "Epoch: 4076: Train loss: 2.450852394104004 Valid loss: 0.9305652976036072\n",
      "Epoch: 4077: Train loss: 2.4531049728393555 Valid loss: 1.2808868885040283\n",
      "Epoch: 4078: Train loss: 2.450340747833252 Valid loss: 0.9312915205955505\n",
      "Epoch: 4079: Train loss: 2.452585220336914 Valid loss: 1.2809648513793945\n",
      "Epoch: 4080: Train loss: 2.449831962585449 Valid loss: 0.9320172071456909\n",
      "Epoch: 4081: Train loss: 2.452068328857422 Valid loss: 1.2810463905334473\n",
      "Epoch: 4082: Train loss: 2.449324607849121 Valid loss: 0.9327411651611328\n",
      "Epoch: 4083: Train loss: 2.4515511989593506 Valid loss: 1.281128168106079\n",
      "Epoch: 4084: Train loss: 2.448817491531372 Valid loss: 0.9334644079208374\n",
      "Epoch: 4085: Train loss: 2.4510366916656494 Valid loss: 1.2812117338180542\n",
      "Epoch: 4086: Train loss: 2.4483110904693604 Valid loss: 0.9341861009597778\n",
      "Epoch: 4087: Train loss: 2.4505228996276855 Valid loss: 1.2812966108322144\n",
      "Epoch: 4088: Train loss: 2.4478070735931396 Valid loss: 0.9349073171615601\n",
      "Epoch: 4089: Train loss: 2.4500114917755127 Valid loss: 1.281383752822876\n",
      "Epoch: 4090: Train loss: 2.447303533554077 Valid loss: 0.9356324076652527\n",
      "Epoch: 4091: Train loss: 2.44950270652771 Valid loss: 1.2814756631851196\n",
      "Epoch: 4092: Train loss: 2.4468016624450684 Valid loss: 0.9363517165184021\n",
      "Epoch: 4093: Train loss: 2.448991298675537 Valid loss: 1.2815673351287842\n",
      "Epoch: 4094: Train loss: 2.446302890777588 Valid loss: 0.9370712041854858\n",
      "Epoch: 4095: Train loss: 2.44848370552063 Valid loss: 1.2816599607467651\n",
      "Epoch: 4096: Train loss: 2.445802688598633 Valid loss: 0.9377937316894531\n",
      "Epoch: 4097: Train loss: 2.4479761123657227 Valid loss: 1.2817575931549072\n",
      "Epoch: 4098: Train loss: 2.4453020095825195 Valid loss: 0.9385116100311279\n",
      "Epoch: 4099: Train loss: 2.4474687576293945 Valid loss: 1.2818514108657837\n",
      "Epoch: 4100: Train loss: 2.4448037147521973 Valid loss: 0.9392271041870117\n",
      "Epoch: 4101: Train loss: 2.446962594985962 Valid loss: 1.2819472551345825\n",
      "Epoch: 4102: Train loss: 2.4443063735961914 Valid loss: 0.939944863319397\n",
      "Epoch: 4103: Train loss: 2.446457862854004 Valid loss: 1.282043218612671\n",
      "Epoch: 4104: Train loss: 2.4438118934631348 Valid loss: 0.9406591057777405\n",
      "Epoch: 4105: Train loss: 2.445953607559204 Valid loss: 1.2821452617645264\n",
      "Epoch: 4106: Train loss: 2.443317413330078 Valid loss: 0.9413772225379944\n",
      "Epoch: 4107: Train loss: 2.4454548358917236 Valid loss: 1.2822530269622803\n",
      "Epoch: 4108: Train loss: 2.4428255558013916 Valid loss: 0.9420906901359558\n",
      "Epoch: 4109: Train loss: 2.4449541568756104 Valid loss: 1.2823539972305298\n",
      "Epoch: 4110: Train loss: 2.4423344135284424 Valid loss: 0.9428075551986694\n",
      "Epoch: 4111: Train loss: 2.4444541931152344 Valid loss: 1.282466173171997\n",
      "Epoch: 4112: Train loss: 2.4418444633483887 Valid loss: 0.9435235857963562\n",
      "Epoch: 4113: Train loss: 2.4439568519592285 Valid loss: 1.2825775146484375\n",
      "Epoch: 4114: Train loss: 2.4413561820983887 Valid loss: 0.9442369341850281\n",
      "Epoch: 4115: Train loss: 2.4434595108032227 Valid loss: 1.282690405845642\n",
      "Epoch: 4116: Train loss: 2.4408671855926514 Valid loss: 0.9449478387832642\n",
      "Epoch: 4117: Train loss: 2.442965269088745 Valid loss: 1.2827987670898438\n",
      "Epoch: 4118: Train loss: 2.4403791427612305 Valid loss: 0.9456620216369629\n",
      "Epoch: 4119: Train loss: 2.442471504211426 Valid loss: 1.2829160690307617\n",
      "Epoch: 4120: Train loss: 2.439894914627075 Valid loss: 0.9463728666305542\n",
      "Epoch: 4121: Train loss: 2.4419772624969482 Valid loss: 1.2830326557159424\n",
      "Epoch: 4122: Train loss: 2.4394102096557617 Valid loss: 0.9470831751823425\n",
      "Epoch: 4123: Train loss: 2.441485643386841 Valid loss: 1.2831549644470215\n",
      "Epoch: 4124: Train loss: 2.4389259815216064 Valid loss: 0.9477929472923279\n",
      "Epoch: 4125: Train loss: 2.440995216369629 Valid loss: 1.2832741737365723\n",
      "Epoch: 4126: Train loss: 2.438445568084717 Valid loss: 0.9485023617744446\n",
      "Epoch: 4127: Train loss: 2.440505266189575 Valid loss: 1.2833971977233887\n",
      "Epoch: 4128: Train loss: 2.4379642009735107 Valid loss: 0.9492127895355225\n",
      "Epoch: 4129: Train loss: 2.440016746520996 Valid loss: 1.2835181951522827\n",
      "Epoch: 4130: Train loss: 2.4374842643737793 Valid loss: 0.9499245882034302\n",
      "Epoch: 4131: Train loss: 2.439530372619629 Valid loss: 1.2836487293243408\n",
      "Epoch: 4132: Train loss: 2.437005043029785 Valid loss: 0.9506311416625977\n",
      "Epoch: 4133: Train loss: 2.43904185295105 Valid loss: 1.2837715148925781\n",
      "Epoch: 4134: Train loss: 2.436525821685791 Valid loss: 0.951333224773407\n",
      "Epoch: 4135: Train loss: 2.4385571479797363 Valid loss: 1.2838983535766602\n",
      "Epoch: 4136: Train loss: 2.436049461364746 Valid loss: 0.9520423412322998\n",
      "Epoch: 4137: Train loss: 2.4380710124969482 Valid loss: 1.284024715423584\n",
      "Epoch: 4138: Train loss: 2.4355721473693848 Valid loss: 0.9527489542961121\n",
      "Epoch: 4139: Train loss: 2.437587022781372 Valid loss: 1.2841551303863525\n",
      "Epoch: 4140: Train loss: 2.4350972175598145 Valid loss: 0.9534480571746826\n",
      "Epoch: 4141: Train loss: 2.4371044635772705 Valid loss: 1.2842856645584106\n",
      "Epoch: 4142: Train loss: 2.4346234798431396 Valid loss: 0.9541553258895874\n",
      "Epoch: 4143: Train loss: 2.4366250038146973 Valid loss: 1.284421443939209\n",
      "Epoch: 4144: Train loss: 2.4341516494750977 Valid loss: 0.9548606872558594\n",
      "Epoch: 4145: Train loss: 2.436145782470703 Valid loss: 1.284559726715088\n",
      "Epoch: 4146: Train loss: 2.43367862701416 Valid loss: 0.9555630087852478\n",
      "Epoch: 4147: Train loss: 2.4356653690338135 Valid loss: 1.2846968173980713\n",
      "Epoch: 4148: Train loss: 2.4332094192504883 Valid loss: 0.9562647938728333\n",
      "Epoch: 4149: Train loss: 2.4351885318756104 Valid loss: 1.2848345041275024\n",
      "Epoch: 4150: Train loss: 2.4327404499053955 Valid loss: 0.9569664597511292\n",
      "Epoch: 4151: Train loss: 2.434711456298828 Valid loss: 1.2849771976470947\n",
      "Epoch: 4152: Train loss: 2.4322736263275146 Valid loss: 0.9576695561408997\n",
      "Epoch: 4153: Train loss: 2.4342405796051025 Valid loss: 1.285125494003296\n",
      "Epoch: 4154: Train loss: 2.431809425354004 Valid loss: 0.9583708643913269\n",
      "Epoch: 4155: Train loss: 2.4337682723999023 Valid loss: 1.2852752208709717\n",
      "Epoch: 4156: Train loss: 2.4313459396362305 Valid loss: 0.9590729475021362\n",
      "Epoch: 4157: Train loss: 2.4332973957061768 Valid loss: 1.285421371459961\n",
      "Epoch: 4158: Train loss: 2.4308836460113525 Valid loss: 0.959772527217865\n",
      "Epoch: 4159: Train loss: 2.432826042175293 Valid loss: 1.2855778932571411\n",
      "Epoch: 4160: Train loss: 2.430419921875 Valid loss: 0.9604712724685669\n",
      "Epoch: 4161: Train loss: 2.432356595993042 Valid loss: 1.285723328590393\n",
      "Epoch: 4162: Train loss: 2.429959297180176 Valid loss: 0.9611740112304688\n",
      "Epoch: 4163: Train loss: 2.4318881034851074 Valid loss: 1.2858757972717285\n",
      "Epoch: 4164: Train loss: 2.4294991493225098 Valid loss: 0.9618704319000244\n",
      "Epoch: 4165: Train loss: 2.43142032623291 Valid loss: 1.2860324382781982\n",
      "Epoch: 4166: Train loss: 2.429039478302002 Valid loss: 0.9625669717788696\n",
      "Epoch: 4167: Train loss: 2.4309544563293457 Valid loss: 1.2861919403076172\n",
      "Epoch: 4168: Train loss: 2.428581476211548 Valid loss: 0.9632645845413208\n",
      "Epoch: 4169: Train loss: 2.430487871170044 Valid loss: 1.286344051361084\n",
      "Epoch: 4170: Train loss: 2.4281251430511475 Valid loss: 0.963961124420166\n",
      "Epoch: 4171: Train loss: 2.430026054382324 Valid loss: 1.2865095138549805\n",
      "Epoch: 4172: Train loss: 2.4276700019836426 Valid loss: 0.9646590352058411\n",
      "Epoch: 4173: Train loss: 2.4295642375946045 Valid loss: 1.2866744995117188\n",
      "Epoch: 4174: Train loss: 2.427215099334717 Valid loss: 0.9653531312942505\n",
      "Epoch: 4175: Train loss: 2.429100751876831 Valid loss: 1.2868335247039795\n",
      "Epoch: 4176: Train loss: 2.4267578125 Valid loss: 0.966046154499054\n",
      "Epoch: 4177: Train loss: 2.428638219833374 Valid loss: 1.2869939804077148\n",
      "Epoch: 4178: Train loss: 2.426304817199707 Valid loss: 0.9667388200759888\n",
      "Epoch: 4179: Train loss: 2.4281787872314453 Valid loss: 1.2871630191802979\n",
      "Epoch: 4180: Train loss: 2.4258553981781006 Valid loss: 0.9674349427223206\n",
      "Epoch: 4181: Train loss: 2.4277212619781494 Valid loss: 1.287337064743042\n",
      "Epoch: 4182: Train loss: 2.425405502319336 Valid loss: 0.9681280255317688\n",
      "Epoch: 4183: Train loss: 2.4272642135620117 Valid loss: 1.287503957748413\n",
      "Epoch: 4184: Train loss: 2.424955129623413 Valid loss: 0.9688198566436768\n",
      "Epoch: 4185: Train loss: 2.4268064498901367 Valid loss: 1.287677526473999\n",
      "Epoch: 4186: Train loss: 2.4245076179504395 Valid loss: 0.9695118069648743\n",
      "Epoch: 4187: Train loss: 2.426351547241211 Valid loss: 1.2878503799438477\n",
      "Epoch: 4188: Train loss: 2.424060821533203 Valid loss: 0.9702032208442688\n",
      "Epoch: 4189: Train loss: 2.4258980751037598 Valid loss: 1.2880210876464844\n",
      "Epoch: 4190: Train loss: 2.4236159324645996 Valid loss: 0.9708928465843201\n",
      "Epoch: 4191: Train loss: 2.425445556640625 Valid loss: 1.2882025241851807\n",
      "Epoch: 4192: Train loss: 2.423170566558838 Valid loss: 0.9715838432312012\n",
      "Epoch: 4193: Train loss: 2.4249916076660156 Valid loss: 1.288377285003662\n",
      "Epoch: 4194: Train loss: 2.4227235317230225 Valid loss: 0.9722729921340942\n",
      "Epoch: 4195: Train loss: 2.424539089202881 Valid loss: 1.2885535955429077\n",
      "Epoch: 4196: Train loss: 2.4222800731658936 Valid loss: 0.9729629158973694\n",
      "Epoch: 4197: Train loss: 2.4240870475769043 Valid loss: 1.288733959197998\n",
      "Epoch: 4198: Train loss: 2.4218358993530273 Valid loss: 0.9736471772193909\n",
      "Epoch: 4199: Train loss: 2.423638105392456 Valid loss: 1.2889145612716675\n",
      "Epoch: 4200: Train loss: 2.4213943481445312 Valid loss: 0.9743344187736511\n",
      "Epoch: 4201: Train loss: 2.4231889247894287 Valid loss: 1.2890952825546265\n",
      "Epoch: 4202: Train loss: 2.420952081680298 Valid loss: 0.9750198125839233\n",
      "Epoch: 4203: Train loss: 2.4227418899536133 Valid loss: 1.2892816066741943\n",
      "Epoch: 4204: Train loss: 2.420516014099121 Valid loss: 0.9757072925567627\n",
      "Epoch: 4205: Train loss: 2.4222981929779053 Valid loss: 1.2894713878631592\n",
      "Epoch: 4206: Train loss: 2.420078754425049 Valid loss: 0.9763929843902588\n",
      "Epoch: 4207: Train loss: 2.421854019165039 Valid loss: 1.2896629571914673\n",
      "Epoch: 4208: Train loss: 2.4196441173553467 Valid loss: 0.9770768284797668\n",
      "Epoch: 4209: Train loss: 2.4214117527008057 Valid loss: 1.289851427078247\n",
      "Epoch: 4210: Train loss: 2.4192094802856445 Valid loss: 0.9777663350105286\n",
      "Epoch: 4211: Train loss: 2.420971632003784 Valid loss: 1.2900441884994507\n",
      "Epoch: 4212: Train loss: 2.418775796890259 Valid loss: 0.9784526228904724\n",
      "Epoch: 4213: Train loss: 2.420531749725342 Valid loss: 1.2902429103851318\n",
      "Epoch: 4214: Train loss: 2.4183430671691895 Valid loss: 0.9791354537010193\n",
      "Epoch: 4215: Train loss: 2.4200899600982666 Valid loss: 1.2904348373413086\n",
      "Epoch: 4216: Train loss: 2.4179108142852783 Valid loss: 0.9798197150230408\n",
      "Epoch: 4217: Train loss: 2.4196531772613525 Valid loss: 1.290635347366333\n",
      "Epoch: 4218: Train loss: 2.41748046875 Valid loss: 0.9805008769035339\n",
      "Epoch: 4219: Train loss: 2.419215679168701 Valid loss: 1.2908354997634888\n",
      "Epoch: 4220: Train loss: 2.417048931121826 Valid loss: 0.9811820387840271\n",
      "Epoch: 4221: Train loss: 2.418776750564575 Valid loss: 1.2910339832305908\n",
      "Epoch: 4222: Train loss: 2.4166183471679688 Valid loss: 0.9818633794784546\n",
      "Epoch: 4223: Train loss: 2.418339967727661 Valid loss: 1.2912346124649048\n",
      "Epoch: 4224: Train loss: 2.4161911010742188 Valid loss: 0.982544481754303\n",
      "Epoch: 4225: Train loss: 2.4179046154022217 Valid loss: 1.2914314270019531\n",
      "Epoch: 4226: Train loss: 2.4157609939575195 Valid loss: 0.983222484588623\n",
      "Epoch: 4227: Train loss: 2.4174699783325195 Valid loss: 1.291639804840088\n",
      "Epoch: 4228: Train loss: 2.4153366088867188 Valid loss: 0.9839012026786804\n",
      "Epoch: 4229: Train loss: 2.417038679122925 Valid loss: 1.2918455600738525\n",
      "Epoch: 4230: Train loss: 2.4149134159088135 Valid loss: 0.9845852255821228\n",
      "Epoch: 4231: Train loss: 2.416607618331909 Valid loss: 1.292053461074829\n",
      "Epoch: 4232: Train loss: 2.41448974609375 Valid loss: 0.9852644801139832\n",
      "Epoch: 4233: Train loss: 2.4161794185638428 Valid loss: 1.292267084121704\n",
      "Epoch: 4234: Train loss: 2.414069652557373 Valid loss: 0.9859437942504883\n",
      "Epoch: 4235: Train loss: 2.4157495498657227 Valid loss: 1.2924809455871582\n",
      "Epoch: 4236: Train loss: 2.413647174835205 Valid loss: 0.9866210222244263\n",
      "Epoch: 4237: Train loss: 2.4153218269348145 Valid loss: 1.292691707611084\n",
      "Epoch: 4238: Train loss: 2.413226842880249 Valid loss: 0.987296998500824\n",
      "Epoch: 4239: Train loss: 2.414895534515381 Valid loss: 1.2929120063781738\n",
      "Epoch: 4240: Train loss: 2.4128074645996094 Valid loss: 0.9879769682884216\n",
      "Epoch: 4241: Train loss: 2.414469003677368 Valid loss: 1.2931264638900757\n",
      "Epoch: 4242: Train loss: 2.4123902320861816 Valid loss: 0.9886518716812134\n",
      "Epoch: 4243: Train loss: 2.414045810699463 Valid loss: 1.293342113494873\n",
      "Epoch: 4244: Train loss: 2.4119746685028076 Valid loss: 0.9893268346786499\n",
      "Epoch: 4245: Train loss: 2.4136221408843994 Valid loss: 1.2935603857040405\n",
      "Epoch: 4246: Train loss: 2.4115591049194336 Valid loss: 0.9900047779083252\n",
      "Epoch: 4247: Train loss: 2.4132018089294434 Valid loss: 1.2937884330749512\n",
      "Epoch: 4248: Train loss: 2.4111428260803223 Valid loss: 0.9906776547431946\n",
      "Epoch: 4249: Train loss: 2.412778854370117 Valid loss: 1.2940129041671753\n",
      "Epoch: 4250: Train loss: 2.410728931427002 Valid loss: 0.9913520216941833\n",
      "Epoch: 4251: Train loss: 2.4123587608337402 Valid loss: 1.294236660003662\n",
      "Epoch: 4252: Train loss: 2.410315752029419 Valid loss: 0.9920250773429871\n",
      "Epoch: 4253: Train loss: 2.411938190460205 Valid loss: 1.2944601774215698\n",
      "Epoch: 4254: Train loss: 2.4099035263061523 Valid loss: 0.9926965832710266\n",
      "Epoch: 4255: Train loss: 2.4115207195281982 Valid loss: 1.2946865558624268\n",
      "Epoch: 4256: Train loss: 2.4094929695129395 Valid loss: 0.9933695197105408\n",
      "Epoch: 4257: Train loss: 2.4111008644104004 Valid loss: 1.2949120998382568\n",
      "Epoch: 4258: Train loss: 2.409080982208252 Valid loss: 0.9940409660339355\n",
      "Epoch: 4259: Train loss: 2.4106831550598145 Valid loss: 1.2951371669769287\n",
      "Epoch: 4260: Train loss: 2.4086692333221436 Valid loss: 0.9947105646133423\n",
      "Epoch: 4261: Train loss: 2.4102652072906494 Valid loss: 1.2953717708587646\n",
      "Epoch: 4262: Train loss: 2.4082586765289307 Valid loss: 0.9953818321228027\n",
      "Epoch: 4263: Train loss: 2.4098501205444336 Valid loss: 1.2956023216247559\n",
      "Epoch: 4264: Train loss: 2.407850742340088 Valid loss: 0.9960499405860901\n",
      "Epoch: 4265: Train loss: 2.4094343185424805 Valid loss: 1.295833706855774\n",
      "Epoch: 4266: Train loss: 2.407444715499878 Valid loss: 0.9967194199562073\n",
      "Epoch: 4267: Train loss: 2.409022331237793 Valid loss: 1.2960660457611084\n",
      "Epoch: 4268: Train loss: 2.4070370197296143 Valid loss: 0.9973857402801514\n",
      "Epoch: 4269: Train loss: 2.4086084365844727 Valid loss: 1.2962969541549683\n",
      "Epoch: 4270: Train loss: 2.406633138656616 Valid loss: 0.9980524778366089\n",
      "Epoch: 4271: Train loss: 2.4081969261169434 Valid loss: 1.2965384721755981\n",
      "Epoch: 4272: Train loss: 2.406229019165039 Valid loss: 0.998722493648529\n",
      "Epoch: 4273: Train loss: 2.4077889919281006 Valid loss: 1.2967780828475952\n",
      "Epoch: 4274: Train loss: 2.405827045440674 Valid loss: 0.9993888139724731\n",
      "Epoch: 4275: Train loss: 2.4073777198791504 Valid loss: 1.2970142364501953\n",
      "Epoch: 4276: Train loss: 2.405423641204834 Valid loss: 1.000056505203247\n",
      "Epoch: 4277: Train loss: 2.4069695472717285 Valid loss: 1.2972543239593506\n",
      "Epoch: 4278: Train loss: 2.405019760131836 Valid loss: 1.0007232427597046\n",
      "Epoch: 4279: Train loss: 2.406559944152832 Valid loss: 1.297493815422058\n",
      "Epoch: 4280: Train loss: 2.4046199321746826 Valid loss: 1.0013867616653442\n",
      "Epoch: 4281: Train loss: 2.4061520099639893 Valid loss: 1.2977392673492432\n",
      "Epoch: 4282: Train loss: 2.4042201042175293 Valid loss: 1.002050518989563\n",
      "Epoch: 4283: Train loss: 2.4057483673095703 Valid loss: 1.297986388206482\n",
      "Epoch: 4284: Train loss: 2.403822183609009 Valid loss: 1.0027191638946533\n",
      "Epoch: 4285: Train loss: 2.4053430557250977 Valid loss: 1.2982285022735596\n",
      "Epoch: 4286: Train loss: 2.4034249782562256 Valid loss: 1.0033812522888184\n",
      "Epoch: 4287: Train loss: 2.404939651489258 Valid loss: 1.2984769344329834\n",
      "Epoch: 4288: Train loss: 2.4030303955078125 Valid loss: 1.0040429830551147\n",
      "Epoch: 4289: Train loss: 2.404539108276367 Valid loss: 1.2987277507781982\n",
      "Epoch: 4290: Train loss: 2.4026355743408203 Valid loss: 1.0047101974487305\n",
      "Epoch: 4291: Train loss: 2.4041383266448975 Valid loss: 1.2989835739135742\n",
      "Epoch: 4292: Train loss: 2.4022417068481445 Valid loss: 1.0053709745407104\n",
      "Epoch: 4293: Train loss: 2.4037370681762695 Valid loss: 1.2992377281188965\n",
      "Epoch: 4294: Train loss: 2.401848554611206 Valid loss: 1.00603187084198\n",
      "Epoch: 4295: Train loss: 2.40333890914917 Valid loss: 1.2994909286499023\n",
      "Epoch: 4296: Train loss: 2.401456117630005 Valid loss: 1.00669527053833\n",
      "Epoch: 4297: Train loss: 2.402939796447754 Valid loss: 1.2997469902038574\n",
      "Epoch: 4298: Train loss: 2.4010629653930664 Valid loss: 1.0073546171188354\n",
      "Epoch: 4299: Train loss: 2.4025392532348633 Valid loss: 1.299998164176941\n",
      "Epoch: 4300: Train loss: 2.4006714820861816 Valid loss: 1.0080174207687378\n",
      "Epoch: 4301: Train loss: 2.40214204788208 Valid loss: 1.3002547025680542\n",
      "Epoch: 4302: Train loss: 2.400282382965088 Valid loss: 1.0086747407913208\n",
      "Epoch: 4303: Train loss: 2.401747703552246 Valid loss: 1.3005123138427734\n",
      "Epoch: 4304: Train loss: 2.3998937606811523 Valid loss: 1.009337306022644\n",
      "Epoch: 4305: Train loss: 2.4013538360595703 Valid loss: 1.3007822036743164\n",
      "Epoch: 4306: Train loss: 2.399505376815796 Valid loss: 1.0099947452545166\n",
      "Epoch: 4307: Train loss: 2.4009599685668945 Valid loss: 1.301039695739746\n",
      "Epoch: 4308: Train loss: 2.399117946624756 Valid loss: 1.0106532573699951\n",
      "Epoch: 4309: Train loss: 2.4005661010742188 Valid loss: 1.3013017177581787\n",
      "Epoch: 4310: Train loss: 2.3987338542938232 Valid loss: 1.011312484741211\n",
      "Epoch: 4311: Train loss: 2.400176763534546 Valid loss: 1.3015687465667725\n",
      "Epoch: 4312: Train loss: 2.3983511924743652 Valid loss: 1.0119717121124268\n",
      "Epoch: 4313: Train loss: 2.3997879028320312 Valid loss: 1.30184006690979\n",
      "Epoch: 4314: Train loss: 2.397967576980591 Valid loss: 1.0126255750656128\n",
      "Epoch: 4315: Train loss: 2.3993959426879883 Valid loss: 1.3021063804626465\n",
      "Epoch: 4316: Train loss: 2.3975820541381836 Valid loss: 1.013283371925354\n",
      "Epoch: 4317: Train loss: 2.3990063667297363 Valid loss: 1.302375316619873\n",
      "Epoch: 4318: Train loss: 2.397202253341675 Valid loss: 1.013939380645752\n",
      "Epoch: 4319: Train loss: 2.398621082305908 Valid loss: 1.3026490211486816\n",
      "Epoch: 4320: Train loss: 2.3968212604522705 Valid loss: 1.014596939086914\n",
      "Epoch: 4321: Train loss: 2.3982324600219727 Valid loss: 1.3029186725616455\n",
      "Epoch: 4322: Train loss: 2.396442174911499 Valid loss: 1.015254020690918\n",
      "Epoch: 4323: Train loss: 2.397847890853882 Valid loss: 1.3031949996948242\n",
      "Epoch: 4324: Train loss: 2.396063804626465 Valid loss: 1.0159062147140503\n",
      "Epoch: 4325: Train loss: 2.3974618911743164 Valid loss: 1.303466558456421\n",
      "Epoch: 4326: Train loss: 2.395683526992798 Valid loss: 1.0165598392486572\n",
      "Epoch: 4327: Train loss: 2.397075891494751 Valid loss: 1.3037407398223877\n",
      "Epoch: 4328: Train loss: 2.395306348800659 Valid loss: 1.0172134637832642\n",
      "Epoch: 4329: Train loss: 2.3966917991638184 Valid loss: 1.3040200471878052\n",
      "Epoch: 4330: Train loss: 2.3949267864227295 Valid loss: 1.0178662538528442\n",
      "Epoch: 4331: Train loss: 2.3963100910186768 Valid loss: 1.3042949438095093\n",
      "Epoch: 4332: Train loss: 2.3945510387420654 Valid loss: 1.0185197591781616\n",
      "Epoch: 4333: Train loss: 2.3959269523620605 Valid loss: 1.3045746088027954\n",
      "Epoch: 4334: Train loss: 2.3941755294799805 Valid loss: 1.0191707611083984\n",
      "Epoch: 4335: Train loss: 2.395545244216919 Valid loss: 1.3048505783081055\n",
      "Epoch: 4336: Train loss: 2.393799304962158 Valid loss: 1.0198191404342651\n",
      "Epoch: 4337: Train loss: 2.395162343978882 Valid loss: 1.305128574371338\n",
      "Epoch: 4338: Train loss: 2.393427610397339 Valid loss: 1.0204704999923706\n",
      "Epoch: 4339: Train loss: 2.394785165786743 Valid loss: 1.3054184913635254\n",
      "Epoch: 4340: Train loss: 2.3930552005767822 Valid loss: 1.0211211442947388\n",
      "Epoch: 4341: Train loss: 2.394406795501709 Valid loss: 1.3057000637054443\n",
      "Epoch: 4342: Train loss: 2.392683982849121 Valid loss: 1.0217727422714233\n",
      "Epoch: 4343: Train loss: 2.3940305709838867 Valid loss: 1.305985689163208\n",
      "Epoch: 4344: Train loss: 2.3923137187957764 Valid loss: 1.0224226713180542\n",
      "Epoch: 4345: Train loss: 2.3936548233032227 Valid loss: 1.3062727451324463\n",
      "Epoch: 4346: Train loss: 2.3919456005096436 Valid loss: 1.023073434829712\n",
      "Epoch: 4347: Train loss: 2.3932812213897705 Valid loss: 1.3065615892410278\n",
      "Epoch: 4348: Train loss: 2.3915796279907227 Valid loss: 1.0237224102020264\n",
      "Epoch: 4349: Train loss: 2.3929076194763184 Valid loss: 1.306854248046875\n",
      "Epoch: 4350: Train loss: 2.3912131786346436 Valid loss: 1.0243734121322632\n",
      "Epoch: 4351: Train loss: 2.392535924911499 Valid loss: 1.3071506023406982\n",
      "Epoch: 4352: Train loss: 2.39084529876709 Valid loss: 1.025019645690918\n",
      "Epoch: 4353: Train loss: 2.392165184020996 Valid loss: 1.307444453239441\n",
      "Epoch: 4354: Train loss: 2.3904805183410645 Valid loss: 1.0256693363189697\n",
      "Epoch: 4355: Train loss: 2.3917908668518066 Valid loss: 1.3077336549758911\n",
      "Epoch: 4356: Train loss: 2.39011287689209 Valid loss: 1.0263140201568604\n",
      "Epoch: 4357: Train loss: 2.391420364379883 Valid loss: 1.30802583694458\n",
      "Epoch: 4358: Train loss: 2.389749765396118 Valid loss: 1.0269591808319092\n",
      "Epoch: 4359: Train loss: 2.39104962348938 Valid loss: 1.3083240985870361\n",
      "Epoch: 4360: Train loss: 2.3893861770629883 Valid loss: 1.0276079177856445\n",
      "Epoch: 4361: Train loss: 2.390681743621826 Valid loss: 1.3086216449737549\n",
      "Epoch: 4362: Train loss: 2.389024496078491 Valid loss: 1.0282528400421143\n",
      "Epoch: 4363: Train loss: 2.390313148498535 Valid loss: 1.3089241981506348\n",
      "Epoch: 4364: Train loss: 2.3886630535125732 Valid loss: 1.0288981199264526\n",
      "Epoch: 4365: Train loss: 2.389948844909668 Valid loss: 1.3092223405838013\n",
      "Epoch: 4366: Train loss: 2.3883004188537598 Valid loss: 1.0295429229736328\n",
      "Epoch: 4367: Train loss: 2.3895792961120605 Valid loss: 1.3095210790634155\n",
      "Epoch: 4368: Train loss: 2.3879404067993164 Valid loss: 1.0301872491836548\n",
      "Epoch: 4369: Train loss: 2.3892128467559814 Valid loss: 1.3098253011703491\n",
      "Epoch: 4370: Train loss: 2.3875820636749268 Valid loss: 1.0308297872543335\n",
      "Epoch: 4371: Train loss: 2.3888471126556396 Valid loss: 1.3101255893707275\n",
      "Epoch: 4372: Train loss: 2.387223243713379 Valid loss: 1.0314733982086182\n",
      "Epoch: 4373: Train loss: 2.388484477996826 Valid loss: 1.3104283809661865\n",
      "Epoch: 4374: Train loss: 2.3868660926818848 Valid loss: 1.0321179628372192\n",
      "Epoch: 4375: Train loss: 2.388123035430908 Valid loss: 1.310734510421753\n",
      "Epoch: 4376: Train loss: 2.3865091800689697 Valid loss: 1.0327564477920532\n",
      "Epoch: 4377: Train loss: 2.3877604007720947 Valid loss: 1.3110463619232178\n",
      "Epoch: 4378: Train loss: 2.3861544132232666 Valid loss: 1.033401370048523\n",
      "Epoch: 4379: Train loss: 2.3874001502990723 Valid loss: 1.3113511800765991\n",
      "Epoch: 4380: Train loss: 2.38580060005188 Valid loss: 1.034041404724121\n",
      "Epoch: 4381: Train loss: 2.387040615081787 Valid loss: 1.3116604089736938\n",
      "Epoch: 4382: Train loss: 2.385446786880493 Valid loss: 1.0346835851669312\n",
      "Epoch: 4383: Train loss: 2.38668155670166 Valid loss: 1.3119720220565796\n",
      "Epoch: 4384: Train loss: 2.3850932121276855 Valid loss: 1.035325527191162\n",
      "Epoch: 4385: Train loss: 2.386322021484375 Valid loss: 1.3122795820236206\n",
      "Epoch: 4386: Train loss: 2.3847427368164062 Valid loss: 1.0359667539596558\n",
      "Epoch: 4387: Train loss: 2.385962963104248 Valid loss: 1.3125944137573242\n",
      "Epoch: 4388: Train loss: 2.3843894004821777 Valid loss: 1.036603569984436\n",
      "Epoch: 4389: Train loss: 2.385608196258545 Valid loss: 1.3129079341888428\n",
      "Epoch: 4390: Train loss: 2.3840396404266357 Valid loss: 1.0372415781021118\n",
      "Epoch: 4391: Train loss: 2.385251760482788 Valid loss: 1.313222885131836\n",
      "Epoch: 4392: Train loss: 2.3836889266967773 Valid loss: 1.037879467010498\n",
      "Epoch: 4393: Train loss: 2.3848936557769775 Valid loss: 1.313535451889038\n",
      "Epoch: 4394: Train loss: 2.38333797454834 Valid loss: 1.0385180711746216\n",
      "Epoch: 4395: Train loss: 2.384539842605591 Valid loss: 1.3138536214828491\n",
      "Epoch: 4396: Train loss: 2.3829901218414307 Valid loss: 1.0391550064086914\n",
      "Epoch: 4397: Train loss: 2.3841848373413086 Valid loss: 1.3141722679138184\n",
      "Epoch: 4398: Train loss: 2.3826417922973633 Valid loss: 1.039793610572815\n",
      "Epoch: 4399: Train loss: 2.3838319778442383 Valid loss: 1.3144928216934204\n",
      "Epoch: 4400: Train loss: 2.3822972774505615 Valid loss: 1.0404303073883057\n",
      "Epoch: 4401: Train loss: 2.38348126411438 Valid loss: 1.3148090839385986\n",
      "Epoch: 4402: Train loss: 2.3819496631622314 Valid loss: 1.0410665273666382\n",
      "Epoch: 4403: Train loss: 2.383127212524414 Valid loss: 1.3151335716247559\n",
      "Epoch: 4404: Train loss: 2.381603717803955 Valid loss: 1.0417025089263916\n",
      "Epoch: 4405: Train loss: 2.3827779293060303 Valid loss: 1.3154563903808594\n",
      "Epoch: 4406: Train loss: 2.381258726119995 Valid loss: 1.042335867881775\n",
      "Epoch: 4407: Train loss: 2.3824269771575928 Valid loss: 1.3157768249511719\n",
      "Epoch: 4408: Train loss: 2.380915641784668 Valid loss: 1.0429717302322388\n",
      "Epoch: 4409: Train loss: 2.382079839706421 Valid loss: 1.3161017894744873\n",
      "Epoch: 4410: Train loss: 2.3805761337280273 Valid loss: 1.0436067581176758\n",
      "Epoch: 4411: Train loss: 2.3817338943481445 Valid loss: 1.3164311647415161\n",
      "Epoch: 4412: Train loss: 2.380232095718384 Valid loss: 1.0442403554916382\n",
      "Epoch: 4413: Train loss: 2.3813836574554443 Valid loss: 1.3167595863342285\n",
      "Epoch: 4414: Train loss: 2.3798913955688477 Valid loss: 1.0448760986328125\n",
      "Epoch: 4415: Train loss: 2.381037950515747 Valid loss: 1.3170828819274902\n",
      "Epoch: 4416: Train loss: 2.379549980163574 Valid loss: 1.0455076694488525\n",
      "Epoch: 4417: Train loss: 2.380692481994629 Valid loss: 1.3174123764038086\n",
      "Epoch: 4418: Train loss: 2.3792099952697754 Valid loss: 1.046139121055603\n",
      "Epoch: 4419: Train loss: 2.3803467750549316 Valid loss: 1.3177378177642822\n",
      "Epoch: 4420: Train loss: 2.378873825073242 Valid loss: 1.04677152633667\n",
      "Epoch: 4421: Train loss: 2.3800032138824463 Valid loss: 1.3180733919143677\n",
      "Epoch: 4422: Train loss: 2.3785340785980225 Valid loss: 1.0474045276641846\n",
      "Epoch: 4423: Train loss: 2.379662036895752 Valid loss: 1.3184078931808472\n",
      "Epoch: 4424: Train loss: 2.378200054168701 Valid loss: 1.0480389595031738\n",
      "Epoch: 4425: Train loss: 2.3793208599090576 Valid loss: 1.3187428712844849\n",
      "Epoch: 4426: Train loss: 2.377865791320801 Valid loss: 1.0486690998077393\n",
      "Epoch: 4427: Train loss: 2.378981351852417 Valid loss: 1.319080114364624\n",
      "Epoch: 4428: Train loss: 2.377530813217163 Valid loss: 1.0493018627166748\n",
      "Epoch: 4429: Train loss: 2.3786416053771973 Valid loss: 1.3194208145141602\n",
      "Epoch: 4430: Train loss: 2.377195119857788 Valid loss: 1.0499327182769775\n",
      "Epoch: 4431: Train loss: 2.3782997131347656 Valid loss: 1.3197567462921143\n",
      "Epoch: 4432: Train loss: 2.3768622875213623 Valid loss: 1.050560712814331\n",
      "Epoch: 4433: Train loss: 2.377962827682495 Valid loss: 1.3200953006744385\n",
      "Epoch: 4434: Train loss: 2.3765320777893066 Valid loss: 1.0511939525604248\n",
      "Epoch: 4435: Train loss: 2.377626657485962 Valid loss: 1.3204351663589478\n",
      "Epoch: 4436: Train loss: 2.376199960708618 Valid loss: 1.0518227815628052\n",
      "Epoch: 4437: Train loss: 2.377291679382324 Valid loss: 1.3207803964614868\n",
      "Epoch: 4438: Train loss: 2.3758704662323 Valid loss: 1.0524564981460571\n",
      "Epoch: 4439: Train loss: 2.376955986022949 Valid loss: 1.3211288452148438\n",
      "Epoch: 4440: Train loss: 2.3755412101745605 Valid loss: 1.0530813932418823\n",
      "Epoch: 4441: Train loss: 2.376622438430786 Valid loss: 1.3214685916900635\n",
      "Epoch: 4442: Train loss: 2.375211477279663 Valid loss: 1.0537116527557373\n",
      "Epoch: 4443: Train loss: 2.3762869834899902 Valid loss: 1.3218129873275757\n",
      "Epoch: 4444: Train loss: 2.374885320663452 Valid loss: 1.0543389320373535\n",
      "Epoch: 4445: Train loss: 2.37595272064209 Valid loss: 1.3221591711044312\n",
      "Epoch: 4446: Train loss: 2.374556064605713 Valid loss: 1.0549665689468384\n",
      "Epoch: 4447: Train loss: 2.3756210803985596 Valid loss: 1.3225059509277344\n",
      "Epoch: 4448: Train loss: 2.3742282390594482 Valid loss: 1.0555909872055054\n",
      "Epoch: 4449: Train loss: 2.375288248062134 Valid loss: 1.322850227355957\n",
      "Epoch: 4450: Train loss: 2.3739013671875 Valid loss: 1.0562188625335693\n",
      "Epoch: 4451: Train loss: 2.374955177307129 Valid loss: 1.323198676109314\n",
      "Epoch: 4452: Train loss: 2.373575448989868 Valid loss: 1.056845784187317\n",
      "Epoch: 4453: Train loss: 2.3746252059936523 Valid loss: 1.3235522508621216\n",
      "Epoch: 4454: Train loss: 2.373251438140869 Valid loss: 1.0574731826782227\n",
      "Epoch: 4455: Train loss: 2.374295711517334 Valid loss: 1.323901653289795\n",
      "Epoch: 4456: Train loss: 2.3729281425476074 Valid loss: 1.0580943822860718\n",
      "Epoch: 4457: Train loss: 2.373966932296753 Valid loss: 1.3242518901824951\n",
      "Epoch: 4458: Train loss: 2.3726041316986084 Valid loss: 1.058720350265503\n",
      "Epoch: 4459: Train loss: 2.3736391067504883 Valid loss: 1.3246046304702759\n",
      "Epoch: 4460: Train loss: 2.372283935546875 Valid loss: 1.0593481063842773\n",
      "Epoch: 4461: Train loss: 2.373311996459961 Valid loss: 1.3249620199203491\n",
      "Epoch: 4462: Train loss: 2.371962308883667 Valid loss: 1.0599725246429443\n",
      "Epoch: 4463: Train loss: 2.3729867935180664 Valid loss: 1.3253165483474731\n",
      "Epoch: 4464: Train loss: 2.371640205383301 Valid loss: 1.060594081878662\n",
      "Epoch: 4465: Train loss: 2.372659921646118 Valid loss: 1.3256734609603882\n",
      "Epoch: 4466: Train loss: 2.3713204860687256 Valid loss: 1.0612194538116455\n",
      "Epoch: 4467: Train loss: 2.3723363876342773 Valid loss: 1.326030969619751\n",
      "Epoch: 4468: Train loss: 2.3710012435913086 Valid loss: 1.0618420839309692\n",
      "Epoch: 4469: Train loss: 2.372013568878174 Valid loss: 1.3263916969299316\n",
      "Epoch: 4470: Train loss: 2.370684862136841 Valid loss: 1.0624651908874512\n",
      "Epoch: 4471: Train loss: 2.371687650680542 Valid loss: 1.3267483711242676\n",
      "Epoch: 4472: Train loss: 2.3703644275665283 Valid loss: 1.0630863904953003\n",
      "Epoch: 4473: Train loss: 2.371363401412964 Valid loss: 1.3271050453186035\n",
      "Epoch: 4474: Train loss: 2.3700461387634277 Valid loss: 1.0637049674987793\n",
      "Epoch: 4475: Train loss: 2.3710410594940186 Valid loss: 1.3274643421173096\n",
      "Epoch: 4476: Train loss: 2.369729518890381 Valid loss: 1.064329981803894\n",
      "Epoch: 4477: Train loss: 2.370718479156494 Valid loss: 1.327823519706726\n",
      "Epoch: 4478: Train loss: 2.369412660598755 Valid loss: 1.0649471282958984\n",
      "Epoch: 4479: Train loss: 2.3703975677490234 Valid loss: 1.3281886577606201\n",
      "Epoch: 4480: Train loss: 2.3690967559814453 Valid loss: 1.0655690431594849\n",
      "Epoch: 4481: Train loss: 2.3700764179229736 Valid loss: 1.3285499811172485\n",
      "Epoch: 4482: Train loss: 2.368781805038452 Valid loss: 1.0661909580230713\n",
      "Epoch: 4483: Train loss: 2.3697571754455566 Valid loss: 1.3289114236831665\n",
      "Epoch: 4484: Train loss: 2.3684675693511963 Valid loss: 1.0668079853057861\n",
      "Epoch: 4485: Train loss: 2.3694357872009277 Valid loss: 1.329276204109192\n",
      "Epoch: 4486: Train loss: 2.368152141571045 Valid loss: 1.0674238204956055\n",
      "Epoch: 4487: Train loss: 2.36911678314209 Valid loss: 1.3296434879302979\n",
      "Epoch: 4488: Train loss: 2.367839813232422 Valid loss: 1.068045735359192\n",
      "Epoch: 4489: Train loss: 2.368800401687622 Valid loss: 1.3300074338912964\n",
      "Epoch: 4490: Train loss: 2.367527961730957 Valid loss: 1.0686622858047485\n",
      "Epoch: 4491: Train loss: 2.368483543395996 Valid loss: 1.3303743600845337\n",
      "Epoch: 4492: Train loss: 2.367216110229492 Valid loss: 1.0692812204360962\n",
      "Epoch: 4493: Train loss: 2.368168830871582 Valid loss: 1.3307445049285889\n",
      "Epoch: 4494: Train loss: 2.3669068813323975 Valid loss: 1.069899320602417\n",
      "Epoch: 4495: Train loss: 2.367852210998535 Valid loss: 1.3311131000518799\n",
      "Epoch: 4496: Train loss: 2.3665966987609863 Valid loss: 1.0705149173736572\n",
      "Epoch: 4497: Train loss: 2.3675355911254883 Valid loss: 1.3314803838729858\n",
      "Epoch: 4498: Train loss: 2.366286516189575 Valid loss: 1.0711312294006348\n",
      "Epoch: 4499: Train loss: 2.367222785949707 Valid loss: 1.3318541049957275\n",
      "Epoch: 4500: Train loss: 2.3659796714782715 Valid loss: 1.071748971939087\n",
      "Epoch: 4501: Train loss: 2.366910457611084 Valid loss: 1.33222496509552\n",
      "Epoch: 4502: Train loss: 2.3656721115112305 Valid loss: 1.0723652839660645\n",
      "Epoch: 4503: Train loss: 2.3666000366210938 Valid loss: 1.3326020240783691\n",
      "Epoch: 4504: Train loss: 2.365363597869873 Valid loss: 1.0729801654815674\n",
      "Epoch: 4505: Train loss: 2.3662869930267334 Valid loss: 1.3329728841781616\n",
      "Epoch: 4506: Train loss: 2.365058183670044 Valid loss: 1.0735950469970703\n",
      "Epoch: 4507: Train loss: 2.365978240966797 Valid loss: 1.3333505392074585\n",
      "Epoch: 4508: Train loss: 2.364753484725952 Valid loss: 1.074211597442627\n",
      "Epoch: 4509: Train loss: 2.3656671047210693 Valid loss: 1.333728313446045\n",
      "Epoch: 4510: Train loss: 2.3644511699676514 Valid loss: 1.0748264789581299\n",
      "Epoch: 4511: Train loss: 2.3653576374053955 Valid loss: 1.3341050148010254\n",
      "Epoch: 4512: Train loss: 2.3641459941864014 Valid loss: 1.0754404067993164\n",
      "Epoch: 4513: Train loss: 2.3650479316711426 Valid loss: 1.3344848155975342\n",
      "Epoch: 4514: Train loss: 2.363842725753784 Valid loss: 1.0760551691055298\n",
      "Epoch: 4515: Train loss: 2.3647401332855225 Valid loss: 1.3348612785339355\n",
      "Epoch: 4516: Train loss: 2.3635387420654297 Valid loss: 1.0766664743423462\n",
      "Epoch: 4517: Train loss: 2.3644332885742188 Valid loss: 1.3352422714233398\n",
      "Epoch: 4518: Train loss: 2.36323618888855 Valid loss: 1.0772817134857178\n",
      "Epoch: 4519: Train loss: 2.364123821258545 Valid loss: 1.3356157541275024\n",
      "Epoch: 4520: Train loss: 2.36293363571167 Valid loss: 1.0778932571411133\n",
      "Epoch: 4521: Train loss: 2.363820791244507 Valid loss: 1.3360011577606201\n",
      "Epoch: 4522: Train loss: 2.3626344203948975 Valid loss: 1.0785073041915894\n",
      "Epoch: 4523: Train loss: 2.363515853881836 Valid loss: 1.3363862037658691\n",
      "Epoch: 4524: Train loss: 2.3623342514038086 Valid loss: 1.0791184902191162\n",
      "Epoch: 4525: Train loss: 2.3632092475891113 Valid loss: 1.3367702960968018\n",
      "Epoch: 4526: Train loss: 2.3620333671569824 Valid loss: 1.0797278881072998\n",
      "Epoch: 4527: Train loss: 2.362902879714966 Valid loss: 1.3371516466140747\n",
      "Epoch: 4528: Train loss: 2.3617348670959473 Valid loss: 1.0803401470184326\n",
      "Epoch: 4529: Train loss: 2.3625988960266113 Valid loss: 1.3375351428985596\n",
      "Epoch: 4530: Train loss: 2.3614375591278076 Valid loss: 1.0809510946273804\n",
      "Epoch: 4531: Train loss: 2.362297773361206 Valid loss: 1.3379205465316772\n",
      "Epoch: 4532: Train loss: 2.361140251159668 Valid loss: 1.081561803817749\n",
      "Epoch: 4533: Train loss: 2.361997127532959 Valid loss: 1.3383088111877441\n",
      "Epoch: 4534: Train loss: 2.3608431816101074 Valid loss: 1.0821716785430908\n",
      "Epoch: 4535: Train loss: 2.3616955280303955 Valid loss: 1.338698148727417\n",
      "Epoch: 4536: Train loss: 2.360548973083496 Valid loss: 1.0827823877334595\n",
      "Epoch: 4537: Train loss: 2.3613970279693604 Valid loss: 1.3390867710113525\n",
      "Epoch: 4538: Train loss: 2.3602542877197266 Valid loss: 1.0833932161331177\n",
      "Epoch: 4539: Train loss: 2.3610963821411133 Valid loss: 1.3394761085510254\n",
      "Epoch: 4540: Train loss: 2.359959840774536 Valid loss: 1.0840015411376953\n",
      "Epoch: 4541: Train loss: 2.360799789428711 Valid loss: 1.3398709297180176\n",
      "Epoch: 4542: Train loss: 2.359666347503662 Valid loss: 1.0846115350723267\n",
      "Epoch: 4543: Train loss: 2.360501289367676 Valid loss: 1.3402612209320068\n",
      "Epoch: 4544: Train loss: 2.3593738079071045 Valid loss: 1.0852200984954834\n",
      "Epoch: 4545: Train loss: 2.360206365585327 Valid loss: 1.3406578302383423\n",
      "Epoch: 4546: Train loss: 2.3590831756591797 Valid loss: 1.085830569267273\n",
      "Epoch: 4547: Train loss: 2.359908103942871 Valid loss: 1.3410464525222778\n",
      "Epoch: 4548: Train loss: 2.3587911128997803 Valid loss: 1.086437702178955\n",
      "Epoch: 4549: Train loss: 2.3596127033233643 Valid loss: 1.3414382934570312\n",
      "Epoch: 4550: Train loss: 2.35850191116333 Valid loss: 1.0870481729507446\n",
      "Epoch: 4551: Train loss: 2.359318494796753 Valid loss: 1.3418400287628174\n",
      "Epoch: 4552: Train loss: 2.358213186264038 Valid loss: 1.087654709815979\n",
      "Epoch: 4553: Train loss: 2.3590261936187744 Valid loss: 1.3422391414642334\n",
      "Epoch: 4554: Train loss: 2.357926368713379 Valid loss: 1.0882635116577148\n",
      "Epoch: 4555: Train loss: 2.3587350845336914 Valid loss: 1.3426363468170166\n",
      "Epoch: 4556: Train loss: 2.3576369285583496 Valid loss: 1.088871717453003\n",
      "Epoch: 4557: Train loss: 2.358440637588501 Valid loss: 1.3430366516113281\n",
      "Epoch: 4558: Train loss: 2.3573501110076904 Valid loss: 1.089477777481079\n",
      "Epoch: 4559: Train loss: 2.3581488132476807 Valid loss: 1.3434375524520874\n",
      "Epoch: 4560: Train loss: 2.3570621013641357 Valid loss: 1.0900846719741821\n",
      "Epoch: 4561: Train loss: 2.3578577041625977 Valid loss: 1.3438389301300049\n",
      "Epoch: 4562: Train loss: 2.3567745685577393 Valid loss: 1.0906895399093628\n",
      "Epoch: 4563: Train loss: 2.3575658798217773 Valid loss: 1.3442389965057373\n",
      "Epoch: 4564: Train loss: 2.3564908504486084 Valid loss: 1.0912963151931763\n",
      "Epoch: 4565: Train loss: 2.3572778701782227 Valid loss: 1.3446390628814697\n",
      "Epoch: 4566: Train loss: 2.3562064170837402 Valid loss: 1.0919007062911987\n",
      "Epoch: 4567: Train loss: 2.356987953186035 Valid loss: 1.3450450897216797\n",
      "Epoch: 4568: Train loss: 2.355923652648926 Valid loss: 1.0925099849700928\n",
      "Epoch: 4569: Train loss: 2.356701374053955 Valid loss: 1.345449686050415\n",
      "Epoch: 4570: Train loss: 2.355639934539795 Valid loss: 1.0931148529052734\n",
      "Epoch: 4571: Train loss: 2.356414318084717 Valid loss: 1.3458529710769653\n",
      "Epoch: 4572: Train loss: 2.355358362197876 Valid loss: 1.093721628189087\n",
      "Epoch: 4573: Train loss: 2.3561272621154785 Valid loss: 1.346264362335205\n",
      "Epoch: 4574: Train loss: 2.3550755977630615 Valid loss: 1.0943207740783691\n",
      "Epoch: 4575: Train loss: 2.3558411598205566 Valid loss: 1.3466706275939941\n",
      "Epoch: 4576: Train loss: 2.3547935485839844 Valid loss: 1.0949249267578125\n",
      "Epoch: 4577: Train loss: 2.355552911758423 Valid loss: 1.347078561782837\n",
      "Epoch: 4578: Train loss: 2.354510545730591 Valid loss: 1.095529317855835\n",
      "Epoch: 4579: Train loss: 2.3552680015563965 Valid loss: 1.3474851846694946\n",
      "Epoch: 4580: Train loss: 2.354231119155884 Valid loss: 1.0961328744888306\n",
      "Epoch: 4581: Train loss: 2.3549821376800537 Valid loss: 1.347890019416809\n",
      "Epoch: 4582: Train loss: 2.3539505004882812 Valid loss: 1.0967329740524292\n",
      "Epoch: 4583: Train loss: 2.354698657989502 Valid loss: 1.3483047485351562\n",
      "Epoch: 4584: Train loss: 2.353670358657837 Valid loss: 1.0973396301269531\n",
      "Epoch: 4585: Train loss: 2.354414939880371 Valid loss: 1.3487142324447632\n",
      "Epoch: 4586: Train loss: 2.3533926010131836 Valid loss: 1.0979400873184204\n",
      "Epoch: 4587: Train loss: 2.3541297912597656 Valid loss: 1.3491226434707642\n",
      "Epoch: 4588: Train loss: 2.35311222076416 Valid loss: 1.0985392332077026\n",
      "Epoch: 4589: Train loss: 2.353846311569214 Valid loss: 1.3495359420776367\n",
      "Epoch: 4590: Train loss: 2.352834701538086 Valid loss: 1.0991441011428833\n",
      "Epoch: 4591: Train loss: 2.3535642623901367 Valid loss: 1.3499447107315063\n",
      "Epoch: 4592: Train loss: 2.3525543212890625 Valid loss: 1.0997415781021118\n",
      "Epoch: 4593: Train loss: 2.3532800674438477 Valid loss: 1.3503515720367432\n",
      "Epoch: 4594: Train loss: 2.352276563644409 Valid loss: 1.1003401279449463\n",
      "Epoch: 4595: Train loss: 2.352999210357666 Valid loss: 1.3507661819458008\n",
      "Epoch: 4596: Train loss: 2.351999282836914 Valid loss: 1.1009407043457031\n",
      "Epoch: 4597: Train loss: 2.3527166843414307 Valid loss: 1.3511766195297241\n",
      "Epoch: 4598: Train loss: 2.351724147796631 Valid loss: 1.1015392541885376\n",
      "Epoch: 4599: Train loss: 2.3524372577667236 Valid loss: 1.3515881299972534\n",
      "Epoch: 4600: Train loss: 2.3514461517333984 Valid loss: 1.1021370887756348\n",
      "Epoch: 4601: Train loss: 2.352156639099121 Valid loss: 1.3520045280456543\n",
      "Epoch: 4602: Train loss: 2.35117244720459 Valid loss: 1.102735996246338\n",
      "Epoch: 4603: Train loss: 2.3518764972686768 Valid loss: 1.3524209260940552\n",
      "Epoch: 4604: Train loss: 2.3509011268615723 Valid loss: 1.1033382415771484\n",
      "Epoch: 4605: Train loss: 2.351602077484131 Valid loss: 1.352840781211853\n",
      "Epoch: 4606: Train loss: 2.350628137588501 Valid loss: 1.1039360761642456\n",
      "Epoch: 4607: Train loss: 2.351325511932373 Valid loss: 1.3532581329345703\n",
      "Epoch: 4608: Train loss: 2.3503551483154297 Valid loss: 1.1045348644256592\n",
      "Epoch: 4609: Train loss: 2.351048469543457 Valid loss: 1.3536773920059204\n",
      "Epoch: 4610: Train loss: 2.350083827972412 Valid loss: 1.1051346063613892\n",
      "Epoch: 4611: Train loss: 2.3507723808288574 Valid loss: 1.354095220565796\n",
      "Epoch: 4612: Train loss: 2.3498127460479736 Valid loss: 1.105729579925537\n",
      "Epoch: 4613: Train loss: 2.350497245788574 Valid loss: 1.354517936706543\n",
      "Epoch: 4614: Train loss: 2.349541425704956 Valid loss: 1.1063281297683716\n",
      "Epoch: 4615: Train loss: 2.3502233028411865 Valid loss: 1.354935646057129\n",
      "Epoch: 4616: Train loss: 2.349273443222046 Valid loss: 1.1069269180297852\n",
      "Epoch: 4617: Train loss: 2.349949359893799 Valid loss: 1.3553563356399536\n",
      "Epoch: 4618: Train loss: 2.3490045070648193 Valid loss: 1.1075222492218018\n",
      "Epoch: 4619: Train loss: 2.3496768474578857 Valid loss: 1.3557811975479126\n",
      "Epoch: 4620: Train loss: 2.3487367630004883 Valid loss: 1.1081205606460571\n",
      "Epoch: 4621: Train loss: 2.3494069576263428 Valid loss: 1.356212854385376\n",
      "Epoch: 4622: Train loss: 2.34847092628479 Valid loss: 1.108717441558838\n",
      "Epoch: 4623: Train loss: 2.349135160446167 Valid loss: 1.3566339015960693\n",
      "Epoch: 4624: Train loss: 2.348203659057617 Valid loss: 1.1093162298202515\n",
      "Epoch: 4625: Train loss: 2.3488657474517822 Valid loss: 1.3570634126663208\n",
      "Epoch: 4626: Train loss: 2.3479373455047607 Valid loss: 1.1099095344543457\n",
      "Epoch: 4627: Train loss: 2.348592758178711 Valid loss: 1.3574849367141724\n",
      "Epoch: 4628: Train loss: 2.347670078277588 Valid loss: 1.1105051040649414\n",
      "Epoch: 4629: Train loss: 2.3483240604400635 Valid loss: 1.357909917831421\n",
      "Epoch: 4630: Train loss: 2.3474035263061523 Valid loss: 1.1110963821411133\n",
      "Epoch: 4631: Train loss: 2.348052501678467 Valid loss: 1.3583368062973022\n",
      "Epoch: 4632: Train loss: 2.347139835357666 Valid loss: 1.111696481704712\n",
      "Epoch: 4633: Train loss: 2.347784996032715 Valid loss: 1.3587716817855835\n",
      "Epoch: 4634: Train loss: 2.346876859664917 Valid loss: 1.1122909784317017\n",
      "Epoch: 4635: Train loss: 2.3475189208984375 Valid loss: 1.35920250415802\n",
      "Epoch: 4636: Train loss: 2.3466153144836426 Valid loss: 1.1128889322280884\n",
      "Epoch: 4637: Train loss: 2.3472518920898438 Valid loss: 1.3596351146697998\n",
      "Epoch: 4638: Train loss: 2.346351146697998 Valid loss: 1.113481044769287\n",
      "Epoch: 4639: Train loss: 2.3469858169555664 Valid loss: 1.3600603342056274\n",
      "Epoch: 4640: Train loss: 2.346088409423828 Valid loss: 1.114077091217041\n",
      "Epoch: 4641: Train loss: 2.3467164039611816 Valid loss: 1.3604929447174072\n",
      "Epoch: 4642: Train loss: 2.3458244800567627 Valid loss: 1.1146653890609741\n",
      "Epoch: 4643: Train loss: 2.346449613571167 Valid loss: 1.3609247207641602\n",
      "Epoch: 4644: Train loss: 2.3455636501312256 Valid loss: 1.1152628660202026\n",
      "Epoch: 4645: Train loss: 2.3461835384368896 Valid loss: 1.3613520860671997\n",
      "Epoch: 4646: Train loss: 2.3453006744384766 Valid loss: 1.1158568859100342\n",
      "Epoch: 4647: Train loss: 2.345917224884033 Valid loss: 1.3617818355560303\n",
      "Epoch: 4648: Train loss: 2.345043182373047 Valid loss: 1.116448163986206\n",
      "Epoch: 4649: Train loss: 2.345653533935547 Valid loss: 1.3622188568115234\n",
      "Epoch: 4650: Train loss: 2.3447797298431396 Valid loss: 1.1170415878295898\n",
      "Epoch: 4651: Train loss: 2.3453900814056396 Valid loss: 1.3626490831375122\n",
      "Epoch: 4652: Train loss: 2.3445210456848145 Valid loss: 1.1176296472549438\n",
      "Epoch: 4653: Train loss: 2.345127582550049 Valid loss: 1.363084316253662\n",
      "Epoch: 4654: Train loss: 2.344263792037964 Valid loss: 1.1182255744934082\n",
      "Epoch: 4655: Train loss: 2.3448665142059326 Valid loss: 1.3635218143463135\n",
      "Epoch: 4656: Train loss: 2.3440074920654297 Valid loss: 1.1188191175460815\n",
      "Epoch: 4657: Train loss: 2.3446044921875 Valid loss: 1.3639605045318604\n",
      "Epoch: 4658: Train loss: 2.343749523162842 Valid loss: 1.1194082498550415\n",
      "Epoch: 4659: Train loss: 2.3443443775177 Valid loss: 1.364395260810852\n",
      "Epoch: 4660: Train loss: 2.3434934616088867 Valid loss: 1.119999647140503\n",
      "Epoch: 4661: Train loss: 2.344082832336426 Valid loss: 1.3648350238800049\n",
      "Epoch: 4662: Train loss: 2.343235731124878 Valid loss: 1.1205918788909912\n",
      "Epoch: 4663: Train loss: 2.343823194503784 Valid loss: 1.3652714490890503\n",
      "Epoch: 4664: Train loss: 2.3429818153381348 Valid loss: 1.121181607246399\n",
      "Epoch: 4665: Train loss: 2.343564510345459 Valid loss: 1.36571204662323\n",
      "Epoch: 4666: Train loss: 2.3427257537841797 Valid loss: 1.1217694282531738\n",
      "Epoch: 4667: Train loss: 2.3433048725128174 Valid loss: 1.3661555051803589\n",
      "Epoch: 4668: Train loss: 2.342471122741699 Valid loss: 1.1223605871200562\n",
      "Epoch: 4669: Train loss: 2.343045234680176 Valid loss: 1.3665921688079834\n",
      "Epoch: 4670: Train loss: 2.342217206954956 Valid loss: 1.1229504346847534\n",
      "Epoch: 4671: Train loss: 2.3427863121032715 Valid loss: 1.3670361042022705\n",
      "Epoch: 4672: Train loss: 2.3419623374938965 Valid loss: 1.123538851737976\n",
      "Epoch: 4673: Train loss: 2.342531681060791 Valid loss: 1.3674778938293457\n",
      "Epoch: 4674: Train loss: 2.3417117595672607 Valid loss: 1.1241285800933838\n",
      "Epoch: 4675: Train loss: 2.3422739505767822 Valid loss: 1.3679213523864746\n",
      "Epoch: 4676: Train loss: 2.341456651687622 Valid loss: 1.1247155666351318\n",
      "Epoch: 4677: Train loss: 2.34201717376709 Valid loss: 1.368359088897705\n",
      "Epoch: 4678: Train loss: 2.3412041664123535 Valid loss: 1.125303864479065\n",
      "Epoch: 4679: Train loss: 2.341761350631714 Valid loss: 1.3688030242919922\n",
      "Epoch: 4680: Train loss: 2.3409533500671387 Valid loss: 1.1258933544158936\n",
      "Epoch: 4681: Train loss: 2.3415067195892334 Valid loss: 1.3692491054534912\n",
      "Epoch: 4682: Train loss: 2.3407034873962402 Valid loss: 1.1264809370040894\n",
      "Epoch: 4683: Train loss: 2.3412516117095947 Valid loss: 1.3696913719177246\n",
      "Epoch: 4684: Train loss: 2.3404531478881836 Valid loss: 1.1270707845687866\n",
      "Epoch: 4685: Train loss: 2.3409981727600098 Valid loss: 1.3701350688934326\n",
      "Epoch: 4686: Train loss: 2.340203285217285 Valid loss: 1.1276576519012451\n",
      "Epoch: 4687: Train loss: 2.3407444953918457 Valid loss: 1.3705806732177734\n",
      "Epoch: 4688: Train loss: 2.339953899383545 Valid loss: 1.1282438039779663\n",
      "Epoch: 4689: Train loss: 2.3404903411865234 Valid loss: 1.371027946472168\n",
      "Epoch: 4690: Train loss: 2.339703321456909 Valid loss: 1.1288297176361084\n",
      "Epoch: 4691: Train loss: 2.3402390480041504 Valid loss: 1.371475100517273\n",
      "Epoch: 4692: Train loss: 2.3394575119018555 Valid loss: 1.1294188499450684\n",
      "Epoch: 4693: Train loss: 2.339986562728882 Valid loss: 1.3719209432601929\n",
      "Epoch: 4694: Train loss: 2.339210033416748 Valid loss: 1.1300042867660522\n",
      "Epoch: 4695: Train loss: 2.339735269546509 Valid loss: 1.3723726272583008\n",
      "Epoch: 4696: Train loss: 2.3389620780944824 Valid loss: 1.1305930614471436\n",
      "Epoch: 4697: Train loss: 2.3394861221313477 Valid loss: 1.372820496559143\n",
      "Epoch: 4698: Train loss: 2.3387136459350586 Valid loss: 1.1311757564544678\n",
      "Epoch: 4699: Train loss: 2.3392322063446045 Valid loss: 1.3732740879058838\n",
      "Epoch: 4700: Train loss: 2.3384671211242676 Valid loss: 1.131763219833374\n",
      "Epoch: 4701: Train loss: 2.3389832973480225 Valid loss: 1.37371826171875\n",
      "Epoch: 4702: Train loss: 2.3382232189178467 Valid loss: 1.1323492527008057\n",
      "Epoch: 4703: Train loss: 2.338735818862915 Valid loss: 1.3741755485534668\n",
      "Epoch: 4704: Train loss: 2.337979316711426 Valid loss: 1.1329354047775269\n",
      "Epoch: 4705: Train loss: 2.338487386703491 Valid loss: 1.3746232986450195\n",
      "Epoch: 4706: Train loss: 2.3377339839935303 Valid loss: 1.1335203647613525\n",
      "Epoch: 4707: Train loss: 2.3382387161254883 Valid loss: 1.3750765323638916\n",
      "Epoch: 4708: Train loss: 2.3374903202056885 Valid loss: 1.1341047286987305\n",
      "Epoch: 4709: Train loss: 2.3379909992218018 Valid loss: 1.375530481338501\n",
      "Epoch: 4710: Train loss: 2.3372490406036377 Valid loss: 1.1346884965896606\n",
      "Epoch: 4711: Train loss: 2.337745189666748 Valid loss: 1.3759887218475342\n",
      "Epoch: 4712: Train loss: 2.3370046615600586 Valid loss: 1.1352739334106445\n",
      "Epoch: 4713: Train loss: 2.3374998569488525 Valid loss: 1.3764400482177734\n",
      "Epoch: 4714: Train loss: 2.3367631435394287 Valid loss: 1.135857343673706\n",
      "Epoch: 4715: Train loss: 2.337254285812378 Valid loss: 1.3768928050994873\n",
      "Epoch: 4716: Train loss: 2.336522340774536 Valid loss: 1.1364408731460571\n",
      "Epoch: 4717: Train loss: 2.3370089530944824 Valid loss: 1.3773456811904907\n",
      "Epoch: 4718: Train loss: 2.33627986907959 Valid loss: 1.1370278596878052\n",
      "Epoch: 4719: Train loss: 2.336763858795166 Valid loss: 1.3778071403503418\n",
      "Epoch: 4720: Train loss: 2.3360402584075928 Valid loss: 1.137609601020813\n",
      "Epoch: 4721: Train loss: 2.3365211486816406 Valid loss: 1.378265142440796\n",
      "Epoch: 4722: Train loss: 2.3357996940612793 Valid loss: 1.1381926536560059\n",
      "Epoch: 4723: Train loss: 2.3362746238708496 Valid loss: 1.3787204027175903\n",
      "Epoch: 4724: Train loss: 2.3355607986450195 Valid loss: 1.1387755870819092\n",
      "Epoch: 4725: Train loss: 2.336033344268799 Valid loss: 1.379180908203125\n",
      "Epoch: 4726: Train loss: 2.3353207111358643 Valid loss: 1.1393595933914185\n",
      "Epoch: 4727: Train loss: 2.335789680480957 Valid loss: 1.3796356916427612\n",
      "Epoch: 4728: Train loss: 2.3350815773010254 Valid loss: 1.1399421691894531\n",
      "Epoch: 4729: Train loss: 2.3355486392974854 Valid loss: 1.3800911903381348\n",
      "Epoch: 4730: Train loss: 2.334843873977661 Valid loss: 1.1405248641967773\n",
      "Epoch: 4731: Train loss: 2.3353054523468018 Valid loss: 1.380555272102356\n",
      "Epoch: 4732: Train loss: 2.334606409072876 Valid loss: 1.1411077976226807\n",
      "Epoch: 4733: Train loss: 2.335066795349121 Valid loss: 1.381014347076416\n",
      "Epoch: 4734: Train loss: 2.3343729972839355 Valid loss: 1.1416901350021362\n",
      "Epoch: 4735: Train loss: 2.334826946258545 Valid loss: 1.3814774751663208\n",
      "Epoch: 4736: Train loss: 2.3341336250305176 Valid loss: 1.1422722339630127\n",
      "Epoch: 4737: Train loss: 2.3345861434936523 Valid loss: 1.3819352388381958\n",
      "Epoch: 4738: Train loss: 2.333895683288574 Valid loss: 1.1428519487380981\n",
      "Epoch: 4739: Train loss: 2.3343441486358643 Valid loss: 1.38239586353302\n",
      "Epoch: 4740: Train loss: 2.3336591720581055 Valid loss: 1.1434334516525269\n",
      "Epoch: 4741: Train loss: 2.33410382270813 Valid loss: 1.382859468460083\n",
      "Epoch: 4742: Train loss: 2.3334240913391113 Valid loss: 1.1440123319625854\n",
      "Epoch: 4743: Train loss: 2.3338663578033447 Valid loss: 1.3833200931549072\n",
      "Epoch: 4744: Train loss: 2.333189010620117 Valid loss: 1.1445969343185425\n",
      "Epoch: 4745: Train loss: 2.3336286544799805 Valid loss: 1.383780598640442\n",
      "Epoch: 4746: Train loss: 2.3329548835754395 Valid loss: 1.1451765298843384\n",
      "Epoch: 4747: Train loss: 2.333390951156616 Valid loss: 1.38424813747406\n",
      "Epoch: 4748: Train loss: 2.3327200412750244 Valid loss: 1.1457576751708984\n",
      "Epoch: 4749: Train loss: 2.333153009414673 Valid loss: 1.384714126586914\n",
      "Epoch: 4750: Train loss: 2.332489252090454 Valid loss: 1.1463344097137451\n",
      "Epoch: 4751: Train loss: 2.3329172134399414 Valid loss: 1.3851805925369263\n",
      "Epoch: 4752: Train loss: 2.3322579860687256 Valid loss: 1.1469165086746216\n",
      "Epoch: 4753: Train loss: 2.33268404006958 Valid loss: 1.3856490850448608\n",
      "Epoch: 4754: Train loss: 2.3320271968841553 Valid loss: 1.1474990844726562\n",
      "Epoch: 4755: Train loss: 2.3324482440948486 Valid loss: 1.3861172199249268\n",
      "Epoch: 4756: Train loss: 2.331794261932373 Valid loss: 1.1480776071548462\n",
      "Epoch: 4757: Train loss: 2.33221435546875 Valid loss: 1.3865818977355957\n",
      "Epoch: 4758: Train loss: 2.33156681060791 Valid loss: 1.1486605405807495\n",
      "Epoch: 4759: Train loss: 2.3319814205169678 Valid loss: 1.3870532512664795\n",
      "Epoch: 4760: Train loss: 2.3313345909118652 Valid loss: 1.1492372751235962\n",
      "Epoch: 4761: Train loss: 2.3317487239837646 Valid loss: 1.3875187635421753\n",
      "Epoch: 4762: Train loss: 2.331108570098877 Valid loss: 1.1498173475265503\n",
      "Epoch: 4763: Train loss: 2.331517219543457 Valid loss: 1.387995719909668\n",
      "Epoch: 4764: Train loss: 2.3308794498443604 Valid loss: 1.1503949165344238\n",
      "Epoch: 4765: Train loss: 2.3312840461730957 Valid loss: 1.3884609937667847\n",
      "Epoch: 4766: Train loss: 2.33064866065979 Valid loss: 1.1509747505187988\n",
      "Epoch: 4767: Train loss: 2.3310494422912598 Valid loss: 1.3889331817626953\n",
      "Epoch: 4768: Train loss: 2.3304224014282227 Valid loss: 1.151552677154541\n",
      "Epoch: 4769: Train loss: 2.330820322036743 Valid loss: 1.3894062042236328\n",
      "Epoch: 4770: Train loss: 2.3301923274993896 Valid loss: 1.1521329879760742\n",
      "Epoch: 4771: Train loss: 2.33058762550354 Valid loss: 1.3898727893829346\n",
      "Epoch: 4772: Train loss: 2.3299639225006104 Valid loss: 1.1527106761932373\n",
      "Epoch: 4773: Train loss: 2.330355405807495 Valid loss: 1.3903392553329468\n",
      "Epoch: 4774: Train loss: 2.3297345638275146 Valid loss: 1.1532864570617676\n",
      "Epoch: 4775: Train loss: 2.330122709274292 Valid loss: 1.3908116817474365\n",
      "Epoch: 4776: Train loss: 2.3295083045959473 Valid loss: 1.153862714767456\n",
      "Epoch: 4777: Train loss: 2.3298938274383545 Valid loss: 1.3912861347198486\n",
      "Epoch: 4778: Train loss: 2.3292815685272217 Valid loss: 1.1544402837753296\n",
      "Epoch: 4779: Train loss: 2.3296632766723633 Valid loss: 1.3917585611343384\n",
      "Epoch: 4780: Train loss: 2.329056739807129 Valid loss: 1.155013918876648\n",
      "Epoch: 4781: Train loss: 2.329434394836426 Valid loss: 1.392232894897461\n",
      "Epoch: 4782: Train loss: 2.3288285732269287 Valid loss: 1.1555912494659424\n",
      "Epoch: 4783: Train loss: 2.3292040824890137 Valid loss: 1.3927017450332642\n",
      "Epoch: 4784: Train loss: 2.3286049365997314 Valid loss: 1.1561686992645264\n",
      "Epoch: 4785: Train loss: 2.328976631164551 Valid loss: 1.3931750059127808\n",
      "Epoch: 4786: Train loss: 2.3283801078796387 Valid loss: 1.1567461490631104\n",
      "Epoch: 4787: Train loss: 2.3287513256073 Valid loss: 1.3936522006988525\n",
      "Epoch: 4788: Train loss: 2.328157901763916 Valid loss: 1.1573234796524048\n",
      "Epoch: 4789: Train loss: 2.328524589538574 Valid loss: 1.3941264152526855\n",
      "Epoch: 4790: Train loss: 2.327934980392456 Valid loss: 1.1579010486602783\n",
      "Epoch: 4791: Train loss: 2.3282995223999023 Valid loss: 1.394607424736023\n",
      "Epoch: 4792: Train loss: 2.3277132511138916 Valid loss: 1.1584737300872803\n",
      "Epoch: 4793: Train loss: 2.3280723094940186 Valid loss: 1.3950817584991455\n",
      "Epoch: 4794: Train loss: 2.3274893760681152 Valid loss: 1.1590495109558105\n",
      "Epoch: 4795: Train loss: 2.3278465270996094 Valid loss: 1.3955553770065308\n",
      "Epoch: 4796: Train loss: 2.327267646789551 Valid loss: 1.1596275568008423\n",
      "Epoch: 4797: Train loss: 2.3276207447052 Valid loss: 1.3960387706756592\n",
      "Epoch: 4798: Train loss: 2.3270461559295654 Valid loss: 1.1602017879486084\n",
      "Epoch: 4799: Train loss: 2.32739520072937 Valid loss: 1.3965121507644653\n",
      "Epoch: 4800: Train loss: 2.326824188232422 Valid loss: 1.1607773303985596\n",
      "Epoch: 4801: Train loss: 2.3271703720092773 Valid loss: 1.3969910144805908\n",
      "Epoch: 4802: Train loss: 2.326601982116699 Valid loss: 1.1613484621047974\n",
      "Epoch: 4803: Train loss: 2.3269460201263428 Valid loss: 1.3974673748016357\n",
      "Epoch: 4804: Train loss: 2.3263816833496094 Valid loss: 1.161925196647644\n",
      "Epoch: 4805: Train loss: 2.326721668243408 Valid loss: 1.3979475498199463\n",
      "Epoch: 4806: Train loss: 2.3261613845825195 Valid loss: 1.1624979972839355\n",
      "Epoch: 4807: Train loss: 2.3264999389648438 Valid loss: 1.3984276056289673\n",
      "Epoch: 4808: Train loss: 2.3259437084198 Valid loss: 1.1630715131759644\n",
      "Epoch: 4809: Train loss: 2.3262765407562256 Valid loss: 1.3989067077636719\n",
      "Epoch: 4810: Train loss: 2.3257217407226562 Valid loss: 1.1636462211608887\n",
      "Epoch: 4811: Train loss: 2.326054811477661 Valid loss: 1.3993823528289795\n",
      "Epoch: 4812: Train loss: 2.3255043029785156 Valid loss: 1.1642205715179443\n",
      "Epoch: 4813: Train loss: 2.325831413269043 Valid loss: 1.399867296218872\n",
      "Epoch: 4814: Train loss: 2.3252851963043213 Valid loss: 1.1647924184799194\n",
      "Epoch: 4815: Train loss: 2.325608968734741 Valid loss: 1.4003454446792603\n",
      "Epoch: 4816: Train loss: 2.3250677585601807 Valid loss: 1.165366530418396\n",
      "Epoch: 4817: Train loss: 2.3253910541534424 Valid loss: 1.4008288383483887\n",
      "Epoch: 4818: Train loss: 2.32485032081604 Valid loss: 1.1659384965896606\n",
      "Epoch: 4819: Train loss: 2.3251702785491943 Valid loss: 1.4013092517852783\n",
      "Epoch: 4820: Train loss: 2.324633836746216 Valid loss: 1.1665127277374268\n",
      "Epoch: 4821: Train loss: 2.324948310852051 Valid loss: 1.4017902612686157\n",
      "Epoch: 4822: Train loss: 2.324416160583496 Valid loss: 1.16708505153656\n",
      "Epoch: 4823: Train loss: 2.3247294425964355 Valid loss: 1.4022748470306396\n",
      "Epoch: 4824: Train loss: 2.3242006301879883 Valid loss: 1.167656421661377\n",
      "Epoch: 4825: Train loss: 2.3245112895965576 Valid loss: 1.4027585983276367\n",
      "Epoch: 4826: Train loss: 2.3239848613739014 Valid loss: 1.168230414390564\n",
      "Epoch: 4827: Train loss: 2.324288845062256 Valid loss: 1.4032390117645264\n",
      "Epoch: 4828: Train loss: 2.323768138885498 Valid loss: 1.1687999963760376\n",
      "Epoch: 4829: Train loss: 2.324070692062378 Valid loss: 1.4037227630615234\n",
      "Epoch: 4830: Train loss: 2.323554039001465 Valid loss: 1.1693730354309082\n",
      "Epoch: 4831: Train loss: 2.323853015899658 Valid loss: 1.4042086601257324\n",
      "Epoch: 4832: Train loss: 2.323338747024536 Valid loss: 1.1699440479278564\n",
      "Epoch: 4833: Train loss: 2.3236377239227295 Valid loss: 1.4046941995620728\n",
      "Epoch: 4834: Train loss: 2.3231253623962402 Valid loss: 1.1705163717269897\n",
      "Epoch: 4835: Train loss: 2.3234214782714844 Valid loss: 1.4051846265792847\n",
      "Epoch: 4836: Train loss: 2.3229129314422607 Valid loss: 1.171087622642517\n",
      "Epoch: 4837: Train loss: 2.3232054710388184 Valid loss: 1.4056695699691772\n",
      "Epoch: 4838: Train loss: 2.3227007389068604 Valid loss: 1.1716628074645996\n",
      "Epoch: 4839: Train loss: 2.322988986968994 Valid loss: 1.4061568975448608\n",
      "Epoch: 4840: Train loss: 2.322489023208618 Valid loss: 1.1722325086593628\n",
      "Epoch: 4841: Train loss: 2.3227739334106445 Valid loss: 1.4066426753997803\n",
      "Epoch: 4842: Train loss: 2.3222765922546387 Valid loss: 1.1728055477142334\n",
      "Epoch: 4843: Train loss: 2.322561264038086 Valid loss: 1.4071335792541504\n",
      "Epoch: 4844: Train loss: 2.322066068649292 Valid loss: 1.1733758449554443\n",
      "Epoch: 4845: Train loss: 2.3223483562469482 Valid loss: 1.4076266288757324\n",
      "Epoch: 4846: Train loss: 2.321857213973999 Valid loss: 1.1739466190338135\n",
      "Epoch: 4847: Train loss: 2.3221328258514404 Valid loss: 1.4081140756607056\n",
      "Epoch: 4848: Train loss: 2.3216445446014404 Valid loss: 1.1745184659957886\n",
      "Epoch: 4849: Train loss: 2.321920394897461 Valid loss: 1.408605933189392\n",
      "Epoch: 4850: Train loss: 2.3214356899261475 Valid loss: 1.1750890016555786\n",
      "Epoch: 4851: Train loss: 2.3217079639434814 Valid loss: 1.409096360206604\n",
      "Epoch: 4852: Train loss: 2.3212289810180664 Valid loss: 1.1756644248962402\n",
      "Epoch: 4853: Train loss: 2.32149600982666 Valid loss: 1.4095877408981323\n",
      "Epoch: 4854: Train loss: 2.3210179805755615 Valid loss: 1.176233172416687\n",
      "Epoch: 4855: Train loss: 2.3212838172912598 Valid loss: 1.4100786447525024\n",
      "Epoch: 4856: Train loss: 2.3208093643188477 Valid loss: 1.176802396774292\n",
      "Epoch: 4857: Train loss: 2.321070671081543 Valid loss: 1.410565733909607\n",
      "Epoch: 4858: Train loss: 2.3205995559692383 Valid loss: 1.1773719787597656\n",
      "Epoch: 4859: Train loss: 2.3208584785461426 Valid loss: 1.411056399345398\n",
      "Epoch: 4860: Train loss: 2.32039213180542 Valid loss: 1.1779438257217407\n",
      "Epoch: 4861: Train loss: 2.320648193359375 Valid loss: 1.4115490913391113\n",
      "Epoch: 4862: Train loss: 2.3201842308044434 Valid loss: 1.1785107851028442\n",
      "Epoch: 4863: Train loss: 2.3204362392425537 Valid loss: 1.412043809890747\n",
      "Epoch: 4864: Train loss: 2.3199753761291504 Valid loss: 1.1790807247161865\n",
      "Epoch: 4865: Train loss: 2.320227861404419 Valid loss: 1.4125345945358276\n",
      "Epoch: 4866: Train loss: 2.3197689056396484 Valid loss: 1.179648756980896\n",
      "Epoch: 4867: Train loss: 2.3200159072875977 Valid loss: 1.4130237102508545\n",
      "Epoch: 4868: Train loss: 2.3195619583129883 Valid loss: 1.1802140474319458\n",
      "Epoch: 4869: Train loss: 2.319808006286621 Valid loss: 1.413515329360962\n",
      "Epoch: 4870: Train loss: 2.3193554878234863 Valid loss: 1.1807830333709717\n",
      "Epoch: 4871: Train loss: 2.3195981979370117 Valid loss: 1.4140121936798096\n",
      "Epoch: 4872: Train loss: 2.3191514015197754 Valid loss: 1.1813545227050781\n",
      "Epoch: 4873: Train loss: 2.319390296936035 Valid loss: 1.414505958557129\n",
      "Epoch: 4874: Train loss: 2.3189451694488525 Valid loss: 1.181921124458313\n",
      "Epoch: 4875: Train loss: 2.319180727005005 Valid loss: 1.4149986505508423\n",
      "Epoch: 4876: Train loss: 2.318739891052246 Valid loss: 1.182491660118103\n",
      "Epoch: 4877: Train loss: 2.318972110748291 Valid loss: 1.4154894351959229\n",
      "Epoch: 4878: Train loss: 2.3185348510742188 Valid loss: 1.1830559968948364\n",
      "Epoch: 4879: Train loss: 2.3187644481658936 Valid loss: 1.415988564491272\n",
      "Epoch: 4880: Train loss: 2.318329095840454 Valid loss: 1.183624029159546\n",
      "Epoch: 4881: Train loss: 2.318556785583496 Valid loss: 1.4164817333221436\n",
      "Epoch: 4882: Train loss: 2.3181231021881104 Valid loss: 1.1841899156570435\n",
      "Epoch: 4883: Train loss: 2.3183484077453613 Valid loss: 1.4169719219207764\n",
      "Epoch: 4884: Train loss: 2.3179187774658203 Valid loss: 1.1847587823867798\n",
      "Epoch: 4885: Train loss: 2.3181405067443848 Valid loss: 1.417468786239624\n",
      "Epoch: 4886: Train loss: 2.3177130222320557 Valid loss: 1.1853238344192505\n",
      "Epoch: 4887: Train loss: 2.317934274673462 Valid loss: 1.4179612398147583\n",
      "Epoch: 4888: Train loss: 2.317511558532715 Valid loss: 1.1858899593353271\n",
      "Epoch: 4889: Train loss: 2.3177285194396973 Valid loss: 1.4184552431106567\n",
      "Epoch: 4890: Train loss: 2.317307710647583 Valid loss: 1.186458706855774\n",
      "Epoch: 4891: Train loss: 2.317523241043091 Valid loss: 1.4189558029174805\n",
      "Epoch: 4892: Train loss: 2.317106008529663 Valid loss: 1.1870225667953491\n",
      "Epoch: 4893: Train loss: 2.317316770553589 Valid loss: 1.419458031654358\n",
      "Epoch: 4894: Train loss: 2.3169050216674805 Valid loss: 1.1875905990600586\n",
      "Epoch: 4895: Train loss: 2.3171133995056152 Valid loss: 1.4199515581130981\n",
      "Epoch: 4896: Train loss: 2.3167035579681396 Valid loss: 1.188159704208374\n",
      "Epoch: 4897: Train loss: 2.316908597946167 Valid loss: 1.4204492568969727\n",
      "Epoch: 4898: Train loss: 2.3165016174316406 Valid loss: 1.1887258291244507\n",
      "Epoch: 4899: Train loss: 2.316704273223877 Valid loss: 1.4209463596343994\n",
      "Epoch: 4900: Train loss: 2.316298246383667 Valid loss: 1.1892879009246826\n",
      "Epoch: 4901: Train loss: 2.316498279571533 Valid loss: 1.4214398860931396\n",
      "Epoch: 4902: Train loss: 2.31609845161438 Valid loss: 1.1898561716079712\n",
      "Epoch: 4903: Train loss: 2.3162949085235596 Valid loss: 1.4219404458999634\n",
      "Epoch: 4904: Train loss: 2.315897226333618 Valid loss: 1.19041907787323\n",
      "Epoch: 4905: Train loss: 2.3160908222198486 Valid loss: 1.4224369525909424\n",
      "Epoch: 4906: Train loss: 2.31569766998291 Valid loss: 1.190987229347229\n",
      "Epoch: 4907: Train loss: 2.3158907890319824 Valid loss: 1.4229400157928467\n",
      "Epoch: 4908: Train loss: 2.3154990673065186 Valid loss: 1.1915507316589355\n",
      "Epoch: 4909: Train loss: 2.3156895637512207 Valid loss: 1.423440933227539\n",
      "Epoch: 4910: Train loss: 2.3152995109558105 Valid loss: 1.192114233970642\n",
      "Epoch: 4911: Train loss: 2.3154852390289307 Valid loss: 1.4239343404769897\n",
      "Epoch: 4912: Train loss: 2.315099000930786 Valid loss: 1.192679524421692\n",
      "Epoch: 4913: Train loss: 2.315284252166748 Valid loss: 1.4244390726089478\n",
      "Epoch: 4914: Train loss: 2.3149030208587646 Valid loss: 1.193245530128479\n",
      "Epoch: 4915: Train loss: 2.3150851726531982 Valid loss: 1.424939513206482\n",
      "Epoch: 4916: Train loss: 2.314706802368164 Valid loss: 1.1938095092773438\n",
      "Epoch: 4917: Train loss: 2.3148839473724365 Valid loss: 1.425441861152649\n",
      "Epoch: 4918: Train loss: 2.3145089149475098 Valid loss: 1.1943790912628174\n",
      "Epoch: 4919: Train loss: 2.3146841526031494 Valid loss: 1.4259449243545532\n",
      "Epoch: 4920: Train loss: 2.314311981201172 Valid loss: 1.1949388980865479\n",
      "Epoch: 4921: Train loss: 2.314484119415283 Valid loss: 1.4264466762542725\n",
      "Epoch: 4922: Train loss: 2.314114809036255 Valid loss: 1.1955050230026245\n",
      "Epoch: 4923: Train loss: 2.314286231994629 Valid loss: 1.4269464015960693\n",
      "Epoch: 4924: Train loss: 2.313918113708496 Valid loss: 1.1960721015930176\n",
      "Epoch: 4925: Train loss: 2.3140869140625 Valid loss: 1.4274492263793945\n",
      "Epoch: 4926: Train loss: 2.313722848892212 Valid loss: 1.196635127067566\n",
      "Epoch: 4927: Train loss: 2.313889503479004 Valid loss: 1.4279524087905884\n",
      "Epoch: 4928: Train loss: 2.3135287761688232 Valid loss: 1.1971991062164307\n",
      "Epoch: 4929: Train loss: 2.313688278198242 Valid loss: 1.4284563064575195\n",
      "Epoch: 4930: Train loss: 2.3133325576782227 Valid loss: 1.1977617740631104\n",
      "Epoch: 4931: Train loss: 2.3134918212890625 Valid loss: 1.4289600849151611\n",
      "Epoch: 4932: Train loss: 2.31313419342041 Valid loss: 1.1983282566070557\n",
      "Epoch: 4933: Train loss: 2.3132944107055664 Valid loss: 1.4294590950012207\n",
      "Epoch: 4934: Train loss: 2.3129405975341797 Valid loss: 1.1988881826400757\n",
      "Epoch: 4935: Train loss: 2.313096523284912 Valid loss: 1.4299665689468384\n",
      "Epoch: 4936: Train loss: 2.3127481937408447 Valid loss: 1.199451208114624\n",
      "Epoch: 4937: Train loss: 2.3129003047943115 Valid loss: 1.430469036102295\n",
      "Epoch: 4938: Train loss: 2.312553644180298 Valid loss: 1.2000185251235962\n",
      "Epoch: 4939: Train loss: 2.3127052783966064 Valid loss: 1.430975079536438\n",
      "Epoch: 4940: Train loss: 2.3123607635498047 Valid loss: 1.2005770206451416\n",
      "Epoch: 4941: Train loss: 2.3125083446502686 Valid loss: 1.4314844608306885\n",
      "Epoch: 4942: Train loss: 2.312169075012207 Valid loss: 1.2011418342590332\n",
      "Epoch: 4943: Train loss: 2.312314033508301 Valid loss: 1.4319895505905151\n",
      "Epoch: 4944: Train loss: 2.3119773864746094 Valid loss: 1.2017062902450562\n",
      "Epoch: 4945: Train loss: 2.312119722366333 Valid loss: 1.4324973821640015\n",
      "Epoch: 4946: Train loss: 2.311783790588379 Valid loss: 1.202267050743103\n",
      "Epoch: 4947: Train loss: 2.3119258880615234 Valid loss: 1.4330044984817505\n",
      "Epoch: 4948: Train loss: 2.3115923404693604 Valid loss: 1.2028321027755737\n",
      "Epoch: 4949: Train loss: 2.31173038482666 Valid loss: 1.4335107803344727\n",
      "Epoch: 4950: Train loss: 2.3114004135131836 Valid loss: 1.2033936977386475\n",
      "Epoch: 4951: Train loss: 2.311534881591797 Valid loss: 1.434016227722168\n",
      "Epoch: 4952: Train loss: 2.311209201812744 Valid loss: 1.2039577960968018\n",
      "Epoch: 4953: Train loss: 2.3113420009613037 Valid loss: 1.4345241785049438\n",
      "Epoch: 4954: Train loss: 2.3110201358795166 Valid loss: 1.2045204639434814\n",
      "Epoch: 4955: Train loss: 2.3111495971679688 Valid loss: 1.4350323677062988\n",
      "Epoch: 4956: Train loss: 2.3108274936676025 Valid loss: 1.2050793170928955\n",
      "Epoch: 4957: Train loss: 2.3109548091888428 Valid loss: 1.4355422258377075\n",
      "Epoch: 4958: Train loss: 2.310638189315796 Valid loss: 1.205642819404602\n",
      "Epoch: 4959: Train loss: 2.310763120651245 Valid loss: 1.4360499382019043\n",
      "Epoch: 4960: Train loss: 2.3104500770568848 Valid loss: 1.2062071561813354\n",
      "Epoch: 4961: Train loss: 2.3105711936950684 Valid loss: 1.4365575313568115\n",
      "Epoch: 4962: Train loss: 2.3102598190307617 Valid loss: 1.2067680358886719\n",
      "Epoch: 4963: Train loss: 2.310380697250366 Valid loss: 1.4370670318603516\n",
      "Epoch: 4964: Train loss: 2.3100697994232178 Valid loss: 1.2073286771774292\n",
      "Epoch: 4965: Train loss: 2.3101871013641357 Valid loss: 1.4375755786895752\n",
      "Epoch: 4966: Train loss: 2.3098809719085693 Valid loss: 1.2078907489776611\n",
      "Epoch: 4967: Train loss: 2.3099966049194336 Valid loss: 1.4380888938903809\n",
      "Epoch: 4968: Train loss: 2.309692859649658 Valid loss: 1.2084497213363647\n",
      "Epoch: 4969: Train loss: 2.309804916381836 Valid loss: 1.4385936260223389\n",
      "Epoch: 4970: Train loss: 2.3095035552978516 Valid loss: 1.2090115547180176\n",
      "Epoch: 4971: Train loss: 2.30961275100708 Valid loss: 1.439106822013855\n",
      "Epoch: 4972: Train loss: 2.3093161582946777 Valid loss: 1.209574580192566\n",
      "Epoch: 4973: Train loss: 2.3094258308410645 Valid loss: 1.4396120309829712\n",
      "Epoch: 4974: Train loss: 2.3091306686401367 Valid loss: 1.2101346254348755\n",
      "Epoch: 4975: Train loss: 2.3092358112335205 Valid loss: 1.4401293992996216\n",
      "Epoch: 4976: Train loss: 2.3089425563812256 Valid loss: 1.2106945514678955\n",
      "Epoch: 4977: Train loss: 2.3090474605560303 Valid loss: 1.4406425952911377\n",
      "Epoch: 4978: Train loss: 2.3087573051452637 Valid loss: 1.2112582921981812\n",
      "Epoch: 4979: Train loss: 2.3088555335998535 Valid loss: 1.4411511421203613\n",
      "Epoch: 4980: Train loss: 2.3085696697235107 Valid loss: 1.2118172645568848\n",
      "Epoch: 4981: Train loss: 2.3086681365966797 Valid loss: 1.4416635036468506\n",
      "Epoch: 4982: Train loss: 2.3083839416503906 Valid loss: 1.2123805284500122\n",
      "Epoch: 4983: Train loss: 2.3084778785705566 Valid loss: 1.4421745538711548\n",
      "Epoch: 4984: Train loss: 2.3081979751586914 Valid loss: 1.212938904762268\n",
      "Epoch: 4985: Train loss: 2.3082916736602783 Valid loss: 1.4426825046539307\n",
      "Epoch: 4986: Train loss: 2.308013439178467 Valid loss: 1.213498592376709\n",
      "Epoch: 4987: Train loss: 2.3081045150756836 Valid loss: 1.4432002305984497\n",
      "Epoch: 4988: Train loss: 2.307828187942505 Valid loss: 1.2140603065490723\n",
      "Epoch: 4989: Train loss: 2.307918071746826 Valid loss: 1.4437105655670166\n",
      "Epoch: 4990: Train loss: 2.307645082473755 Valid loss: 1.2146220207214355\n",
      "Epoch: 4991: Train loss: 2.3077313899993896 Valid loss: 1.4442262649536133\n",
      "Epoch: 4992: Train loss: 2.3074607849121094 Valid loss: 1.2151778936386108\n",
      "Epoch: 4993: Train loss: 2.3075437545776367 Valid loss: 1.4447357654571533\n",
      "Epoch: 4994: Train loss: 2.3072752952575684 Valid loss: 1.215738296508789\n",
      "Epoch: 4995: Train loss: 2.307356595993042 Valid loss: 1.4452513456344604\n",
      "Epoch: 4996: Train loss: 2.30709171295166 Valid loss: 1.2162973880767822\n",
      "Epoch: 4997: Train loss: 2.30717134475708 Valid loss: 1.4457639455795288\n",
      "Epoch: 4998: Train loss: 2.306907892227173 Valid loss: 1.2168561220169067\n",
      "Epoch: 4999: Train loss: 2.30698299407959 Valid loss: 1.4462765455245972\n",
      "Epoch: 5000: Train loss: 2.306723117828369 Valid loss: 1.2174160480499268\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=5_000,\n",
    "    optimizer=optimizer,\n",
    "    model=seq_model,\n",
    "    loss_fn=nn.MSELoss(),\n",
    "    t_u_train=t_un_train,\n",
    "    t_u_val = t_un_val,\n",
    "    t_c_train=t_c_train,\n",
    "    t_c_val=t_c_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x789598caef50>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "t_range = torch.arange(20, 90).unsqueeze(1)\n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), '^')\n",
    "plt.plot(t_range.numpy(), seq_model(0.1 * t_range).detach().numpy(), 'c-')\n",
    "plt.plot(t_u.numpy(), seq_model(0.1 * t_u).detach().numpy(), 'kx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
